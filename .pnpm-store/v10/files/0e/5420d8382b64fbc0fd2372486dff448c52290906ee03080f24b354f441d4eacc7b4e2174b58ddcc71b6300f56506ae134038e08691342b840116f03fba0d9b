import { AIMessage, SystemMessage, ToolMessage, anthropicPromptCachingMiddleware, createAgent, createMiddleware, humanInTheLoopMiddleware, summarizationMiddleware, todoListMiddleware, tool } from "langchain";
import { Command, REMOVE_ALL_MESSAGES, getCurrentTaskInput, isCommand } from "@langchain/langgraph";
import { z } from "zod/v4";
import micromatch from "micromatch";
import { basename } from "path";
import { HumanMessage, RemoveMessage } from "@langchain/core/messages";
import { z as z$1 } from "zod";
import yaml from "yaml";
import "@langchain/core/utils/env";
import "@langchain/core/utils/function_calling";
import "@langchain/core/utils/json_schema";
import "@langchain/core/language_models/chat_models";
import "@langchain/core/runnables";
import "@langchain/core/output_parsers";
import "@langchain/core/output_parsers/openai_tools";
import "@langchain/core/outputs";
import "@langchain/core/language_models/llms";
import "@langchain/core/utils/chunk_array";
import "@langchain/core/embeddings";
import "@langchain/core/singletons";
import fs from "node:fs/promises";
import fs$1 from "node:fs";
import path from "node:path";
import { spawn } from "node:child_process";
import fg from "fast-glob";
import os from "node:os";

//#region src/backends/protocol.ts
/**
* Type guard to check if a backend supports execution.
*
* @param backend - Backend instance to check
* @returns True if the backend implements SandboxBackendProtocol
*/
function isSandboxBackend(backend) {
	return typeof backend.execute === "function" && typeof backend.id === "string";
}

//#endregion
//#region src/backends/utils.ts
/**
* Shared utility functions for memory backend implementations.
*
* This module contains both user-facing string formatters and structured
* helpers used by backends and the composite router. Structured helpers
* enable composition without fragile string parsing.
*/
const EMPTY_CONTENT_WARNING = "System reminder: File exists but has empty contents";
const MAX_LINE_LENGTH = 1e4;
const LINE_NUMBER_WIDTH = 6;
/**
* Sanitize tool_call_id to prevent path traversal and separator issues.
*
* Replaces dangerous characters (., /, \) with underscores.
*/
function sanitizeToolCallId(toolCallId) {
	return toolCallId.replace(/\./g, "_").replace(/\//g, "_").replace(/\\/g, "_");
}
/**
* Format file content with line numbers (cat -n style).
*
* Chunks lines longer than MAX_LINE_LENGTH with continuation markers (e.g., 5.1, 5.2).
*
* @param content - File content as string or list of lines
* @param startLine - Starting line number (default: 1)
* @returns Formatted content with line numbers and continuation markers
*/
function formatContentWithLineNumbers(content, startLine = 1) {
	let lines;
	if (typeof content === "string") {
		lines = content.split("\n");
		if (lines.length > 0 && lines[lines.length - 1] === "") lines = lines.slice(0, -1);
	} else lines = content;
	const resultLines = [];
	for (let i = 0; i < lines.length; i++) {
		const line = lines[i];
		const lineNum = i + startLine;
		if (line.length <= MAX_LINE_LENGTH) resultLines.push(`${lineNum.toString().padStart(LINE_NUMBER_WIDTH)}\t${line}`);
		else {
			const numChunks = Math.ceil(line.length / MAX_LINE_LENGTH);
			for (let chunkIdx = 0; chunkIdx < numChunks; chunkIdx++) {
				const start = chunkIdx * MAX_LINE_LENGTH;
				const end = Math.min(start + MAX_LINE_LENGTH, line.length);
				const chunk = line.substring(start, end);
				if (chunkIdx === 0) resultLines.push(`${lineNum.toString().padStart(LINE_NUMBER_WIDTH)}\t${chunk}`);
				else {
					const continuationMarker = `${lineNum}.${chunkIdx}`;
					resultLines.push(`${continuationMarker.padStart(LINE_NUMBER_WIDTH)}\t${chunk}`);
				}
			}
		}
	}
	return resultLines.join("\n");
}
/**
* Check if content is empty and return warning message.
*
* @param content - Content to check
* @returns Warning message if empty, null otherwise
*/
function checkEmptyContent(content) {
	if (!content || content.trim() === "") return EMPTY_CONTENT_WARNING;
	return null;
}
/**
* Convert FileData to plain string content.
*
* @param fileData - FileData object with 'content' key
* @returns Content as string with lines joined by newlines
*/
function fileDataToString(fileData) {
	return fileData.content.join("\n");
}
/**
* Create a FileData object with timestamps.
*
* @param content - File content as string
* @param createdAt - Optional creation timestamp (ISO format)
* @returns FileData object with content and timestamps
*/
function createFileData(content, createdAt) {
	const lines = typeof content === "string" ? content.split("\n") : content;
	const now = (/* @__PURE__ */ new Date()).toISOString();
	return {
		content: lines,
		created_at: createdAt || now,
		modified_at: now
	};
}
/**
* Update FileData with new content, preserving creation timestamp.
*
* @param fileData - Existing FileData object
* @param content - New content as string
* @returns Updated FileData object
*/
function updateFileData(fileData, content) {
	const lines = typeof content === "string" ? content.split("\n") : content;
	const now = (/* @__PURE__ */ new Date()).toISOString();
	return {
		content: lines,
		created_at: fileData.created_at,
		modified_at: now
	};
}
/**
* Format file data for read response with line numbers.
*
* @param fileData - FileData object
* @param offset - Line offset (0-indexed)
* @param limit - Maximum number of lines
* @returns Formatted content or error message
*/
function formatReadResponse(fileData, offset, limit$1) {
	const content = fileDataToString(fileData);
	const emptyMsg = checkEmptyContent(content);
	if (emptyMsg) return emptyMsg;
	const lines = content.split("\n");
	const startIdx = offset;
	const endIdx = Math.min(startIdx + limit$1, lines.length);
	if (startIdx >= lines.length) return `Error: Line offset ${offset} exceeds file length (${lines.length} lines)`;
	return formatContentWithLineNumbers(lines.slice(startIdx, endIdx), startIdx + 1);
}
/**
* Perform string replacement with occurrence validation.
*
* @param content - Original content
* @param oldString - String to replace
* @param newString - Replacement string
* @param replaceAll - Whether to replace all occurrences
* @returns Tuple of [new_content, occurrences] on success, or error message string
*/
function performStringReplacement(content, oldString, newString, replaceAll) {
	const occurrences = content.split(oldString).length - 1;
	if (occurrences === 0) return `Error: String not found in file: '${oldString}'`;
	if (occurrences > 1 && !replaceAll) return `Error: String '${oldString}' appears ${occurrences} times in file. Use replace_all=True to replace all instances, or provide a more specific string with surrounding context.`;
	return [content.split(oldString).join(newString), occurrences];
}
/**
* Validate and normalize a directory path.
*
* Ensures paths are safe to use by preventing directory traversal attacks
* and enforcing consistent formatting. All paths are normalized to use
* forward slashes and start with a leading slash.
*
* This function is designed for virtual filesystem paths and rejects
* Windows absolute paths (e.g., C:/..., F:/...) to maintain consistency
* and prevent path format ambiguity.
*
* @param path - Path to validate
* @returns Normalized path starting with / and ending with /
* @throws Error if path is invalid
*
* @example
* ```typescript
* validatePath("foo/bar")  // Returns: "/foo/bar/"
* validatePath("/./foo//bar")  // Returns: "/foo/bar/"
* validatePath("../etc/passwd")  // Throws: Path traversal not allowed
* validatePath("C:\\Users\\file")  // Throws: Windows absolute paths not supported
* ```
*/
function validatePath(path$2) {
	const pathStr = path$2 || "/";
	if (!pathStr || pathStr.trim() === "") throw new Error("Path cannot be empty");
	let normalized = pathStr.startsWith("/") ? pathStr : "/" + pathStr;
	if (!normalized.endsWith("/")) normalized += "/";
	return normalized;
}
/**
* Search files dict for paths matching glob pattern.
*
* @param files - Dictionary of file paths to FileData
* @param pattern - Glob pattern (e.g., `*.py`, `**\/*.ts`)
* @param path - Base path to search from
* @returns Newline-separated file paths, sorted by modification time (most recent first).
*          Returns "No files found" if no matches.
*
* @example
* ```typescript
* const files = {"/src/main.py": FileData(...), "/test.py": FileData(...)};
* globSearchFiles(files, "*.py", "/");
* // Returns: "/test.py\n/src/main.py" (sorted by modified_at)
* ```
*/
function globSearchFiles(files, pattern, path$2 = "/") {
	let normalizedPath;
	try {
		normalizedPath = validatePath(path$2);
	} catch {
		return "No files found";
	}
	const filtered = Object.fromEntries(Object.entries(files).filter(([fp]) => fp.startsWith(normalizedPath)));
	const effectivePattern = pattern;
	const matches = [];
	for (const [filePath, fileData] of Object.entries(filtered)) {
		let relative = filePath.substring(normalizedPath.length);
		if (relative.startsWith("/")) relative = relative.substring(1);
		if (!relative) {
			const parts = filePath.split("/");
			relative = parts[parts.length - 1] || "";
		}
		if (micromatch.isMatch(relative, effectivePattern, {
			dot: true,
			nobrace: false
		})) matches.push([filePath, fileData.modified_at]);
	}
	matches.sort((a, b) => b[1].localeCompare(a[1]));
	if (matches.length === 0) return "No files found";
	return matches.map(([fp]) => fp).join("\n");
}
/**
* Return structured grep matches from an in-memory files mapping.
*
* Returns a list of GrepMatch on success, or a string for invalid inputs
* (e.g., invalid regex). We deliberately do not raise here to keep backends
* non-throwing in tool contexts and preserve user-facing error messages.
*/
function grepMatchesFromFiles(files, pattern, path$2 = null, glob = null) {
	let regex;
	try {
		regex = new RegExp(pattern);
	} catch (e) {
		return `Invalid regex pattern: ${e.message}`;
	}
	let normalizedPath;
	try {
		normalizedPath = validatePath(path$2);
	} catch {
		return [];
	}
	let filtered = Object.fromEntries(Object.entries(files).filter(([fp]) => fp.startsWith(normalizedPath)));
	if (glob) filtered = Object.fromEntries(Object.entries(filtered).filter(([fp]) => micromatch.isMatch(basename(fp), glob, {
		dot: true,
		nobrace: false
	})));
	const matches = [];
	for (const [filePath, fileData] of Object.entries(filtered)) for (let i = 0; i < fileData.content.length; i++) {
		const line = fileData.content[i];
		const lineNum = i + 1;
		if (regex.test(line)) matches.push({
			path: filePath,
			line: lineNum,
			text: line
		});
	}
	return matches;
}

//#endregion
//#region src/backends/state.ts
/**
* Backend that stores files in agent state (ephemeral).
*
* Uses LangGraph's state management and checkpointing. Files persist within
* a conversation thread but not across threads. State is automatically
* checkpointed after each agent step.
*
* Special handling: Since LangGraph state must be updated via Command objects
* (not direct mutation), operations return filesUpdate in WriteResult/EditResult
* for the middleware to apply via Command.
*/
var StateBackend = class {
	stateAndStore;
	constructor(stateAndStore) {
		this.stateAndStore = stateAndStore;
	}
	/**
	* Get files from current state.
	*/
	getFiles() {
		return this.stateAndStore.state.files || {};
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	lsInfo(path$2) {
		const files = this.getFiles();
		const infos = [];
		const subdirs = /* @__PURE__ */ new Set();
		const normalizedPath = path$2.endsWith("/") ? path$2 : path$2 + "/";
		for (const [k, fd] of Object.entries(files)) {
			if (!k.startsWith(normalizedPath)) continue;
			const relative = k.substring(normalizedPath.length);
			if (relative.includes("/")) {
				const subdirName = relative.split("/")[0];
				subdirs.add(normalizedPath + subdirName + "/");
				continue;
			}
			const size = fd.content.join("\n").length;
			infos.push({
				path: k,
				is_dir: false,
				size,
				modified_at: fd.modified_at
			});
		}
		for (const subdir of Array.from(subdirs).sort()) infos.push({
			path: subdir,
			is_dir: true,
			size: 0,
			modified_at: ""
		});
		infos.sort((a, b) => a.path.localeCompare(b.path));
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	read(filePath, offset = 0, limit$1 = 500) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) return `Error: File '${filePath}' not found`;
		return formatReadResponse(fileData, offset, limit$1);
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	readRaw(filePath) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) throw new Error(`File '${filePath}' not found`);
		return fileData;
	}
	/**
	* Create a new file with content.
	* Returns WriteResult with filesUpdate to update LangGraph state.
	*/
	write(filePath, content) {
		if (filePath in this.getFiles()) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		const newFileData = createFileData(content);
		return {
			path: filePath,
			filesUpdate: { [filePath]: newFileData }
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult with filesUpdate and occurrences.
	*/
	edit(filePath, oldString, newString, replaceAll = false) {
		const fileData = this.getFiles()[filePath];
		if (!fileData) return { error: `Error: File '${filePath}' not found` };
		const result = performStringReplacement(fileDataToString(fileData), oldString, newString, replaceAll);
		if (typeof result === "string") return { error: result };
		const [newContent, occurrences] = result;
		const newFileData = updateFileData(fileData, newContent);
		return {
			path: filePath,
			filesUpdate: { [filePath]: newFileData },
			occurrences
		};
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	grepRaw(pattern, path$2 = "/", glob = null) {
		return grepMatchesFromFiles(this.getFiles(), pattern, path$2, glob);
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	globInfo(pattern, path$2 = "/") {
		const files = this.getFiles();
		const result = globSearchFiles(files, pattern, path$2);
		if (result === "No files found") return [];
		const paths = result.split("\n");
		const infos = [];
		for (const p of paths) {
			const fd = files[p];
			const size = fd ? fd.content.join("\n").length : 0;
			infos.push({
				path: p,
				is_dir: false,
				size,
				modified_at: fd?.modified_at || ""
			});
		}
		return infos;
	}
	/**
	* Upload multiple files.
	*
	* Note: Since LangGraph state must be updated via Command objects,
	* the caller must apply filesUpdate via Command after calling this method.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	uploadFiles(files) {
		const responses = [];
		const updates = {};
		for (const [path$2, content] of files) try {
			updates[path$2] = createFileData(new TextDecoder().decode(content));
			responses.push({
				path: path$2,
				error: null
			});
		} catch {
			responses.push({
				path: path$2,
				error: "invalid_path"
			});
		}
		const result = responses;
		result.filesUpdate = updates;
		return result;
	}
	/**
	* Download multiple files.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	downloadFiles(paths) {
		const files = this.getFiles();
		const responses = [];
		for (const path$2 of paths) {
			const fileData = files[path$2];
			if (!fileData) {
				responses.push({
					path: path$2,
					content: null,
					error: "file_not_found"
				});
				continue;
			}
			const contentStr = fileDataToString(fileData);
			const content = new TextEncoder().encode(contentStr);
			responses.push({
				path: path$2,
				content,
				error: null
			});
		}
		return responses;
	}
};

//#endregion
//#region src/middleware/fs.ts
/**
* Middleware for providing filesystem tools to an agent.
*
* Provides ls, read_file, write_file, edit_file, glob, and grep tools with support for:
* - Pluggable backends (StateBackend, StoreBackend, FilesystemBackend, CompositeBackend)
* - Tool result eviction for large outputs
*/
/**
* Tools that should be excluded from the large result eviction logic.
*
* This array contains tools that should NOT have their results evicted to the filesystem
* when they exceed token limits. Tools are excluded for different reasons:
*
* 1. Tools with built-in truncation (ls, glob, grep):
*    These tools truncate their own output when it becomes too large. When these tools
*    produce truncated output due to many matches, it typically indicates the query
*    needs refinement rather than full result preservation. In such cases, the truncated
*    matches are potentially more like noise and the LLM should be prompted to narrow
*    its search criteria instead.
*
* 2. Tools with problematic truncation behavior (read_file):
*    read_file is tricky to handle as the failure mode here is single long lines
*    (e.g., imagine a jsonl file with very long payloads on each line). If we try to
*    truncate the result of read_file, the agent may then attempt to re-read the
*    truncated file using read_file again, which won't help.
*
* 3. Tools that never exceed limits (edit_file, write_file):
*    These tools return minimal confirmation messages and are never expected to produce
*    output large enough to exceed token limits, so checking them would be unnecessary.
*/
const TOOLS_EXCLUDED_FROM_EVICTION = [
	"ls",
	"glob",
	"grep",
	"read_file",
	"edit_file",
	"write_file"
];
/**
* Approximate number of characters per token for truncation calculations.
* Using 4 chars per token as a conservative approximation (actual ratio varies by content)
* This errs on the high side to avoid premature eviction of content that might fit.
*/
const NUM_CHARS_PER_TOKEN = 4;
/**
* Message template for evicted tool results.
*/
const TOO_LARGE_TOOL_MSG = `Tool result too large, the result of this tool call {tool_call_id} was saved in the filesystem at this path: {file_path}
You can read the result from the filesystem by using the read_file tool, but make sure to only read part of the result at a time.
You can do this by specifying an offset and limit in the read_file tool call.
For example, to read the first 100 lines, you can use the read_file tool with offset=0 and limit=100.

Here is a preview showing the head and tail of the result (lines of the form
... [N lines truncated] ...
indicate omitted lines in the middle of the content):

{content_sample}`;
/**
* Create a preview of content showing head and tail with truncation marker.
*
* @param contentStr - The full content string to preview.
* @param headLines - Number of lines to show from the start (default: 5).
* @param tailLines - Number of lines to show from the end (default: 5).
* @returns Formatted preview string with line numbers.
*/
function createContentPreview(contentStr, headLines = 5, tailLines = 5) {
	const lines = contentStr.split("\n");
	if (lines.length <= headLines + tailLines) return formatContentWithLineNumbers(lines.map((line) => line.substring(0, 1e3)), 1);
	const head = lines.slice(0, headLines).map((line) => line.substring(0, 1e3));
	const tail = lines.slice(-tailLines).map((line) => line.substring(0, 1e3));
	const headSample = formatContentWithLineNumbers(head, 1);
	const truncationNotice = `\n... [${lines.length - headLines - tailLines} lines truncated] ...\n`;
	const tailSample = formatContentWithLineNumbers(tail, lines.length - tailLines + 1);
	return headSample + truncationNotice + tailSample;
}
/**
* Zod v3 schema for FileData (re-export from backends)
*/
const FileDataSchema = z.object({
	content: z.array(z.string()),
	created_at: z.string(),
	modified_at: z.string()
});
/**
* Merge file updates with support for deletions.
*/
function fileDataReducer(left, right) {
	if (left === void 0) {
		const result$1 = {};
		for (const [key, value] of Object.entries(right)) if (value !== null) result$1[key] = value;
		return result$1;
	}
	const result = { ...left };
	for (const [key, value] of Object.entries(right)) if (value === null) delete result[key];
	else result[key] = value;
	return result;
}
/**
* Shared filesystem state schema.
* Defined at module level to ensure the same object identity is used across all agents,
* preventing "Channel already exists with different type" errors when multiple agents
* use createFilesystemMiddleware.
*/
const FilesystemStateSchema = z.object({ files: z.record(z.string(), FileDataSchema).default({}).meta({ reducer: {
	fn: fileDataReducer,
	schema: z.record(z.string(), FileDataSchema.nullable())
} }) });
/**
* Resolve backend from factory or instance.
*
* @param backend - Backend instance or factory function
* @param stateAndStore - State and store container for backend initialization
*/
function getBackend(backend, stateAndStore) {
	if (typeof backend === "function") return backend(stateAndStore);
	return backend;
}
const FILESYSTEM_SYSTEM_PROMPT = `## Filesystem Tools \`ls\`, \`read_file\`, \`write_file\`, \`edit_file\`, \`glob\`, \`grep\`

You have access to a filesystem which you can interact with using these tools.
All file paths must start with a /.

- ls: list files in a directory (requires absolute path)
- read_file: read a file from the filesystem
- write_file: write to a file in the filesystem
- edit_file: edit a file in the filesystem
- glob: find files matching a pattern (e.g., "**/*.py")
- grep: search for text within files`;
const LS_TOOL_DESCRIPTION = `Lists all files in a directory.

This is useful for exploring the filesystem and finding the right file to read or edit.
You should almost ALWAYS use this tool before using the read_file or edit_file tools.`;
const READ_FILE_TOOL_DESCRIPTION = `Reads a file from the filesystem.

Assume this tool is able to read all files. If the User provides a path to a file assume that path is valid. It is okay to read a file that does not exist; an error will be returned.

Usage:
- By default, it reads up to 500 lines starting from the beginning of the file
- **IMPORTANT for large files and codebase exploration**: Use pagination with offset and limit parameters to avoid context overflow
  - First scan: read_file(path, limit=100) to see file structure
  - Read more sections: read_file(path, offset=100, limit=200) for next 200 lines
  - Only omit limit (read full file) when necessary for editing
- Specify offset and limit: read_file(path, offset=0, limit=100) reads first 100 lines
- Results are returned using cat -n format, with line numbers starting at 1
- Lines longer than 10,000 characters will be split into multiple lines with continuation markers (e.g., 5.1, 5.2, etc.). When you specify a limit, these continuation lines count towards the limit.
- You have the capability to call multiple tools in a single response. It is always better to speculatively read multiple files as a batch that are potentially useful.
- If you read a file that exists but has empty contents you will receive a system reminder warning in place of file contents.
- You should ALWAYS make sure a file has been read before editing it.`;
const WRITE_FILE_TOOL_DESCRIPTION = `Writes to a new file in the filesystem.

Usage:
- The write_file tool will create a new file.
- Prefer to edit existing files (with the edit_file tool) over creating new ones when possible.`;
const EDIT_FILE_TOOL_DESCRIPTION = `Performs exact string replacements in files.

Usage:
- You must read the file before editing. This tool will error if you attempt an edit without reading the file first.
- When editing, preserve the exact indentation (tabs/spaces) from the read output. Never include line number prefixes in old_string or new_string.
- ALWAYS prefer editing existing files over creating new ones.
- Only use emojis if the user explicitly requests it.`;
const GLOB_TOOL_DESCRIPTION = `Find files matching a glob pattern.

Supports standard glob patterns: \`*\` (any characters), \`**\` (any directories), \`?\` (single character).
Returns a list of absolute file paths that match the pattern.

Examples:
- \`**/*.py\` - Find all Python files
- \`*.txt\` - Find all text files in root
- \`/subdir/**/*.md\` - Find all markdown files under /subdir`;
const GREP_TOOL_DESCRIPTION = `Search for a text pattern across files.

Searches for literal text (not regex) and returns matching files or content based on output_mode.

Examples:
- Search all files: \`grep(pattern="TODO")\`
- Search Python files only: \`grep(pattern="import", glob="*.py")\`
- Show matching lines: \`grep(pattern="error", output_mode="content")\``;
const EXECUTE_TOOL_DESCRIPTION = `Executes a shell command in an isolated sandbox environment.

Usage:
Executes a given command in the sandbox environment with proper handling and security measures.
Before executing the command, please follow these steps:

1. Directory Verification:
   - If the command will create new directories or files, first use the ls tool to verify the parent directory exists and is the correct location
   - For example, before running "mkdir foo/bar", first use ls to check that "foo" exists and is the intended parent directory

2. Command Execution:
   - Always quote file paths that contain spaces with double quotes (e.g., cd "path with spaces/file.txt")
   - Examples of proper quoting:
     - cd "/Users/name/My Documents" (correct)
     - cd /Users/name/My Documents (incorrect - will fail)
     - python "/path/with spaces/script.py" (correct)
     - python /path/with spaces/script.py (incorrect - will fail)
   - After ensuring proper quoting, execute the command
   - Capture the output of the command

Usage notes:
  - Commands run in an isolated sandbox environment
  - Returns combined stdout/stderr output with exit code
  - If the output is very large, it may be truncated
  - VERY IMPORTANT: You MUST avoid using search commands like find and grep. Instead use the grep, glob tools to search. You MUST avoid read tools like cat, head, tail, and use read_file to read files.
  - When issuing multiple commands, use the ';' or '&&' operator to separate them. DO NOT use newlines (newlines are ok in quoted strings)
    - Use '&&' when commands depend on each other (e.g., "mkdir dir && cd dir")
    - Use ';' only when you need to run commands sequentially but don't care if earlier commands fail
  - Try to maintain your current working directory throughout the session by using absolute paths and avoiding usage of cd

Examples:
  Good examples:
    - execute(command="pytest /foo/bar/tests")
    - execute(command="python /path/to/script.py")
    - execute(command="npm install && npm test")

  Bad examples (avoid these):
    - execute(command="cd /foo/bar && pytest tests")  # Use absolute path instead
    - execute(command="cat file.txt")  # Use read_file tool instead
    - execute(command="find . -name '*.py'")  # Use glob tool instead
    - execute(command="grep -r 'pattern' .")  # Use grep tool instead

Note: This tool is only available if the backend supports execution (SandboxBackendProtocol).
If execution is not supported, the tool will return an error message.`;
const EXECUTION_SYSTEM_PROMPT = `## Execute Tool \`execute\`

You have access to an \`execute\` tool for running shell commands in a sandboxed environment.
Use this tool to run commands, scripts, tests, builds, and other shell operations.

- execute: run a shell command in the sandbox (returns output and exit code)`;
/**
* Create ls tool using backend.
*/
function createLsTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const path$2 = input.path || "/";
		const infos = await resolvedBackend.lsInfo(path$2);
		if (infos.length === 0) return `No files found in ${path$2}`;
		const lines = [];
		for (const info of infos) if (info.is_dir) lines.push(`${info.path} (directory)`);
		else {
			const size = info.size ? ` (${info.size} bytes)` : "";
			lines.push(`${info.path}${size}`);
		}
		return lines.join("\n");
	}, {
		name: "ls",
		description: customDescription || LS_TOOL_DESCRIPTION,
		schema: z.object({ path: z.string().optional().default("/").describe("Directory path to list (default: /)") })
	});
}
/**
* Create read_file tool using backend.
*/
function createReadFileTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, offset = 0, limit: limit$1 = 500 } = input;
		return await resolvedBackend.read(file_path, offset, limit$1);
	}, {
		name: "read_file",
		description: customDescription || READ_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to read"),
			offset: z.coerce.number().optional().default(0).describe("Line offset to start reading from (0-indexed)"),
			limit: z.coerce.number().optional().default(500).describe("Maximum number of lines to read")
		})
	});
}
/**
* Create write_file tool using backend.
*/
function createWriteFileTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, content } = input;
		const result = await resolvedBackend.write(file_path, content);
		if (result.error) return result.error;
		const message = new ToolMessage({
			content: `Successfully wrote to '${file_path}'`,
			tool_call_id: config.toolCall?.id,
			name: "write_file",
			metadata: result.metadata
		});
		if (result.filesUpdate) return new Command({ update: {
			files: result.filesUpdate,
			messages: [message]
		} });
		return message;
	}, {
		name: "write_file",
		description: customDescription || WRITE_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to write"),
			content: z.string().describe("Content to write to the file")
		})
	});
}
/**
* Create edit_file tool using backend.
*/
function createEditFileTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { file_path, old_string, new_string, replace_all = false } = input;
		const result = await resolvedBackend.edit(file_path, old_string, new_string, replace_all);
		if (result.error) return result.error;
		const message = new ToolMessage({
			content: `Successfully replaced ${result.occurrences} occurrence(s) in '${file_path}'`,
			tool_call_id: config.toolCall?.id,
			name: "edit_file",
			metadata: result.metadata
		});
		if (result.filesUpdate) return new Command({ update: {
			files: result.filesUpdate,
			messages: [message]
		} });
		return message;
	}, {
		name: "edit_file",
		description: customDescription || EDIT_FILE_TOOL_DESCRIPTION,
		schema: z.object({
			file_path: z.string().describe("Absolute path to the file to edit"),
			old_string: z.string().describe("String to be replaced (must match exactly)"),
			new_string: z.string().describe("String to replace with"),
			replace_all: z.boolean().optional().default(false).describe("Whether to replace all occurrences")
		})
	});
}
/**
* Create glob tool using backend.
*/
function createGlobTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { pattern, path: path$2 = "/" } = input;
		const infos = await resolvedBackend.globInfo(pattern, path$2);
		if (infos.length === 0) return `No files found matching pattern '${pattern}'`;
		return infos.map((info) => info.path).join("\n");
	}, {
		name: "glob",
		description: customDescription || GLOB_TOOL_DESCRIPTION,
		schema: z.object({
			pattern: z.string().describe("Glob pattern (e.g., '*.py', '**/*.ts')"),
			path: z.string().optional().default("/").describe("Base path to search from (default: /)")
		})
	});
}
/**
* Create grep tool using backend.
*/
function createGrepTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		const { pattern, path: path$2 = "/", glob = null } = input;
		const result = await resolvedBackend.grepRaw(pattern, path$2, glob);
		if (typeof result === "string") return result;
		if (result.length === 0) return `No matches found for pattern '${pattern}'`;
		const lines = [];
		let currentFile = null;
		for (const match of result) {
			if (match.path !== currentFile) {
				currentFile = match.path;
				lines.push(`\n${currentFile}:`);
			}
			lines.push(`  ${match.line}: ${match.text}`);
		}
		return lines.join("\n");
	}, {
		name: "grep",
		description: customDescription || GREP_TOOL_DESCRIPTION,
		schema: z.object({
			pattern: z.string().describe("Regex pattern to search for"),
			path: z.string().optional().default("/").describe("Base path to search from (default: /)"),
			glob: z.string().optional().nullable().describe("Optional glob pattern to filter files (e.g., '*.py')")
		})
	});
}
/**
* Create execute tool using backend.
*/
function createExecuteTool(backend, options) {
	const { customDescription } = options;
	return tool(async (input, config) => {
		const resolvedBackend = getBackend(backend, {
			state: getCurrentTaskInput(config),
			store: config.store
		});
		if (!isSandboxBackend(resolvedBackend)) return "Error: Execution not available. This agent's backend does not support command execution (SandboxBackendProtocol). To use the execute tool, provide a backend that implements SandboxBackendProtocol.";
		const result = await resolvedBackend.execute(input.command);
		const parts = [result.output];
		if (result.exitCode !== null) {
			const status = result.exitCode === 0 ? "succeeded" : "failed";
			parts.push(`\n[Command ${status} with exit code ${result.exitCode}]`);
		}
		if (result.truncated) parts.push("\n[Output was truncated due to size limits]");
		return parts.join("");
	}, {
		name: "execute",
		description: customDescription || EXECUTE_TOOL_DESCRIPTION,
		schema: z.object({ command: z.string().describe("The shell command to execute") })
	});
}
/**
* Create filesystem middleware with all tools and features.
*/
function createFilesystemMiddleware(options = {}) {
	const { backend = (stateAndStore) => new StateBackend(stateAndStore), systemPrompt: customSystemPrompt = null, customToolDescriptions = null, toolTokenLimitBeforeEvict = 2e4 } = options;
	const baseSystemPrompt = customSystemPrompt || FILESYSTEM_SYSTEM_PROMPT;
	return createMiddleware({
		name: "FilesystemMiddleware",
		stateSchema: FilesystemStateSchema,
		tools: [
			createLsTool(backend, { customDescription: customToolDescriptions?.ls }),
			createReadFileTool(backend, { customDescription: customToolDescriptions?.read_file }),
			createWriteFileTool(backend, { customDescription: customToolDescriptions?.write_file }),
			createEditFileTool(backend, { customDescription: customToolDescriptions?.edit_file }),
			createGlobTool(backend, { customDescription: customToolDescriptions?.glob }),
			createGrepTool(backend, { customDescription: customToolDescriptions?.grep }),
			createExecuteTool(backend, { customDescription: customToolDescriptions?.execute })
		],
		wrapModelCall: async (request, handler) => {
			const supportsExecution = isSandboxBackend(getBackend(backend, {
				state: request.state || {},
				store: request.config?.store
			}));
			let tools = request.tools;
			if (!supportsExecution) tools = tools.filter((t) => t.name !== "execute");
			let systemPrompt = baseSystemPrompt;
			if (supportsExecution) systemPrompt = `${systemPrompt}\n\n${EXECUTION_SYSTEM_PROMPT}`;
			const currentSystemPrompt = request.systemPrompt || "";
			const newSystemPrompt = currentSystemPrompt ? `${currentSystemPrompt}\n\n${systemPrompt}` : systemPrompt;
			return handler({
				...request,
				tools,
				systemPrompt: newSystemPrompt
			});
		},
		wrapToolCall: async (request, handler) => {
			if (!toolTokenLimitBeforeEvict) return handler(request);
			const toolName = request.toolCall?.name;
			if (toolName && TOOLS_EXCLUDED_FROM_EVICTION.includes(toolName)) return handler(request);
			const result = await handler(request);
			async function processToolMessage(msg, toolTokenLimitBeforeEvict$1) {
				if (typeof msg.content === "string" && msg.content.length > toolTokenLimitBeforeEvict$1 * NUM_CHARS_PER_TOKEN) {
					const resolvedBackend = getBackend(backend, {
						state: request.state || {},
						store: request.config?.store
					});
					const evictPath = `/large_tool_results/${sanitizeToolCallId(request.toolCall?.id || msg.tool_call_id)}`;
					const writeResult = await resolvedBackend.write(evictPath, msg.content);
					if (writeResult.error) return {
						message: msg,
						filesUpdate: null
					};
					const contentSample = createContentPreview(msg.content);
					return {
						message: new ToolMessage({
							content: TOO_LARGE_TOOL_MSG.replace("{tool_call_id}", msg.tool_call_id).replace("{file_path}", evictPath).replace("{content_sample}", contentSample),
							tool_call_id: msg.tool_call_id,
							name: msg.name
						}),
						filesUpdate: writeResult.filesUpdate
					};
				}
				return {
					message: msg,
					filesUpdate: null
				};
			}
			if (ToolMessage.isInstance(result)) {
				const processed = await processToolMessage(result, toolTokenLimitBeforeEvict);
				if (processed.filesUpdate) return new Command({ update: {
					files: processed.filesUpdate,
					messages: [processed.message]
				} });
				return processed.message;
			}
			if (isCommand(result)) {
				const update = result.update;
				if (!update?.messages) return result;
				let hasLargeResults = false;
				const accumulatedFiles = update.files ? { ...update.files } : {};
				const processedMessages = [];
				for (const msg of update.messages) if (ToolMessage.isInstance(msg)) {
					const processed = await processToolMessage(msg, toolTokenLimitBeforeEvict);
					processedMessages.push(processed.message);
					if (processed.filesUpdate) {
						hasLargeResults = true;
						Object.assign(accumulatedFiles, processed.filesUpdate);
					}
				} else processedMessages.push(msg);
				if (hasLargeResults) return new Command({ update: {
					...update,
					messages: processedMessages,
					files: accumulatedFiles
				} });
			}
			return result;
		}
	});
}

//#endregion
//#region src/middleware/subagents.ts
const DEFAULT_SUBAGENT_PROMPT = "In order to complete the objective that the user asks of you, you have access to a number of standard tools.";
const EXCLUDED_STATE_KEYS = [
	"messages",
	"todos",
	"structuredResponse",
	"files"
];
const DEFAULT_GENERAL_PURPOSE_DESCRIPTION = "General-purpose agent for researching complex questions, searching for files and content, and executing multi-step tasks. When you are searching for a keyword or file and are not confident that you will find the right match in the first few tries use this agent to perform the search for you. This agent has access to all tools as the main agent.";
function getTaskToolDescription(subagentDescriptions) {
	return `
Launch an ephemeral subagent to handle complex, multi-step independent tasks with isolated context windows.

Available agent types and the tools they have access to:
${subagentDescriptions.join("\n")}

When using the Task tool, you must specify a subagent_type parameter to select which agent type to use.

## Usage notes:
1. Launch multiple agents concurrently whenever possible, to maximize performance; to do that, use a single message with multiple tool uses
2. When the agent is done, it will return a single message back to you. The result returned by the agent is not visible to the user. To show the user the result, you should send a text message back to the user with a concise summary of the result.
3. Each agent invocation is stateless. You will not be able to send additional messages to the agent, nor will the agent be able to communicate with you outside of its final report. Therefore, your prompt should contain a highly detailed task description for the agent to perform autonomously and you should specify exactly what information the agent should return back to you in its final and only message to you.
4. The agent's outputs should generally be trusted
5. Clearly tell the agent whether you expect it to create content, perform analysis, or just do research (search, file reads, web fetches, etc.), since it is not aware of the user's intent
6. If the agent description mentions that it should be used proactively, then you should try your best to use it without the user having to ask for it first. Use your judgement.
7. When only the general-purpose agent is provided, you should use it for all tasks. It is great for isolating context and token usage, and completing specific, complex tasks, as it has all the same capabilities as the main agent.

### Example usage of the general-purpose agent:

<example_agent_descriptions>
"general-purpose": use this agent for general purpose tasks, it has access to all tools as the main agent.
</example_agent_descriptions>

<example>
User: "I want to conduct research on the accomplishments of Lebron James, Michael Jordan, and Kobe Bryant, and then compare them."
Assistant: *Uses the task tool in parallel to conduct isolated research on each of the three players*
Assistant: *Synthesizes the results of the three isolated research tasks and responds to the User*
<commentary>
Research is a complex, multi-step task in it of itself.
The research of each individual player is not dependent on the research of the other players.
The assistant uses the task tool to break down the complex objective into three isolated tasks.
Each research task only needs to worry about context and tokens about one player, then returns synthesized information about each player as the Tool Result.
This means each research task can dive deep and spend tokens and context deeply researching each player, but the final result is synthesized information, and saves us tokens in the long run when comparing the players to each other.
</commentary>
</example>

<example>
User: "Analyze a single large code repository for security vulnerabilities and generate a report."
Assistant: *Launches a single \`task\` subagent for the repository analysis*
Assistant: *Receives report and integrates results into final summary*
<commentary>
Subagent is used to isolate a large, context-heavy task, even though there is only one. This prevents the main thread from being overloaded with details.
If the user then asks followup questions, we have a concise report to reference instead of the entire history of analysis and tool calls, which is good and saves us time and money.
</commentary>
</example>

<example>
User: "Schedule two meetings for me and prepare agendas for each."
Assistant: *Calls the task tool in parallel to launch two \`task\` subagents (one per meeting) to prepare agendas*
Assistant: *Returns final schedules and agendas*
<commentary>
Tasks are simple individually, but subagents help silo agenda preparation.
Each subagent only needs to worry about the agenda for one meeting.
</commentary>
</example>

<example>
User: "I want to order a pizza from Dominos, order a burger from McDonald's, and order a salad from Subway."
Assistant: *Calls tools directly in parallel to order a pizza from Dominos, a burger from McDonald's, and a salad from Subway*
<commentary>
The assistant did not use the task tool because the objective is super simple and clear and only requires a few trivial tool calls.
It is better to just complete the task directly and NOT use the \`task\`tool.
</commentary>
</example>

### Example usage with custom agents:

<example_agent_descriptions>
"content-reviewer": use this agent after you are done creating significant content or documents
"greeting-responder": use this agent when to respond to user greetings with a friendly joke
"research-analyst": use this agent to conduct thorough research on complex topics
</example_agent_description>

<example>
user: "Please write a function that checks if a number is prime"
assistant: Sure let me write a function that checks if a number is prime
assistant: First let me use the Write tool to write a function that checks if a number is prime
assistant: I'm going to use the Write tool to write the following code:
<code>
function isPrime(n) {
  if (n <= 1) return false
  for (let i = 2; i * i <= n; i++) {
    if (n % i === 0) return false
  }
  return true
}
</code>
<commentary>
Since significant content was created and the task was completed, now use the content-reviewer agent to review the work
</commentary>
assistant: Now let me use the content-reviewer agent to review the code
assistant: Uses the Task tool to launch with the content-reviewer agent
</example>

<example>
user: "Can you help me research the environmental impact of different renewable energy sources and create a comprehensive report?"
<commentary>
This is a complex research task that would benefit from using the research-analyst agent to conduct thorough analysis
</commentary>
assistant: I'll help you research the environmental impact of renewable energy sources. Let me use the research-analyst agent to conduct comprehensive research on this topic.
assistant: Uses the Task tool to launch with the research-analyst agent, providing detailed instructions about what research to conduct and what format the report should take
</example>

<example>
user: "Hello"
<commentary>
Since the user is greeting, use the greeting-responder agent to respond with a friendly joke
</commentary>
assistant: "I'm going to use the Task tool to launch with the greeting-responder agent"
</example>
  `.trim();
}
const TASK_SYSTEM_PROMPT = `## \`task\` (subagent spawner)

You have access to a \`task\` tool to launch short-lived subagents that handle isolated tasks. These agents are ephemeral — they live only for the duration of the task and return a single result.

When to use the task tool:
- When a task is complex and multi-step, and can be fully delegated in isolation
- When a task is independent of other tasks and can run in parallel
- When a task requires focused reasoning or heavy token/context usage that would bloat the orchestrator thread
- When sandboxing improves reliability (e.g. code execution, structured searches, data formatting)
- When you only care about the output of the subagent, and not the intermediate steps (ex. performing a lot of research and then returned a synthesized report, performing a series of computations or lookups to achieve a concise, relevant answer.)

Subagent lifecycle:
1. **Spawn** → Provide clear role, instructions, and expected output
2. **Run** → The subagent completes the task autonomously
3. **Return** → The subagent provides a single structured result
4. **Reconcile** → Incorporate or synthesize the result into the main thread

When NOT to use the task tool:
- If you need to see the intermediate reasoning or steps after the subagent has completed (the task tool hides them)
- If the task is trivial (a few tool calls or simple lookup)
- If delegating does not reduce token usage, complexity, or context switching
- If splitting would add latency without benefit

## Important Task Tool Usage Notes to Remember
- Whenever possible, parallelize the work that you do. This is true for both tool_calls, and for tasks. Whenever you have independent steps to complete - make tool_calls, or kick off tasks (subagents) in parallel to accomplish them faster. This saves time for the user, which is incredibly important.
- Remember to use the \`task\` tool to silo independent tasks within a multi-part objective.
- You should use the \`task\` tool whenever you have a complex task that will take multiple steps, and is independent from other tasks that the agent needs to complete. These agents are highly competent and efficient.`;
/**
* Filter state to exclude certain keys when passing to subagents
*/
function filterStateForSubagent(state) {
	const filtered = {};
	for (const [key, value] of Object.entries(state)) if (!EXCLUDED_STATE_KEYS.includes(key)) filtered[key] = value;
	return filtered;
}
/**
* Create Command with filtered state update from subagent result
*/
function returnCommandWithStateUpdate(result, toolCallId) {
	const stateUpdate = filterStateForSubagent(result);
	const messages = result.messages;
	const lastMessage = messages?.[messages.length - 1];
	return new Command({ update: {
		...stateUpdate,
		messages: [new ToolMessage({
			content: lastMessage?.content || "Task completed",
			tool_call_id: toolCallId,
			name: "task"
		})]
	} });
}
/**
* Create subagent instances from specifications
*/
function getSubagents(options) {
	const { defaultModel, defaultTools, defaultMiddleware, defaultInterruptOn, subagents, generalPurposeAgent } = options;
	const defaultSubagentMiddleware = defaultMiddleware || [];
	const agents = {};
	const subagentDescriptions = [];
	if (generalPurposeAgent) {
		const generalPurposeMiddleware = [...defaultSubagentMiddleware];
		if (defaultInterruptOn) generalPurposeMiddleware.push(humanInTheLoopMiddleware({ interruptOn: defaultInterruptOn }));
		agents["general-purpose"] = createAgent({
			model: defaultModel,
			systemPrompt: DEFAULT_SUBAGENT_PROMPT,
			tools: defaultTools,
			middleware: generalPurposeMiddleware
		});
		subagentDescriptions.push(`- general-purpose: ${DEFAULT_GENERAL_PURPOSE_DESCRIPTION}`);
	}
	for (const agentParams of subagents) {
		subagentDescriptions.push(`- ${agentParams.name}: ${agentParams.description}`);
		if ("runnable" in agentParams) agents[agentParams.name] = agentParams.runnable;
		else {
			const middleware = agentParams.middleware ? [...defaultSubagentMiddleware, ...agentParams.middleware] : [...defaultSubagentMiddleware];
			const interruptOn = agentParams.interruptOn || defaultInterruptOn;
			if (interruptOn) middleware.push(humanInTheLoopMiddleware({ interruptOn }));
			agents[agentParams.name] = createAgent({
				model: agentParams.model ?? defaultModel,
				systemPrompt: agentParams.systemPrompt,
				tools: agentParams.tools ?? defaultTools,
				middleware
			});
		}
	}
	return {
		agents,
		descriptions: subagentDescriptions
	};
}
/**
* Create the task tool for invoking subagents
*/
function createTaskTool(options) {
	const { defaultModel, defaultTools, defaultMiddleware, defaultInterruptOn, subagents, generalPurposeAgent, taskDescription } = options;
	const { agents: subagentGraphs, descriptions: subagentDescriptions } = getSubagents({
		defaultModel,
		defaultTools,
		defaultMiddleware,
		defaultInterruptOn,
		subagents,
		generalPurposeAgent
	});
	return tool(async (input, config) => {
		const { description, subagent_type } = input;
		if (!(subagent_type in subagentGraphs)) {
			const allowedTypes = Object.keys(subagentGraphs).map((k) => `\`${k}\``).join(", ");
			throw new Error(`Error: invoked agent of type ${subagent_type}, the only allowed types are ${allowedTypes}`);
		}
		const subagent = subagentGraphs[subagent_type];
		const subagentState = filterStateForSubagent(getCurrentTaskInput());
		subagentState.messages = [new HumanMessage({ content: description })];
		const result = await subagent.invoke(subagentState, config);
		if (!config.toolCall?.id) throw new Error("Tool call ID is required for subagent invocation");
		return returnCommandWithStateUpdate(result, config.toolCall.id);
	}, {
		name: "task",
		description: taskDescription ? taskDescription : getTaskToolDescription(subagentDescriptions),
		schema: z.object({
			description: z.string().describe("The task to execute with the selected agent"),
			subagent_type: z.string().describe(`Name of the agent to use. Available: ${Object.keys(subagentGraphs).join(", ")}`)
		})
	});
}
/**
* Create subagent middleware with task tool
*/
function createSubAgentMiddleware(options) {
	const { defaultModel, defaultTools = [], defaultMiddleware = null, defaultInterruptOn = null, subagents = [], systemPrompt = TASK_SYSTEM_PROMPT, generalPurposeAgent = true, taskDescription = null } = options;
	return createMiddleware({
		name: "subAgentMiddleware",
		tools: [createTaskTool({
			defaultModel,
			defaultTools,
			defaultMiddleware,
			defaultInterruptOn,
			subagents,
			generalPurposeAgent,
			taskDescription
		})],
		wrapModelCall: async (request, handler) => {
			if (systemPrompt !== null) return handler({
				...request,
				systemMessage: request.systemMessage.concat(new SystemMessage({ content: systemPrompt }))
			});
			return handler(request);
		}
	});
}

//#endregion
//#region src/middleware/patch_tool_calls.ts
/**
* Create middleware that patches dangling tool calls in the messages history.
*
* When an AI message contains tool_calls but subsequent messages don't include
* the corresponding ToolMessage responses, this middleware adds synthetic
* ToolMessages saying the tool call was cancelled.
*
* @returns AgentMiddleware that patches dangling tool calls
*
* @example
* ```typescript
* import { createAgent } from "langchain";
* import { createPatchToolCallsMiddleware } from "./middleware/patch_tool_calls";
*
* const agent = createAgent({
*   model: "claude-sonnet-4-5-20250929",
*   middleware: [createPatchToolCallsMiddleware()],
* });
* ```
*/
function createPatchToolCallsMiddleware() {
	return createMiddleware({
		name: "patchToolCallsMiddleware",
		beforeAgent: async (state) => {
			const messages = state.messages;
			if (!messages || messages.length === 0) return;
			const patchedMessages = [];
			for (let i = 0; i < messages.length; i++) {
				const msg = messages[i];
				patchedMessages.push(msg);
				if (AIMessage.isInstance(msg) && msg.tool_calls != null) {
					for (const toolCall of msg.tool_calls) if (!messages.slice(i).find((m) => ToolMessage.isInstance(m) && m.tool_call_id === toolCall.id)) {
						const toolMsg = `Tool call ${toolCall.name} with id ${toolCall.id} was cancelled - another message came in before it could be completed.`;
						patchedMessages.push(new ToolMessage({
							content: toolMsg,
							name: toolCall.name,
							tool_call_id: toolCall.id
						}));
					}
				}
			}
			return { messages: [new RemoveMessage({ id: REMOVE_ALL_MESSAGES }), ...patchedMessages] };
		}
	});
}

//#endregion
//#region src/middleware/memory.ts
/**
* Middleware for loading agent memory/context from AGENTS.md files.
*
* This module implements support for the AGENTS.md specification (https://agents.md/),
* loading memory/context from configurable sources and injecting into the system prompt.
*
* ## Overview
*
* AGENTS.md files provide project-specific context and instructions to help AI agents
* work effectively. Unlike skills (which are on-demand workflows), memory is always
* loaded and provides persistent context.
*
* ## Usage
*
* ```typescript
* import { createMemoryMiddleware } from "@anthropic/deepagents";
* import { FilesystemBackend } from "@anthropic/deepagents";
*
* // Security: FilesystemBackend allows reading/writing from the entire filesystem.
* // Either ensure the agent is running within a sandbox OR add human-in-the-loop (HIL)
* // approval to file operations.
* const backend = new FilesystemBackend({ rootDir: "/" });
*
* const middleware = createMemoryMiddleware({
*   backend,
*   sources: [
*     "~/.deepagents/AGENTS.md",
*     "./.deepagents/AGENTS.md",
*   ],
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* ## Memory Sources
*
* Sources are simply paths to AGENTS.md files that are loaded in order and combined.
* Multiple sources are concatenated in order, with all content included.
* Later sources appear after earlier ones in the combined prompt.
*
* ## File Format
*
* AGENTS.md files are standard Markdown with no required structure.
* Common sections include:
* - Project overview
* - Build/test commands
* - Code style guidelines
* - Architecture notes
*/
/**
* State schema for memory middleware.
*/
const MemoryStateSchema = z$1.object({ memoryContents: z$1.record(z$1.string(), z$1.string()).optional() });
/**
* Default system prompt template for memory.
* Ported from Python's comprehensive memory guidelines.
*/
const MEMORY_SYSTEM_PROMPT = `<agent_memory>
{memory_contents}
</agent_memory>

<memory_guidelines>
    The above <agent_memory> was loaded in from files in your filesystem. As you learn from your interactions with the user, you can save new knowledge by calling the \`edit_file\` tool.

    **Learning from feedback:**
    - One of your MAIN PRIORITIES is to learn from your interactions with the user. These learnings can be implicit or explicit. This means that in the future, you will remember this important information.
    - When you need to remember something, updating memory must be your FIRST, IMMEDIATE action - before responding to the user, before calling other tools, before doing anything else. Just update memory immediately.
    - When user says something is better/worse, capture WHY and encode it as a pattern.
    - Each correction is a chance to improve permanently - don't just fix the immediate issue, update your instructions.
    - A great opportunity to update your memories is when the user interrupts a tool call and provides feedback. You should update your memories immediately before revising the tool call.
    - Look for the underlying principle behind corrections, not just the specific mistake.
    - The user might not explicitly ask you to remember something, but if they provide information that is useful for future use, you should update your memories immediately.

    **Asking for information:**
    - If you lack context to perform an action (e.g. send a Slack DM, requires a user ID/email) you should explicitly ask the user for this information.
    - It is preferred for you to ask for information, don't assume anything that you do not know!
    - When the user provides information that is useful for future use, you should update your memories immediately.

    **When to update memories:**
    - When the user explicitly asks you to remember something (e.g., "remember my email", "save this preference")
    - When the user describes your role or how you should behave (e.g., "you are a web researcher", "always do X")
    - When the user gives feedback on your work - capture what was wrong and how to improve
    - When the user provides information required for tool use (e.g., slack channel ID, email addresses)
    - When the user provides context useful for future tasks, such as how to use tools, or which actions to take in a particular situation
    - When you discover new patterns or preferences (coding styles, conventions, workflows)

    **When to NOT update memories:**
    - When the information is temporary or transient (e.g., "I'm running late", "I'm on my phone right now")
    - When the information is a one-time task request (e.g., "Find me a recipe", "What's 25 * 4?")
    - When the information is a simple question that doesn't reveal lasting preferences (e.g., "What day is it?", "Can you explain X?")
    - When the information is an acknowledgment or small talk (e.g., "Sounds good!", "Hello", "Thanks for that")
    - When the information is stale or irrelevant in future conversations
    - Never store API keys, access tokens, passwords, or any other credentials in any file, memory, or system prompt.
    - If the user asks where to put API keys or provides an API key, do NOT echo or save it.

    **Examples:**
    Example 1 (remembering user information):
    User: Can you connect to my google account?
    Agent: Sure, I'll connect to your google account, what's your google account email?
    User: john@example.com
    Agent: Let me save this to my memory.
    Tool Call: edit_file(...) -> remembers that the user's google account email is john@example.com

    Example 2 (remembering implicit user preferences):
    User: Can you write me an example for creating a deep agent in LangChain?
    Agent: Sure, I'll write you an example for creating a deep agent in LangChain <example code in Python>
    User: Can you do this in JavaScript
    Agent: Let me save this to my memory.
    Tool Call: edit_file(...) -> remembers that the user prefers to get LangChain code examples in JavaScript
    Agent: Sure, here is the JavaScript example<example code in JavaScript>

    Example 3 (do not remember transient information):
    User: I'm going to play basketball tonight so I will be offline for a few hours.
    Agent: Okay I'll add a block to your calendar.
    Tool Call: create_calendar_event(...) -> just calls a tool, does not commit anything to memory, as it is transient information
</memory_guidelines>`;
/**
* Format loaded memory contents for injection into prompt.
* Pairs memory locations with their contents for clarity.
*/
function formatMemoryContents(contents, sources) {
	if (Object.keys(contents).length === 0) return "(No memory loaded)";
	const sections = [];
	for (const path$2 of sources) if (contents[path$2]) sections.push(`${path$2}\n${contents[path$2]}`);
	if (sections.length === 0) return "(No memory loaded)";
	return sections.join("\n\n");
}
/**
* Load memory content from a backend path.
*
* @param backend - Backend to load from.
* @param path - Path to the AGENTS.md file.
* @returns File content if found, null otherwise.
*/
async function loadMemoryFromBackend(backend, path$2) {
	if (!backend.downloadFiles) {
		const content = await backend.read(path$2);
		if (content.startsWith("Error:")) return null;
		return content;
	}
	const results = await backend.downloadFiles([path$2]);
	if (results.length !== 1) throw new Error(`Expected 1 response for path ${path$2}, got ${results.length}`);
	const response = results[0];
	if (response.error != null) {
		if (response.error === "file_not_found") return null;
		throw new Error(`Failed to download ${path$2}: ${response.error}`);
	}
	if (response.content != null) return new TextDecoder().decode(response.content);
	return null;
}
/**
* Create middleware for loading agent memory from AGENTS.md files.
*
* Loads memory content from configured sources and injects into the system prompt.
* Supports multiple sources that are combined together.
*
* @param options - Configuration options
* @returns AgentMiddleware for memory loading and injection
*
* @example
* ```typescript
* const middleware = createMemoryMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     "~/.deepagents/AGENTS.md",
*     "./.deepagents/AGENTS.md",
*   ],
* });
* ```
*/
function createMemoryMiddleware(options) {
	const { backend, sources } = options;
	/**
	* Resolve backend from instance or factory.
	*/
	function getBackend$1(state) {
		if (typeof backend === "function") return backend({ state });
		return backend;
	}
	return createMiddleware({
		name: "MemoryMiddleware",
		stateSchema: MemoryStateSchema,
		async beforeAgent(state) {
			if ("memoryContents" in state && state.memoryContents != null) return;
			const resolvedBackend = getBackend$1(state);
			const contents = {};
			for (const path$2 of sources) try {
				const content = await loadMemoryFromBackend(resolvedBackend, path$2);
				if (content) contents[path$2] = content;
			} catch (error) {
				console.debug(`Failed to load memory from ${path$2}:`, error);
			}
			return { memoryContents: contents };
		},
		wrapModelCall(request, handler) {
			const formattedContents = formatMemoryContents(request.state?.memoryContents || {}, sources);
			const memorySection = MEMORY_SYSTEM_PROMPT.replace("{memory_contents}", formattedContents);
			const currentSystemPrompt = request.systemPrompt || "";
			const newSystemPrompt = currentSystemPrompt ? `${memorySection}\n\n${currentSystemPrompt}` : memorySection;
			return handler({
				...request,
				systemPrompt: newSystemPrompt
			});
		}
	});
}

//#endregion
//#region src/middleware/skills.ts
/**
* Backend-agnostic skills middleware for loading agent skills from any backend.
*
* This middleware implements Anthropic's agent skills pattern with progressive disclosure,
* loading skills from backend storage via configurable sources.
*
* ## Architecture
*
* Skills are loaded from one or more **sources** - paths in a backend where skills are
* organized. Sources are loaded in order, with later sources overriding earlier ones
* when skills have the same name (last one wins). This enables layering: base -> user
* -> project -> team skills.
*
* The middleware uses backend APIs exclusively (no direct filesystem access), making it
* portable across different storage backends (filesystem, state, remote storage, etc.).
*
* ## Usage
*
* ```typescript
* import { createSkillsMiddleware, FilesystemBackend } from "@anthropic/deepagents";
*
* const middleware = createSkillsMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     "/skills/user/",
*     "/skills/project/",
*   ],
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* Or use the `skills` parameter on createDeepAgent:
*
* ```typescript
* const agent = createDeepAgent({
*   skills: ["/skills/user/", "/skills/project/"],
* });
* ```
*/
const MAX_SKILL_FILE_SIZE = 10 * 1024 * 1024;
const MAX_SKILL_NAME_LENGTH = 64;
const MAX_SKILL_DESCRIPTION_LENGTH = 1024;
/**
* State schema for skills middleware.
*/
const SkillsStateSchema = z$1.object({ skillsMetadata: z$1.array(z$1.object({
	name: z$1.string(),
	description: z$1.string(),
	path: z$1.string(),
	license: z$1.string().nullable().optional(),
	compatibility: z$1.string().nullable().optional(),
	metadata: z$1.record(z$1.string(), z$1.string()).optional(),
	allowedTools: z$1.array(z$1.string()).optional()
})).optional() });
/**
* Skills System Documentation prompt template.
*/
const SKILLS_SYSTEM_PROMPT = `
## Skills System

You have access to a skills library that provides specialized capabilities and domain knowledge.

{skills_locations}

**Available Skills:**

{skills_list}

**How to Use Skills (Progressive Disclosure):**

Skills follow a **progressive disclosure** pattern - you know they exist (name + description above), but you only read the full instructions when needed:

1. **Recognize when a skill applies**: Check if the user's task matches any skill's description
2. **Read the skill's full instructions**: The skill list above shows the exact path to use with read_file
3. **Follow the skill's instructions**: SKILL.md contains step-by-step workflows, best practices, and examples
4. **Access supporting files**: Skills may include Python scripts, configs, or reference docs - use absolute paths

**When to Use Skills:**
- When the user's request matches a skill's domain (e.g., "research X" → web-research skill)
- When you need specialized knowledge or structured workflows
- When a skill provides proven patterns for complex tasks

**Skills are Self-Documenting:**
- Each SKILL.md tells you exactly what the skill does and how to use it
- The skill list above shows the full path for each skill's SKILL.md file

**Executing Skill Scripts:**
Skills may contain Python scripts or other executable files. Always use absolute paths from the skill list.

**Example Workflow:**

User: "Can you research the latest developments in quantum computing?"

1. Check available skills above → See "web-research" skill with its full path
2. Read the skill using the path shown in the list
3. Follow the skill's research workflow (search → organize → synthesize)
4. Use any helper scripts with absolute paths

Remember: Skills are tools to make you more capable and consistent. When in doubt, check if a skill exists for the task!
`;
/**
* Validate skill name per Agent Skills specification.
*/
function validateSkillName$1(name, directoryName) {
	if (!name) return {
		valid: false,
		error: "name is required"
	};
	if (name.length > MAX_SKILL_NAME_LENGTH) return {
		valid: false,
		error: "name exceeds 64 characters"
	};
	if (!/^[a-z0-9]+(-[a-z0-9]+)*$/.test(name)) return {
		valid: false,
		error: "name must be lowercase alphanumeric with single hyphens only"
	};
	if (name !== directoryName) return {
		valid: false,
		error: `name '${name}' must match directory name '${directoryName}'`
	};
	return {
		valid: true,
		error: ""
	};
}
/**
* Parse YAML frontmatter from SKILL.md content.
*/
function parseSkillMetadataFromContent(content, skillPath, directoryName) {
	if (content.length > MAX_SKILL_FILE_SIZE) {
		console.warn(`Skipping ${skillPath}: content too large (${content.length} bytes)`);
		return null;
	}
	const match = content.match(/^---\s*\n([\s\S]*?)\n---\s*\n/);
	if (!match) {
		console.warn(`Skipping ${skillPath}: no valid YAML frontmatter found`);
		return null;
	}
	const frontmatterStr = match[1];
	let frontmatterData;
	try {
		frontmatterData = yaml.parse(frontmatterStr);
	} catch (e) {
		console.warn(`Invalid YAML in ${skillPath}:`, e);
		return null;
	}
	if (!frontmatterData || typeof frontmatterData !== "object") {
		console.warn(`Skipping ${skillPath}: frontmatter is not a mapping`);
		return null;
	}
	const name = frontmatterData.name;
	const description = frontmatterData.description;
	if (!name || !description) {
		console.warn(`Skipping ${skillPath}: missing required 'name' or 'description'`);
		return null;
	}
	const validation = validateSkillName$1(String(name), directoryName);
	if (!validation.valid) console.warn(`Skill '${name}' in ${skillPath} does not follow Agent Skills specification: ${validation.error}. Consider renaming for spec compliance.`);
	let descriptionStr = String(description).trim();
	if (descriptionStr.length > MAX_SKILL_DESCRIPTION_LENGTH) {
		console.warn(`Description exceeds ${MAX_SKILL_DESCRIPTION_LENGTH} characters in ${skillPath}, truncating`);
		descriptionStr = descriptionStr.slice(0, MAX_SKILL_DESCRIPTION_LENGTH);
	}
	const allowedToolsStr = frontmatterData["allowed-tools"];
	const allowedTools = allowedToolsStr ? allowedToolsStr.split(" ") : [];
	return {
		name: String(name),
		description: descriptionStr,
		path: skillPath,
		metadata: frontmatterData.metadata || {},
		license: typeof frontmatterData.license === "string" ? frontmatterData.license.trim() || null : null,
		compatibility: typeof frontmatterData.compatibility === "string" ? frontmatterData.compatibility.trim() || null : null,
		allowedTools
	};
}
/**
* List all skills from a backend source.
*/
async function listSkillsFromBackend(backend, sourcePath) {
	const skills = [];
	const normalizedPath = sourcePath.endsWith("/") ? sourcePath : `${sourcePath}/`;
	let fileInfos;
	try {
		fileInfos = await backend.lsInfo(normalizedPath);
	} catch {
		return [];
	}
	const entries = fileInfos.map((info) => ({
		name: info.path.replace(/\/$/, "").split("/").pop() || "",
		type: info.is_dir ? "directory" : "file"
	}));
	for (const entry of entries) {
		if (entry.type !== "directory") continue;
		const skillMdPath = `${normalizedPath}${entry.name}/SKILL.md`;
		let content;
		if (backend.downloadFiles) {
			const results = await backend.downloadFiles([skillMdPath]);
			if (results.length !== 1) continue;
			const response = results[0];
			if (response.error != null || response.content == null) continue;
			content = new TextDecoder().decode(response.content);
		} else {
			const readResult = await backend.read(skillMdPath);
			if (readResult.startsWith("Error:")) continue;
			content = readResult;
		}
		const metadata = parseSkillMetadataFromContent(content, skillMdPath, entry.name);
		if (metadata) skills.push(metadata);
	}
	return skills;
}
/**
* Format skills locations for display in system prompt.
* Shows priority indicator for the last source (highest priority).
*/
function formatSkillsLocations(sources) {
	if (sources.length === 0) return "**Skills Sources:** None configured";
	const lines = [];
	for (let i = 0; i < sources.length; i++) {
		const sourcePath = sources[i];
		const name = sourcePath.replace(/\/$/, "").split("/").filter(Boolean).pop()?.replace(/^./, (c) => c.toUpperCase()) || "Skills";
		const suffix = i === sources.length - 1 ? " (higher priority)" : "";
		lines.push(`**${name} Skills**: \`${sourcePath}\`${suffix}`);
	}
	return lines.join("\n");
}
/**
* Format skills metadata for display in system prompt.
* Shows allowed tools for each skill if specified.
*/
function formatSkillsList(skills, sources) {
	if (skills.length === 0) return `(No skills available yet. You can create skills in ${sources.map((s) => `\`${s}\``).join(" or ")})`;
	const lines = [];
	for (const skill of skills) {
		lines.push(`- **${skill.name}**: ${skill.description}`);
		if (skill.allowedTools && skill.allowedTools.length > 0) lines.push(`  → Allowed tools: ${skill.allowedTools.join(", ")}`);
		lines.push(`  → Read \`${skill.path}\` for full instructions`);
	}
	return lines.join("\n");
}
/**
* Create backend-agnostic middleware for loading and exposing agent skills.
*
* This middleware loads skills from configurable backend sources and injects
* skill metadata into the system prompt. It implements the progressive disclosure
* pattern: skill names and descriptions are shown in the prompt, but the agent
* reads full SKILL.md content only when needed.
*
* @param options - Configuration options
* @returns AgentMiddleware for skills loading and injection
*
* @example
* ```typescript
* const middleware = createSkillsMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: ["/skills/user/", "/skills/project/"],
* });
* ```
*/
function createSkillsMiddleware(options) {
	const { backend, sources } = options;
	let loadedSkills = [];
	/**
	* Resolve backend from instance or factory.
	*/
	function getBackend$1(state) {
		if (typeof backend === "function") return backend({ state });
		return backend;
	}
	return createMiddleware({
		name: "SkillsMiddleware",
		stateSchema: SkillsStateSchema,
		async beforeAgent(state) {
			if (loadedSkills.length > 0) return;
			if ("skillsMetadata" in state && state.skillsMetadata != null) {
				loadedSkills = state.skillsMetadata;
				return;
			}
			const resolvedBackend = getBackend$1(state);
			const allSkills = /* @__PURE__ */ new Map();
			for (const sourcePath of sources) try {
				const skills = await listSkillsFromBackend(resolvedBackend, sourcePath);
				for (const skill of skills) allSkills.set(skill.name, skill);
			} catch (error) {
				console.debug(`[BackendSkillsMiddleware] Failed to load skills from ${sourcePath}:`, error);
			}
			loadedSkills = Array.from(allSkills.values());
			return { skillsMetadata: loadedSkills };
		},
		wrapModelCall(request, handler) {
			const skillsMetadata = loadedSkills.length > 0 ? loadedSkills : request.state?.skillsMetadata || [];
			const skillsLocations = formatSkillsLocations(sources);
			const skillsList = formatSkillsList(skillsMetadata, sources);
			const skillsSection = SKILLS_SYSTEM_PROMPT.replace("{skills_locations}", skillsLocations).replace("{skills_list}", skillsList);
			const currentSystemPrompt = request.systemPrompt || "";
			const newSystemPrompt = currentSystemPrompt ? `${currentSystemPrompt}\n\n${skillsSection}` : skillsSection;
			return handler({
				...request,
				systemPrompt: newSystemPrompt
			});
		}
	});
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/tslib.mjs
function __classPrivateFieldSet(receiver, state, value, kind, f) {
	if (kind === "m") throw new TypeError("Private method is not writable");
	if (kind === "a" && !f) throw new TypeError("Private accessor was defined without a setter");
	if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError("Cannot write private member to an object whose class did not declare it");
	return kind === "a" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value), value;
}
function __classPrivateFieldGet(receiver, state, kind, f) {
	if (kind === "a" && !f) throw new TypeError("Private accessor was defined without a getter");
	if (typeof state === "function" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError("Cannot read private member from an object whose class did not declare it");
	return kind === "m" ? f : kind === "a" ? f.call(receiver) : f ? f.value : state.get(receiver);
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/uuid.mjs
/**
* https://stackoverflow.com/a/2117523
*/
let uuid4 = function() {
	const { crypto: crypto$1 } = globalThis;
	if (crypto$1?.randomUUID) {
		uuid4 = crypto$1.randomUUID.bind(crypto$1);
		return crypto$1.randomUUID();
	}
	const u8 = new Uint8Array(1);
	const randomByte = crypto$1 ? () => crypto$1.getRandomValues(u8)[0] : () => Math.random() * 255 & 255;
	return "10000000-1000-4000-8000-100000000000".replace(/[018]/g, (c) => (+c ^ randomByte() & 15 >> +c / 4).toString(16));
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/errors.mjs
function isAbortError(err) {
	return typeof err === "object" && err !== null && ("name" in err && err.name === "AbortError" || "message" in err && String(err.message).includes("FetchRequestCanceledException"));
}
const castToError = (err) => {
	if (err instanceof Error) return err;
	if (typeof err === "object" && err !== null) {
		try {
			if (Object.prototype.toString.call(err) === "[object Error]") {
				const error = new Error(err.message, err.cause ? { cause: err.cause } : {});
				if (err.stack) error.stack = err.stack;
				if (err.cause && !error.cause) error.cause = err.cause;
				if (err.name) error.name = err.name;
				return error;
			}
		} catch {}
		try {
			return new Error(JSON.stringify(err));
		} catch {}
	}
	return new Error(err);
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/core/error.mjs
var OpenAIError = class extends Error {};
var APIError = class APIError extends OpenAIError {
	constructor(status, error, message, headers) {
		super(`${APIError.makeMessage(status, error, message)}`);
		this.status = status;
		this.headers = headers;
		this.requestID = headers?.get("x-request-id");
		this.error = error;
		const data = error;
		this.code = data?.["code"];
		this.param = data?.["param"];
		this.type = data?.["type"];
	}
	static makeMessage(status, error, message) {
		const msg = error?.message ? typeof error.message === "string" ? error.message : JSON.stringify(error.message) : error ? JSON.stringify(error) : message;
		if (status && msg) return `${status} ${msg}`;
		if (status) return `${status} status code (no body)`;
		if (msg) return msg;
		return "(no status code or body)";
	}
	static generate(status, errorResponse, message, headers) {
		if (!status || !headers) return new APIConnectionError({
			message,
			cause: castToError(errorResponse)
		});
		const error = errorResponse?.["error"];
		if (status === 400) return new BadRequestError(status, error, message, headers);
		if (status === 401) return new AuthenticationError(status, error, message, headers);
		if (status === 403) return new PermissionDeniedError(status, error, message, headers);
		if (status === 404) return new NotFoundError(status, error, message, headers);
		if (status === 409) return new ConflictError(status, error, message, headers);
		if (status === 422) return new UnprocessableEntityError(status, error, message, headers);
		if (status === 429) return new RateLimitError(status, error, message, headers);
		if (status >= 500) return new InternalServerError(status, error, message, headers);
		return new APIError(status, error, message, headers);
	}
};
var APIUserAbortError = class extends APIError {
	constructor({ message } = {}) {
		super(void 0, void 0, message || "Request was aborted.", void 0);
	}
};
var APIConnectionError = class extends APIError {
	constructor({ message, cause }) {
		super(void 0, void 0, message || "Connection error.", void 0);
		if (cause) this.cause = cause;
	}
};
var APIConnectionTimeoutError = class extends APIConnectionError {
	constructor({ message } = {}) {
		super({ message: message ?? "Request timed out." });
	}
};
var BadRequestError = class extends APIError {};
var AuthenticationError = class extends APIError {};
var PermissionDeniedError = class extends APIError {};
var NotFoundError = class extends APIError {};
var ConflictError = class extends APIError {};
var UnprocessableEntityError = class extends APIError {};
var RateLimitError = class extends APIError {};
var InternalServerError = class extends APIError {};
var LengthFinishReasonError = class extends OpenAIError {
	constructor() {
		super(`Could not parse response content as the length limit was reached`);
	}
};
var ContentFilterFinishReasonError = class extends OpenAIError {
	constructor() {
		super(`Could not parse response content as the request was rejected by the content filter`);
	}
};
var InvalidWebhookSignatureError = class extends Error {
	constructor(message) {
		super(message);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/values.mjs
const startsWithSchemeRegexp = /^[a-z][a-z0-9+.-]*:/i;
const isAbsoluteURL = (url) => {
	return startsWithSchemeRegexp.test(url);
};
let isArray = (val) => (isArray = Array.isArray, isArray(val));
let isReadonlyArray = isArray;
/** Returns an object if the given value isn't an object, otherwise returns as-is */
function maybeObj(x) {
	if (typeof x !== "object") return {};
	return x ?? {};
}
function isEmptyObj(obj) {
	if (!obj) return true;
	for (const _k in obj) return false;
	return true;
}
function hasOwn(obj, key) {
	return Object.prototype.hasOwnProperty.call(obj, key);
}
function isObj(obj) {
	return obj != null && typeof obj === "object" && !Array.isArray(obj);
}
const validatePositiveInteger = (name, n) => {
	if (typeof n !== "number" || !Number.isInteger(n)) throw new OpenAIError(`${name} must be an integer`);
	if (n < 0) throw new OpenAIError(`${name} must be a positive integer`);
	return n;
};
const safeJSON = (text) => {
	try {
		return JSON.parse(text);
	} catch (err) {
		return;
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/sleep.mjs
const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/version.mjs
const VERSION = "6.16.0";

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/detect-platform.mjs
const isRunningInBrowser = () => {
	return typeof window !== "undefined" && typeof window.document !== "undefined" && typeof navigator !== "undefined";
};
/**
* Note this does not detect 'browser'; for that, use getBrowserInfo().
*/
function getDetectedPlatform() {
	if (typeof Deno !== "undefined" && Deno.build != null) return "deno";
	if (typeof EdgeRuntime !== "undefined") return "edge";
	if (Object.prototype.toString.call(typeof globalThis.process !== "undefined" ? globalThis.process : 0) === "[object process]") return "node";
	return "unknown";
}
const getPlatformProperties = () => {
	const detectedPlatform = getDetectedPlatform();
	if (detectedPlatform === "deno") return {
		"X-Stainless-Lang": "js",
		"X-Stainless-Package-Version": VERSION,
		"X-Stainless-OS": normalizePlatform(Deno.build.os),
		"X-Stainless-Arch": normalizeArch(Deno.build.arch),
		"X-Stainless-Runtime": "deno",
		"X-Stainless-Runtime-Version": typeof Deno.version === "string" ? Deno.version : Deno.version?.deno ?? "unknown"
	};
	if (typeof EdgeRuntime !== "undefined") return {
		"X-Stainless-Lang": "js",
		"X-Stainless-Package-Version": VERSION,
		"X-Stainless-OS": "Unknown",
		"X-Stainless-Arch": `other:${EdgeRuntime}`,
		"X-Stainless-Runtime": "edge",
		"X-Stainless-Runtime-Version": globalThis.process.version
	};
	if (detectedPlatform === "node") return {
		"X-Stainless-Lang": "js",
		"X-Stainless-Package-Version": VERSION,
		"X-Stainless-OS": normalizePlatform(globalThis.process.platform ?? "unknown"),
		"X-Stainless-Arch": normalizeArch(globalThis.process.arch ?? "unknown"),
		"X-Stainless-Runtime": "node",
		"X-Stainless-Runtime-Version": globalThis.process.version ?? "unknown"
	};
	const browserInfo = getBrowserInfo();
	if (browserInfo) return {
		"X-Stainless-Lang": "js",
		"X-Stainless-Package-Version": VERSION,
		"X-Stainless-OS": "Unknown",
		"X-Stainless-Arch": "unknown",
		"X-Stainless-Runtime": `browser:${browserInfo.browser}`,
		"X-Stainless-Runtime-Version": browserInfo.version
	};
	return {
		"X-Stainless-Lang": "js",
		"X-Stainless-Package-Version": VERSION,
		"X-Stainless-OS": "Unknown",
		"X-Stainless-Arch": "unknown",
		"X-Stainless-Runtime": "unknown",
		"X-Stainless-Runtime-Version": "unknown"
	};
};
function getBrowserInfo() {
	if (typeof navigator === "undefined" || !navigator) return null;
	for (const { key, pattern } of [
		{
			key: "edge",
			pattern: /Edge(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/
		},
		{
			key: "ie",
			pattern: /MSIE(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/
		},
		{
			key: "ie",
			pattern: /Trident(?:.*rv\:(\d+)\.(\d+)(?:\.(\d+))?)?/
		},
		{
			key: "chrome",
			pattern: /Chrome(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/
		},
		{
			key: "firefox",
			pattern: /Firefox(?:\W+(\d+)\.(\d+)(?:\.(\d+))?)?/
		},
		{
			key: "safari",
			pattern: /(?:Version\W+(\d+)\.(\d+)(?:\.(\d+))?)?(?:\W+Mobile\S*)?\W+Safari/
		}
	]) {
		const match = pattern.exec(navigator.userAgent);
		if (match) return {
			browser: key,
			version: `${match[1] || 0}.${match[2] || 0}.${match[3] || 0}`
		};
	}
	return null;
}
const normalizeArch = (arch) => {
	if (arch === "x32") return "x32";
	if (arch === "x86_64" || arch === "x64") return "x64";
	if (arch === "arm") return "arm";
	if (arch === "aarch64" || arch === "arm64") return "arm64";
	if (arch) return `other:${arch}`;
	return "unknown";
};
const normalizePlatform = (platform) => {
	platform = platform.toLowerCase();
	if (platform.includes("ios")) return "iOS";
	if (platform === "android") return "Android";
	if (platform === "darwin") return "MacOS";
	if (platform === "win32") return "Windows";
	if (platform === "freebsd") return "FreeBSD";
	if (platform === "openbsd") return "OpenBSD";
	if (platform === "linux") return "Linux";
	if (platform) return `Other:${platform}`;
	return "Unknown";
};
let _platformHeaders;
const getPlatformHeaders = () => {
	return _platformHeaders ?? (_platformHeaders = getPlatformProperties());
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/shims.mjs
function getDefaultFetch() {
	if (typeof fetch !== "undefined") return fetch;
	throw new Error("`fetch` is not defined as a global; Either pass `fetch` to the client, `new OpenAI({ fetch })` or polyfill the global, `globalThis.fetch = fetch`");
}
function makeReadableStream(...args) {
	const ReadableStream = globalThis.ReadableStream;
	if (typeof ReadableStream === "undefined") throw new Error("`ReadableStream` is not defined as a global; You will need to polyfill it, `globalThis.ReadableStream = ReadableStream`");
	return new ReadableStream(...args);
}
function ReadableStreamFrom(iterable) {
	let iter = Symbol.asyncIterator in iterable ? iterable[Symbol.asyncIterator]() : iterable[Symbol.iterator]();
	return makeReadableStream({
		start() {},
		async pull(controller) {
			const { done, value } = await iter.next();
			if (done) controller.close();
			else controller.enqueue(value);
		},
		async cancel() {
			await iter.return?.();
		}
	});
}
/**
* Most browsers don't yet have async iterable support for ReadableStream,
* and Node has a very different way of reading bytes from its "ReadableStream".
*
* This polyfill was pulled from https://github.com/MattiasBuelens/web-streams-polyfill/pull/122#issuecomment-1627354490
*/
function ReadableStreamToAsyncIterable(stream) {
	if (stream[Symbol.asyncIterator]) return stream;
	const reader = stream.getReader();
	return {
		async next() {
			try {
				const result = await reader.read();
				if (result?.done) reader.releaseLock();
				return result;
			} catch (e) {
				reader.releaseLock();
				throw e;
			}
		},
		async return() {
			const cancelPromise = reader.cancel();
			reader.releaseLock();
			await cancelPromise;
			return {
				done: true,
				value: void 0
			};
		},
		[Symbol.asyncIterator]() {
			return this;
		}
	};
}
/**
* Cancels a ReadableStream we don't need to consume.
* See https://undici.nodejs.org/#/?id=garbage-collection
*/
async function CancelReadableStream(stream) {
	if (stream === null || typeof stream !== "object") return;
	if (stream[Symbol.asyncIterator]) {
		await stream[Symbol.asyncIterator]().return?.();
		return;
	}
	const reader = stream.getReader();
	const cancelPromise = reader.cancel();
	reader.releaseLock();
	await cancelPromise;
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/request-options.mjs
const FallbackEncoder = ({ headers, body }) => {
	return {
		bodyHeaders: { "content-type": "application/json" },
		body: JSON.stringify(body)
	};
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/qs/formats.mjs
const default_format = "RFC3986";
const default_formatter = (v) => String(v);
const formatters = {
	RFC1738: (v) => String(v).replace(/%20/g, "+"),
	RFC3986: default_formatter
};
const RFC1738 = "RFC1738";

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/qs/utils.mjs
let has = (obj, key) => (has = Object.hasOwn ?? Function.prototype.call.bind(Object.prototype.hasOwnProperty), has(obj, key));
const hex_table = /* @__PURE__ */ (() => {
	const array = [];
	for (let i = 0; i < 256; ++i) array.push("%" + ((i < 16 ? "0" : "") + i.toString(16)).toUpperCase());
	return array;
})();
const limit = 1024;
const encode = (str$1, _defaultEncoder, charset, _kind, format) => {
	if (str$1.length === 0) return str$1;
	let string = str$1;
	if (typeof str$1 === "symbol") string = Symbol.prototype.toString.call(str$1);
	else if (typeof str$1 !== "string") string = String(str$1);
	if (charset === "iso-8859-1") return escape(string).replace(/%u[0-9a-f]{4}/gi, function($0) {
		return "%26%23" + parseInt($0.slice(2), 16) + "%3B";
	});
	let out = "";
	for (let j = 0; j < string.length; j += limit) {
		const segment = string.length >= limit ? string.slice(j, j + limit) : string;
		const arr = [];
		for (let i = 0; i < segment.length; ++i) {
			let c = segment.charCodeAt(i);
			if (c === 45 || c === 46 || c === 95 || c === 126 || c >= 48 && c <= 57 || c >= 65 && c <= 90 || c >= 97 && c <= 122 || format === RFC1738 && (c === 40 || c === 41)) {
				arr[arr.length] = segment.charAt(i);
				continue;
			}
			if (c < 128) {
				arr[arr.length] = hex_table[c];
				continue;
			}
			if (c < 2048) {
				arr[arr.length] = hex_table[192 | c >> 6] + hex_table[128 | c & 63];
				continue;
			}
			if (c < 55296 || c >= 57344) {
				arr[arr.length] = hex_table[224 | c >> 12] + hex_table[128 | c >> 6 & 63] + hex_table[128 | c & 63];
				continue;
			}
			i += 1;
			c = 65536 + ((c & 1023) << 10 | segment.charCodeAt(i) & 1023);
			arr[arr.length] = hex_table[240 | c >> 18] + hex_table[128 | c >> 12 & 63] + hex_table[128 | c >> 6 & 63] + hex_table[128 | c & 63];
		}
		out += arr.join("");
	}
	return out;
};
function is_buffer(obj) {
	if (!obj || typeof obj !== "object") return false;
	return !!(obj.constructor && obj.constructor.isBuffer && obj.constructor.isBuffer(obj));
}
function maybe_map(val, fn) {
	if (isArray(val)) {
		const mapped = [];
		for (let i = 0; i < val.length; i += 1) mapped.push(fn(val[i]));
		return mapped;
	}
	return fn(val);
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/qs/stringify.mjs
const array_prefix_generators = {
	brackets(prefix) {
		return String(prefix) + "[]";
	},
	comma: "comma",
	indices(prefix, key) {
		return String(prefix) + "[" + key + "]";
	},
	repeat(prefix) {
		return String(prefix);
	}
};
const push_to_array = function(arr, value_or_array) {
	Array.prototype.push.apply(arr, isArray(value_or_array) ? value_or_array : [value_or_array]);
};
let toISOString;
const defaults = {
	addQueryPrefix: false,
	allowDots: false,
	allowEmptyArrays: false,
	arrayFormat: "indices",
	charset: "utf-8",
	charsetSentinel: false,
	delimiter: "&",
	encode: true,
	encodeDotInKeys: false,
	encoder: encode,
	encodeValuesOnly: false,
	format: default_format,
	formatter: default_formatter,
	indices: false,
	serializeDate(date) {
		return (toISOString ?? (toISOString = Function.prototype.call.bind(Date.prototype.toISOString)))(date);
	},
	skipNulls: false,
	strictNullHandling: false
};
function is_non_nullish_primitive(v) {
	return typeof v === "string" || typeof v === "number" || typeof v === "boolean" || typeof v === "symbol" || typeof v === "bigint";
}
const sentinel = {};
function inner_stringify(object, prefix, generateArrayPrefix, commaRoundTrip, allowEmptyArrays, strictNullHandling, skipNulls, encodeDotInKeys, encoder, filter, sort, allowDots, serializeDate, format, formatter, encodeValuesOnly, charset, sideChannel) {
	let obj = object;
	let tmp_sc = sideChannel;
	let step = 0;
	let find_flag = false;
	while ((tmp_sc = tmp_sc.get(sentinel)) !== void 0 && !find_flag) {
		const pos = tmp_sc.get(object);
		step += 1;
		if (typeof pos !== "undefined") if (pos === step) throw new RangeError("Cyclic object value");
		else find_flag = true;
		if (typeof tmp_sc.get(sentinel) === "undefined") step = 0;
	}
	if (typeof filter === "function") obj = filter(prefix, obj);
	else if (obj instanceof Date) obj = serializeDate?.(obj);
	else if (generateArrayPrefix === "comma" && isArray(obj)) obj = maybe_map(obj, function(value) {
		if (value instanceof Date) return serializeDate?.(value);
		return value;
	});
	if (obj === null) {
		if (strictNullHandling) return encoder && !encodeValuesOnly ? encoder(prefix, defaults.encoder, charset, "key", format) : prefix;
		obj = "";
	}
	if (is_non_nullish_primitive(obj) || is_buffer(obj)) {
		if (encoder) {
			const key_value = encodeValuesOnly ? prefix : encoder(prefix, defaults.encoder, charset, "key", format);
			return [formatter?.(key_value) + "=" + formatter?.(encoder(obj, defaults.encoder, charset, "value", format))];
		}
		return [formatter?.(prefix) + "=" + formatter?.(String(obj))];
	}
	const values = [];
	if (typeof obj === "undefined") return values;
	let obj_keys;
	if (generateArrayPrefix === "comma" && isArray(obj)) {
		if (encodeValuesOnly && encoder) obj = maybe_map(obj, encoder);
		obj_keys = [{ value: obj.length > 0 ? obj.join(",") || null : void 0 }];
	} else if (isArray(filter)) obj_keys = filter;
	else {
		const keys = Object.keys(obj);
		obj_keys = sort ? keys.sort(sort) : keys;
	}
	const encoded_prefix = encodeDotInKeys ? String(prefix).replace(/\./g, "%2E") : String(prefix);
	const adjusted_prefix = commaRoundTrip && isArray(obj) && obj.length === 1 ? encoded_prefix + "[]" : encoded_prefix;
	if (allowEmptyArrays && isArray(obj) && obj.length === 0) return adjusted_prefix + "[]";
	for (let j = 0; j < obj_keys.length; ++j) {
		const key = obj_keys[j];
		const value = typeof key === "object" && typeof key.value !== "undefined" ? key.value : obj[key];
		if (skipNulls && value === null) continue;
		const encoded_key = allowDots && encodeDotInKeys ? key.replace(/\./g, "%2E") : key;
		const key_prefix = isArray(obj) ? typeof generateArrayPrefix === "function" ? generateArrayPrefix(adjusted_prefix, encoded_key) : adjusted_prefix : adjusted_prefix + (allowDots ? "." + encoded_key : "[" + encoded_key + "]");
		sideChannel.set(object, step);
		const valueSideChannel = /* @__PURE__ */ new WeakMap();
		valueSideChannel.set(sentinel, sideChannel);
		push_to_array(values, inner_stringify(value, key_prefix, generateArrayPrefix, commaRoundTrip, allowEmptyArrays, strictNullHandling, skipNulls, encodeDotInKeys, generateArrayPrefix === "comma" && encodeValuesOnly && isArray(obj) ? null : encoder, filter, sort, allowDots, serializeDate, format, formatter, encodeValuesOnly, charset, valueSideChannel));
	}
	return values;
}
function normalize_stringify_options(opts = defaults) {
	if (typeof opts.allowEmptyArrays !== "undefined" && typeof opts.allowEmptyArrays !== "boolean") throw new TypeError("`allowEmptyArrays` option can only be `true` or `false`, when provided");
	if (typeof opts.encodeDotInKeys !== "undefined" && typeof opts.encodeDotInKeys !== "boolean") throw new TypeError("`encodeDotInKeys` option can only be `true` or `false`, when provided");
	if (opts.encoder !== null && typeof opts.encoder !== "undefined" && typeof opts.encoder !== "function") throw new TypeError("Encoder has to be a function.");
	const charset = opts.charset || defaults.charset;
	if (typeof opts.charset !== "undefined" && opts.charset !== "utf-8" && opts.charset !== "iso-8859-1") throw new TypeError("The charset option must be either utf-8, iso-8859-1, or undefined");
	let format = default_format;
	if (typeof opts.format !== "undefined") {
		if (!has(formatters, opts.format)) throw new TypeError("Unknown format option provided.");
		format = opts.format;
	}
	const formatter = formatters[format];
	let filter = defaults.filter;
	if (typeof opts.filter === "function" || isArray(opts.filter)) filter = opts.filter;
	let arrayFormat;
	if (opts.arrayFormat && opts.arrayFormat in array_prefix_generators) arrayFormat = opts.arrayFormat;
	else if ("indices" in opts) arrayFormat = opts.indices ? "indices" : "repeat";
	else arrayFormat = defaults.arrayFormat;
	if ("commaRoundTrip" in opts && typeof opts.commaRoundTrip !== "boolean") throw new TypeError("`commaRoundTrip` must be a boolean, or absent");
	const allowDots = typeof opts.allowDots === "undefined" ? !!opts.encodeDotInKeys === true ? true : defaults.allowDots : !!opts.allowDots;
	return {
		addQueryPrefix: typeof opts.addQueryPrefix === "boolean" ? opts.addQueryPrefix : defaults.addQueryPrefix,
		allowDots,
		allowEmptyArrays: typeof opts.allowEmptyArrays === "boolean" ? !!opts.allowEmptyArrays : defaults.allowEmptyArrays,
		arrayFormat,
		charset,
		charsetSentinel: typeof opts.charsetSentinel === "boolean" ? opts.charsetSentinel : defaults.charsetSentinel,
		commaRoundTrip: !!opts.commaRoundTrip,
		delimiter: typeof opts.delimiter === "undefined" ? defaults.delimiter : opts.delimiter,
		encode: typeof opts.encode === "boolean" ? opts.encode : defaults.encode,
		encodeDotInKeys: typeof opts.encodeDotInKeys === "boolean" ? opts.encodeDotInKeys : defaults.encodeDotInKeys,
		encoder: typeof opts.encoder === "function" ? opts.encoder : defaults.encoder,
		encodeValuesOnly: typeof opts.encodeValuesOnly === "boolean" ? opts.encodeValuesOnly : defaults.encodeValuesOnly,
		filter,
		format,
		formatter,
		serializeDate: typeof opts.serializeDate === "function" ? opts.serializeDate : defaults.serializeDate,
		skipNulls: typeof opts.skipNulls === "boolean" ? opts.skipNulls : defaults.skipNulls,
		sort: typeof opts.sort === "function" ? opts.sort : null,
		strictNullHandling: typeof opts.strictNullHandling === "boolean" ? opts.strictNullHandling : defaults.strictNullHandling
	};
}
function stringify(object, opts = {}) {
	let obj = object;
	const options = normalize_stringify_options(opts);
	let obj_keys;
	let filter;
	if (typeof options.filter === "function") {
		filter = options.filter;
		obj = filter("", obj);
	} else if (isArray(options.filter)) {
		filter = options.filter;
		obj_keys = filter;
	}
	const keys = [];
	if (typeof obj !== "object" || obj === null) return "";
	const generateArrayPrefix = array_prefix_generators[options.arrayFormat];
	const commaRoundTrip = generateArrayPrefix === "comma" && options.commaRoundTrip;
	if (!obj_keys) obj_keys = Object.keys(obj);
	if (options.sort) obj_keys.sort(options.sort);
	const sideChannel = /* @__PURE__ */ new WeakMap();
	for (let i = 0; i < obj_keys.length; ++i) {
		const key = obj_keys[i];
		if (options.skipNulls && obj[key] === null) continue;
		push_to_array(keys, inner_stringify(obj[key], key, generateArrayPrefix, commaRoundTrip, options.allowEmptyArrays, options.strictNullHandling, options.skipNulls, options.encodeDotInKeys, options.encode ? options.encoder : null, options.filter, options.sort, options.allowDots, options.serializeDate, options.format, options.formatter, options.encodeValuesOnly, options.charset, sideChannel));
	}
	const joined = keys.join(options.delimiter);
	let prefix = options.addQueryPrefix === true ? "?" : "";
	if (options.charsetSentinel) if (options.charset === "iso-8859-1") prefix += "utf8=%26%2310003%3B&";
	else prefix += "utf8=%E2%9C%93&";
	return joined.length > 0 ? prefix + joined : "";
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/bytes.mjs
function concatBytes(buffers) {
	let length = 0;
	for (const buffer of buffers) length += buffer.length;
	const output = new Uint8Array(length);
	let index = 0;
	for (const buffer of buffers) {
		output.set(buffer, index);
		index += buffer.length;
	}
	return output;
}
let encodeUTF8_;
function encodeUTF8(str$1) {
	let encoder;
	return (encodeUTF8_ ?? (encoder = new globalThis.TextEncoder(), encodeUTF8_ = encoder.encode.bind(encoder)))(str$1);
}
let decodeUTF8_;
function decodeUTF8(bytes) {
	let decoder;
	return (decodeUTF8_ ?? (decoder = new globalThis.TextDecoder(), decodeUTF8_ = decoder.decode.bind(decoder)))(bytes);
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/decoders/line.mjs
var _LineDecoder_buffer, _LineDecoder_carriageReturnIndex;
/**
* A re-implementation of httpx's `LineDecoder` in Python that handles incrementally
* reading lines from text.
*
* https://github.com/encode/httpx/blob/920333ea98118e9cf617f246905d7b202510941c/httpx/_decoders.py#L258
*/
var LineDecoder = class {
	constructor() {
		_LineDecoder_buffer.set(this, void 0);
		_LineDecoder_carriageReturnIndex.set(this, void 0);
		__classPrivateFieldSet(this, _LineDecoder_buffer, new Uint8Array(), "f");
		__classPrivateFieldSet(this, _LineDecoder_carriageReturnIndex, null, "f");
	}
	decode(chunk) {
		if (chunk == null) return [];
		const binaryChunk = chunk instanceof ArrayBuffer ? new Uint8Array(chunk) : typeof chunk === "string" ? encodeUTF8(chunk) : chunk;
		__classPrivateFieldSet(this, _LineDecoder_buffer, concatBytes([__classPrivateFieldGet(this, _LineDecoder_buffer, "f"), binaryChunk]), "f");
		const lines = [];
		let patternIndex;
		while ((patternIndex = findNewlineIndex(__classPrivateFieldGet(this, _LineDecoder_buffer, "f"), __classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f"))) != null) {
			if (patternIndex.carriage && __classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f") == null) {
				__classPrivateFieldSet(this, _LineDecoder_carriageReturnIndex, patternIndex.index, "f");
				continue;
			}
			if (__classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f") != null && (patternIndex.index !== __classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f") + 1 || patternIndex.carriage)) {
				lines.push(decodeUTF8(__classPrivateFieldGet(this, _LineDecoder_buffer, "f").subarray(0, __classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f") - 1)));
				__classPrivateFieldSet(this, _LineDecoder_buffer, __classPrivateFieldGet(this, _LineDecoder_buffer, "f").subarray(__classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f")), "f");
				__classPrivateFieldSet(this, _LineDecoder_carriageReturnIndex, null, "f");
				continue;
			}
			const endIndex = __classPrivateFieldGet(this, _LineDecoder_carriageReturnIndex, "f") !== null ? patternIndex.preceding - 1 : patternIndex.preceding;
			const line = decodeUTF8(__classPrivateFieldGet(this, _LineDecoder_buffer, "f").subarray(0, endIndex));
			lines.push(line);
			__classPrivateFieldSet(this, _LineDecoder_buffer, __classPrivateFieldGet(this, _LineDecoder_buffer, "f").subarray(patternIndex.index), "f");
			__classPrivateFieldSet(this, _LineDecoder_carriageReturnIndex, null, "f");
		}
		return lines;
	}
	flush() {
		if (!__classPrivateFieldGet(this, _LineDecoder_buffer, "f").length) return [];
		return this.decode("\n");
	}
};
_LineDecoder_buffer = /* @__PURE__ */ new WeakMap(), _LineDecoder_carriageReturnIndex = /* @__PURE__ */ new WeakMap();
LineDecoder.NEWLINE_CHARS = new Set(["\n", "\r"]);
LineDecoder.NEWLINE_REGEXP = /\r\n|[\n\r]/g;
/**
* This function searches the buffer for the end patterns, (\r or \n)
* and returns an object with the index preceding the matched newline and the
* index after the newline char. `null` is returned if no new line is found.
*
* ```ts
* findNewLineIndex('abc\ndef') -> { preceding: 2, index: 3 }
* ```
*/
function findNewlineIndex(buffer, startIndex) {
	const newline = 10;
	const carriage = 13;
	for (let i = startIndex ?? 0; i < buffer.length; i++) {
		if (buffer[i] === newline) return {
			preceding: i,
			index: i + 1,
			carriage: false
		};
		if (buffer[i] === carriage) return {
			preceding: i,
			index: i + 1,
			carriage: true
		};
	}
	return null;
}
function findDoubleNewlineIndex(buffer) {
	const newline = 10;
	const carriage = 13;
	for (let i = 0; i < buffer.length - 1; i++) {
		if (buffer[i] === newline && buffer[i + 1] === newline) return i + 2;
		if (buffer[i] === carriage && buffer[i + 1] === carriage) return i + 2;
		if (buffer[i] === carriage && buffer[i + 1] === newline && i + 3 < buffer.length && buffer[i + 2] === carriage && buffer[i + 3] === newline) return i + 4;
	}
	return -1;
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/log.mjs
const levelNumbers = {
	off: 0,
	error: 200,
	warn: 300,
	info: 400,
	debug: 500
};
const parseLogLevel = (maybeLevel, sourceName, client) => {
	if (!maybeLevel) return;
	if (hasOwn(levelNumbers, maybeLevel)) return maybeLevel;
	loggerFor(client).warn(`${sourceName} was set to ${JSON.stringify(maybeLevel)}, expected one of ${JSON.stringify(Object.keys(levelNumbers))}`);
};
function noop() {}
function makeLogFn(fnLevel, logger, logLevel) {
	if (!logger || levelNumbers[fnLevel] > levelNumbers[logLevel]) return noop;
	else return logger[fnLevel].bind(logger);
}
const noopLogger = {
	error: noop,
	warn: noop,
	info: noop,
	debug: noop
};
let cachedLoggers = /* @__PURE__ */ new WeakMap();
function loggerFor(client) {
	const logger = client.logger;
	const logLevel = client.logLevel ?? "off";
	if (!logger) return noopLogger;
	const cachedLogger = cachedLoggers.get(logger);
	if (cachedLogger && cachedLogger[0] === logLevel) return cachedLogger[1];
	const levelLogger = {
		error: makeLogFn("error", logger, logLevel),
		warn: makeLogFn("warn", logger, logLevel),
		info: makeLogFn("info", logger, logLevel),
		debug: makeLogFn("debug", logger, logLevel)
	};
	cachedLoggers.set(logger, [logLevel, levelLogger]);
	return levelLogger;
}
const formatRequestDetails = (details) => {
	if (details.options) {
		details.options = { ...details.options };
		delete details.options["headers"];
	}
	if (details.headers) details.headers = Object.fromEntries((details.headers instanceof Headers ? [...details.headers] : Object.entries(details.headers)).map(([name, value]) => [name, name.toLowerCase() === "authorization" || name.toLowerCase() === "cookie" || name.toLowerCase() === "set-cookie" ? "***" : value]));
	if ("retryOfRequestLogID" in details) {
		if (details.retryOfRequestLogID) details.retryOf = details.retryOfRequestLogID;
		delete details.retryOfRequestLogID;
	}
	return details;
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/core/streaming.mjs
var _Stream_client;
var Stream = class Stream {
	constructor(iterator, controller, client) {
		this.iterator = iterator;
		_Stream_client.set(this, void 0);
		this.controller = controller;
		__classPrivateFieldSet(this, _Stream_client, client, "f");
	}
	static fromSSEResponse(response, controller, client) {
		let consumed = false;
		const logger = client ? loggerFor(client) : console;
		async function* iterator() {
			if (consumed) throw new OpenAIError("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
			consumed = true;
			let done = false;
			try {
				for await (const sse of _iterSSEMessages(response, controller)) {
					if (done) continue;
					if (sse.data.startsWith("[DONE]")) {
						done = true;
						continue;
					}
					if (sse.event === null || !sse.event.startsWith("thread.")) {
						let data;
						try {
							data = JSON.parse(sse.data);
						} catch (e) {
							logger.error(`Could not parse message into JSON:`, sse.data);
							logger.error(`From chunk:`, sse.raw);
							throw e;
						}
						if (data && data.error) throw new APIError(void 0, data.error, void 0, response.headers);
						yield data;
					} else {
						let data;
						try {
							data = JSON.parse(sse.data);
						} catch (e) {
							console.error(`Could not parse message into JSON:`, sse.data);
							console.error(`From chunk:`, sse.raw);
							throw e;
						}
						if (sse.event == "error") throw new APIError(void 0, data.error, data.message, void 0);
						yield {
							event: sse.event,
							data
						};
					}
				}
				done = true;
			} catch (e) {
				if (isAbortError(e)) return;
				throw e;
			} finally {
				if (!done) controller.abort();
			}
		}
		return new Stream(iterator, controller, client);
	}
	/**
	* Generates a Stream from a newline-separated ReadableStream
	* where each item is a JSON value.
	*/
	static fromReadableStream(readableStream, controller, client) {
		let consumed = false;
		async function* iterLines() {
			const lineDecoder = new LineDecoder();
			const iter = ReadableStreamToAsyncIterable(readableStream);
			for await (const chunk of iter) for (const line of lineDecoder.decode(chunk)) yield line;
			for (const line of lineDecoder.flush()) yield line;
		}
		async function* iterator() {
			if (consumed) throw new OpenAIError("Cannot iterate over a consumed stream, use `.tee()` to split the stream.");
			consumed = true;
			let done = false;
			try {
				for await (const line of iterLines()) {
					if (done) continue;
					if (line) yield JSON.parse(line);
				}
				done = true;
			} catch (e) {
				if (isAbortError(e)) return;
				throw e;
			} finally {
				if (!done) controller.abort();
			}
		}
		return new Stream(iterator, controller, client);
	}
	[(_Stream_client = /* @__PURE__ */ new WeakMap(), Symbol.asyncIterator)]() {
		return this.iterator();
	}
	/**
	* Splits the stream into two streams which can be
	* independently read from at different speeds.
	*/
	tee() {
		const left = [];
		const right = [];
		const iterator = this.iterator();
		const teeIterator = (queue) => {
			return { next: () => {
				if (queue.length === 0) {
					const result = iterator.next();
					left.push(result);
					right.push(result);
				}
				return queue.shift();
			} };
		};
		return [new Stream(() => teeIterator(left), this.controller, __classPrivateFieldGet(this, _Stream_client, "f")), new Stream(() => teeIterator(right), this.controller, __classPrivateFieldGet(this, _Stream_client, "f"))];
	}
	/**
	* Converts this stream to a newline-separated ReadableStream of
	* JSON stringified values in the stream
	* which can be turned back into a Stream with `Stream.fromReadableStream()`.
	*/
	toReadableStream() {
		const self = this;
		let iter;
		return makeReadableStream({
			async start() {
				iter = self[Symbol.asyncIterator]();
			},
			async pull(ctrl) {
				try {
					const { value, done } = await iter.next();
					if (done) return ctrl.close();
					const bytes = encodeUTF8(JSON.stringify(value) + "\n");
					ctrl.enqueue(bytes);
				} catch (err) {
					ctrl.error(err);
				}
			},
			async cancel() {
				await iter.return?.();
			}
		});
	}
};
async function* _iterSSEMessages(response, controller) {
	if (!response.body) {
		controller.abort();
		if (typeof globalThis.navigator !== "undefined" && globalThis.navigator.product === "ReactNative") throw new OpenAIError(`The default react-native fetch implementation does not support streaming. Please use expo/fetch: https://docs.expo.dev/versions/latest/sdk/expo/#expofetch-api`);
		throw new OpenAIError(`Attempted to iterate over a response with no body`);
	}
	const sseDecoder = new SSEDecoder();
	const lineDecoder = new LineDecoder();
	const iter = ReadableStreamToAsyncIterable(response.body);
	for await (const sseChunk of iterSSEChunks(iter)) for (const line of lineDecoder.decode(sseChunk)) {
		const sse = sseDecoder.decode(line);
		if (sse) yield sse;
	}
	for (const line of lineDecoder.flush()) {
		const sse = sseDecoder.decode(line);
		if (sse) yield sse;
	}
}
/**
* Given an async iterable iterator, iterates over it and yields full
* SSE chunks, i.e. yields when a double new-line is encountered.
*/
async function* iterSSEChunks(iterator) {
	let data = new Uint8Array();
	for await (const chunk of iterator) {
		if (chunk == null) continue;
		const binaryChunk = chunk instanceof ArrayBuffer ? new Uint8Array(chunk) : typeof chunk === "string" ? encodeUTF8(chunk) : chunk;
		let newData = new Uint8Array(data.length + binaryChunk.length);
		newData.set(data);
		newData.set(binaryChunk, data.length);
		data = newData;
		let patternIndex;
		while ((patternIndex = findDoubleNewlineIndex(data)) !== -1) {
			yield data.slice(0, patternIndex);
			data = data.slice(patternIndex);
		}
	}
	if (data.length > 0) yield data;
}
var SSEDecoder = class {
	constructor() {
		this.event = null;
		this.data = [];
		this.chunks = [];
	}
	decode(line) {
		if (line.endsWith("\r")) line = line.substring(0, line.length - 1);
		if (!line) {
			if (!this.event && !this.data.length) return null;
			const sse = {
				event: this.event,
				data: this.data.join("\n"),
				raw: this.chunks
			};
			this.event = null;
			this.data = [];
			this.chunks = [];
			return sse;
		}
		this.chunks.push(line);
		if (line.startsWith(":")) return null;
		let [fieldname, _, value] = partition(line, ":");
		if (value.startsWith(" ")) value = value.substring(1);
		if (fieldname === "event") this.event = value;
		else if (fieldname === "data") this.data.push(value);
		return null;
	}
};
function partition(str$1, delimiter) {
	const index = str$1.indexOf(delimiter);
	if (index !== -1) return [
		str$1.substring(0, index),
		delimiter,
		str$1.substring(index + delimiter.length)
	];
	return [
		str$1,
		"",
		""
	];
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/parse.mjs
async function defaultParseResponse(client, props) {
	const { response, requestLogID, retryOfRequestLogID, startTime } = props;
	const body = await (async () => {
		if (props.options.stream) {
			loggerFor(client).debug("response", response.status, response.url, response.headers, response.body);
			if (props.options.__streamClass) return props.options.__streamClass.fromSSEResponse(response, props.controller, client);
			return Stream.fromSSEResponse(response, props.controller, client);
		}
		if (response.status === 204) return null;
		if (props.options.__binaryResponse) return response;
		const mediaType = response.headers.get("content-type")?.split(";")[0]?.trim();
		if (mediaType?.includes("application/json") || mediaType?.endsWith("+json")) return addRequestID(await response.json(), response);
		return await response.text();
	})();
	loggerFor(client).debug(`[${requestLogID}] response parsed`, formatRequestDetails({
		retryOfRequestLogID,
		url: response.url,
		status: response.status,
		body,
		durationMs: Date.now() - startTime
	}));
	return body;
}
function addRequestID(value, response) {
	if (!value || typeof value !== "object" || Array.isArray(value)) return value;
	return Object.defineProperty(value, "_request_id", {
		value: response.headers.get("x-request-id"),
		enumerable: false
	});
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/core/api-promise.mjs
var _APIPromise_client;
/**
* A subclass of `Promise` providing additional helper methods
* for interacting with the SDK.
*/
var APIPromise = class APIPromise extends Promise {
	constructor(client, responsePromise, parseResponse$1 = defaultParseResponse) {
		super((resolve) => {
			resolve(null);
		});
		this.responsePromise = responsePromise;
		this.parseResponse = parseResponse$1;
		_APIPromise_client.set(this, void 0);
		__classPrivateFieldSet(this, _APIPromise_client, client, "f");
	}
	_thenUnwrap(transform) {
		return new APIPromise(__classPrivateFieldGet(this, _APIPromise_client, "f"), this.responsePromise, async (client, props) => addRequestID(transform(await this.parseResponse(client, props), props), props.response));
	}
	/**
	* Gets the raw `Response` instance instead of parsing the response
	* data.
	*
	* If you want to parse the response body but still get the `Response`
	* instance, you can use {@link withResponse()}.
	*
	* 👋 Getting the wrong TypeScript type for `Response`?
	* Try setting `"moduleResolution": "NodeNext"` or add `"lib": ["DOM"]`
	* to your `tsconfig.json`.
	*/
	asResponse() {
		return this.responsePromise.then((p) => p.response);
	}
	/**
	* Gets the parsed response data, the raw `Response` instance and the ID of the request,
	* returned via the X-Request-ID header which is useful for debugging requests and reporting
	* issues to OpenAI.
	*
	* If you just want to get the raw `Response` instance without parsing it,
	* you can use {@link asResponse()}.
	*
	* 👋 Getting the wrong TypeScript type for `Response`?
	* Try setting `"moduleResolution": "NodeNext"` or add `"lib": ["DOM"]`
	* to your `tsconfig.json`.
	*/
	async withResponse() {
		const [data, response] = await Promise.all([this.parse(), this.asResponse()]);
		return {
			data,
			response,
			request_id: response.headers.get("x-request-id")
		};
	}
	parse() {
		if (!this.parsedPromise) this.parsedPromise = this.responsePromise.then((data) => this.parseResponse(__classPrivateFieldGet(this, _APIPromise_client, "f"), data));
		return this.parsedPromise;
	}
	then(onfulfilled, onrejected) {
		return this.parse().then(onfulfilled, onrejected);
	}
	catch(onrejected) {
		return this.parse().catch(onrejected);
	}
	finally(onfinally) {
		return this.parse().finally(onfinally);
	}
};
_APIPromise_client = /* @__PURE__ */ new WeakMap();

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/core/pagination.mjs
var _AbstractPage_client;
var AbstractPage = class {
	constructor(client, response, body, options) {
		_AbstractPage_client.set(this, void 0);
		__classPrivateFieldSet(this, _AbstractPage_client, client, "f");
		this.options = options;
		this.response = response;
		this.body = body;
	}
	hasNextPage() {
		if (!this.getPaginatedItems().length) return false;
		return this.nextPageRequestOptions() != null;
	}
	async getNextPage() {
		const nextOptions = this.nextPageRequestOptions();
		if (!nextOptions) throw new OpenAIError("No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.");
		return await __classPrivateFieldGet(this, _AbstractPage_client, "f").requestAPIList(this.constructor, nextOptions);
	}
	async *iterPages() {
		let page = this;
		yield page;
		while (page.hasNextPage()) {
			page = await page.getNextPage();
			yield page;
		}
	}
	async *[(_AbstractPage_client = /* @__PURE__ */ new WeakMap(), Symbol.asyncIterator)]() {
		for await (const page of this.iterPages()) for (const item of page.getPaginatedItems()) yield item;
	}
};
/**
* This subclass of Promise will resolve to an instantiated Page once the request completes.
*
* It also implements AsyncIterable to allow auto-paginating iteration on an unawaited list call, eg:
*
*    for await (const item of client.items.list()) {
*      console.log(item)
*    }
*/
var PagePromise = class extends APIPromise {
	constructor(client, request, Page$1) {
		super(client, request, async (client$1, props) => new Page$1(client$1, props.response, await defaultParseResponse(client$1, props), props.options));
	}
	/**
	* Allow auto-paginating iteration on an unawaited list call, eg:
	*
	*    for await (const item of client.items.list()) {
	*      console.log(item)
	*    }
	*/
	async *[Symbol.asyncIterator]() {
		const page = await this;
		for await (const item of page) yield item;
	}
};
/**
* Note: no pagination actually occurs yet, this is for forwards-compatibility.
*/
var Page = class extends AbstractPage {
	constructor(client, response, body, options) {
		super(client, response, body, options);
		this.data = body.data || [];
		this.object = body.object;
	}
	getPaginatedItems() {
		return this.data ?? [];
	}
	nextPageRequestOptions() {
		return null;
	}
};
var CursorPage = class extends AbstractPage {
	constructor(client, response, body, options) {
		super(client, response, body, options);
		this.data = body.data || [];
		this.has_more = body.has_more || false;
	}
	getPaginatedItems() {
		return this.data ?? [];
	}
	hasNextPage() {
		if (this.has_more === false) return false;
		return super.hasNextPage();
	}
	nextPageRequestOptions() {
		const data = this.getPaginatedItems();
		const id = data[data.length - 1]?.id;
		if (!id) return null;
		return {
			...this.options,
			query: {
				...maybeObj(this.options.query),
				after: id
			}
		};
	}
};
var ConversationCursorPage = class extends AbstractPage {
	constructor(client, response, body, options) {
		super(client, response, body, options);
		this.data = body.data || [];
		this.has_more = body.has_more || false;
		this.last_id = body.last_id || "";
	}
	getPaginatedItems() {
		return this.data ?? [];
	}
	hasNextPage() {
		if (this.has_more === false) return false;
		return super.hasNextPage();
	}
	nextPageRequestOptions() {
		const cursor = this.last_id;
		if (!cursor) return null;
		return {
			...this.options,
			query: {
				...maybeObj(this.options.query),
				after: cursor
			}
		};
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/uploads.mjs
const checkFileSupport = () => {
	if (typeof File === "undefined") {
		const { process: process$1 } = globalThis;
		const isOldNode = typeof process$1?.versions?.node === "string" && parseInt(process$1.versions.node.split(".")) < 20;
		throw new Error("`File` is not defined as a global, which is required for file uploads." + (isOldNode ? " Update to Node 20 LTS or newer, or set `globalThis.File` to `import('node:buffer').File`." : ""));
	}
};
/**
* Construct a `File` instance. This is used to ensure a helpful error is thrown
* for environments that don't define a global `File` yet.
*/
function makeFile(fileBits, fileName, options) {
	checkFileSupport();
	return new File(fileBits, fileName ?? "unknown_file", options);
}
function getName(value) {
	return (typeof value === "object" && value !== null && ("name" in value && value.name && String(value.name) || "url" in value && value.url && String(value.url) || "filename" in value && value.filename && String(value.filename) || "path" in value && value.path && String(value.path)) || "").split(/[\\/]/).pop() || void 0;
}
const isAsyncIterable = (value) => value != null && typeof value === "object" && typeof value[Symbol.asyncIterator] === "function";
/**
* Returns a multipart/form-data request if any part of the given request body contains a File / Blob value.
* Otherwise returns the request as is.
*/
const maybeMultipartFormRequestOptions = async (opts, fetch$1) => {
	if (!hasUploadableValue(opts.body)) return opts;
	return {
		...opts,
		body: await createForm(opts.body, fetch$1)
	};
};
const multipartFormRequestOptions = async (opts, fetch$1) => {
	return {
		...opts,
		body: await createForm(opts.body, fetch$1)
	};
};
const supportsFormDataMap = /* @__PURE__ */ new WeakMap();
/**
* node-fetch doesn't support the global FormData object in recent node versions. Instead of sending
* properly-encoded form data, it just stringifies the object, resulting in a request body of "[object FormData]".
* This function detects if the fetch function provided supports the global FormData object to avoid
* confusing error messages later on.
*/
function supportsFormData(fetchObject) {
	const fetch$1 = typeof fetchObject === "function" ? fetchObject : fetchObject.fetch;
	const cached = supportsFormDataMap.get(fetch$1);
	if (cached) return cached;
	const promise = (async () => {
		try {
			const FetchResponse = "Response" in fetch$1 ? fetch$1.Response : (await fetch$1("data:,")).constructor;
			const data = new FormData();
			if (data.toString() === await new FetchResponse(data).text()) return false;
			return true;
		} catch {
			return true;
		}
	})();
	supportsFormDataMap.set(fetch$1, promise);
	return promise;
}
const createForm = async (body, fetch$1) => {
	if (!await supportsFormData(fetch$1)) throw new TypeError("The provided fetch function does not support file uploads with the current global FormData class.");
	const form = new FormData();
	await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));
	return form;
};
const isNamedBlob = (value) => value instanceof Blob && "name" in value;
const isUploadable = (value) => typeof value === "object" && value !== null && (value instanceof Response || isAsyncIterable(value) || isNamedBlob(value));
const hasUploadableValue = (value) => {
	if (isUploadable(value)) return true;
	if (Array.isArray(value)) return value.some(hasUploadableValue);
	if (value && typeof value === "object") {
		for (const k in value) if (hasUploadableValue(value[k])) return true;
	}
	return false;
};
const addFormValue = async (form, key, value) => {
	if (value === void 0) return;
	if (value == null) throw new TypeError(`Received null for "${key}"; to pass null in FormData, you must use the string 'null'`);
	if (typeof value === "string" || typeof value === "number" || typeof value === "boolean") form.append(key, String(value));
	else if (value instanceof Response) form.append(key, makeFile([await value.blob()], getName(value)));
	else if (isAsyncIterable(value)) form.append(key, makeFile([await new Response(ReadableStreamFrom(value)).blob()], getName(value)));
	else if (isNamedBlob(value)) form.append(key, value, getName(value));
	else if (Array.isArray(value)) await Promise.all(value.map((entry) => addFormValue(form, key + "[]", entry)));
	else if (typeof value === "object") await Promise.all(Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)));
	else throw new TypeError(`Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`);
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/to-file.mjs
/**
* This check adds the arrayBuffer() method type because it is available and used at runtime
*/
const isBlobLike = (value) => value != null && typeof value === "object" && typeof value.size === "number" && typeof value.type === "string" && typeof value.text === "function" && typeof value.slice === "function" && typeof value.arrayBuffer === "function";
/**
* This check adds the arrayBuffer() method type because it is available and used at runtime
*/
const isFileLike = (value) => value != null && typeof value === "object" && typeof value.name === "string" && typeof value.lastModified === "number" && isBlobLike(value);
const isResponseLike = (value) => value != null && typeof value === "object" && typeof value.url === "string" && typeof value.blob === "function";
/**
* Helper for creating a {@link File} to pass to an SDK upload method from a variety of different data formats
* @param value the raw content of the file. Can be an {@link Uploadable}, BlobLikePart, or AsyncIterable of BlobLikeParts
* @param {string=} name the name of the file. If omitted, toFile will try to determine a file name from bits if possible
* @param {Object=} options additional properties
* @param {string=} options.type the MIME type of the content
* @param {number=} options.lastModified the last modified timestamp
* @returns a {@link File} with the given properties
*/
async function toFile(value, name, options) {
	checkFileSupport();
	value = await value;
	if (isFileLike(value)) {
		if (value instanceof File) return value;
		return makeFile([await value.arrayBuffer()], value.name);
	}
	if (isResponseLike(value)) {
		const blob = await value.blob();
		name || (name = new URL(value.url).pathname.split(/[\\/]/).pop());
		return makeFile(await getBytes(blob), name, options);
	}
	const parts = await getBytes(value);
	name || (name = getName(value));
	if (!options?.type) {
		const type = parts.find((part) => typeof part === "object" && "type" in part && part.type);
		if (typeof type === "string") options = {
			...options,
			type
		};
	}
	return makeFile(parts, name, options);
}
async function getBytes(value) {
	let parts = [];
	if (typeof value === "string" || ArrayBuffer.isView(value) || value instanceof ArrayBuffer) parts.push(value);
	else if (isBlobLike(value)) parts.push(value instanceof Blob ? value : await value.arrayBuffer());
	else if (isAsyncIterable(value)) for await (const chunk of value) parts.push(...await getBytes(chunk));
	else {
		const constructor = value?.constructor?.name;
		throw new Error(`Unexpected data type: ${typeof value}${constructor ? `; constructor: ${constructor}` : ""}${propsForError(value)}`);
	}
	return parts;
}
function propsForError(value) {
	if (typeof value !== "object" || value === null) return "";
	return `; props: [${Object.getOwnPropertyNames(value).map((p) => `"${p}"`).join(", ")}]`;
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/core/resource.mjs
var APIResource = class {
	constructor(client) {
		this._client = client;
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/path.mjs
/**
* Percent-encode everything that isn't safe to have in a path without encoding safe chars.
*
* Taken from https://datatracker.ietf.org/doc/html/rfc3986#section-3.3:
* > unreserved  = ALPHA / DIGIT / "-" / "." / "_" / "~"
* > sub-delims  = "!" / "$" / "&" / "'" / "(" / ")" / "*" / "+" / "," / ";" / "="
* > pchar       = unreserved / pct-encoded / sub-delims / ":" / "@"
*/
function encodeURIPath(str$1) {
	return str$1.replace(/[^A-Za-z0-9\-._~!$&'()*+,;=:@]+/g, encodeURIComponent);
}
const EMPTY = /* @__PURE__ */ Object.freeze(/* @__PURE__ */ Object.create(null));
const createPathTagFunction = (pathEncoder = encodeURIPath) => function path$2(statics, ...params) {
	if (statics.length === 1) return statics[0];
	let postPath = false;
	const invalidSegments = [];
	const path$3 = statics.reduce((previousValue, currentValue, index) => {
		if (/[?#]/.test(currentValue)) postPath = true;
		const value = params[index];
		let encoded = (postPath ? encodeURIComponent : pathEncoder)("" + value);
		if (index !== params.length && (value == null || typeof value === "object" && value.toString === Object.getPrototypeOf(Object.getPrototypeOf(value.hasOwnProperty ?? EMPTY) ?? EMPTY)?.toString)) {
			encoded = value + "";
			invalidSegments.push({
				start: previousValue.length + currentValue.length,
				length: encoded.length,
				error: `Value of type ${Object.prototype.toString.call(value).slice(8, -1)} is not a valid path parameter`
			});
		}
		return previousValue + currentValue + (index === params.length ? "" : encoded);
	}, "");
	const pathOnly = path$3.split(/[?#]/, 1)[0];
	const invalidSegmentPattern = /(?<=^|\/)(?:\.|%2e){1,2}(?=\/|$)/gi;
	let match;
	while ((match = invalidSegmentPattern.exec(pathOnly)) !== null) invalidSegments.push({
		start: match.index,
		length: match[0].length,
		error: `Value "${match[0]}" can\'t be safely passed as a path parameter`
	});
	invalidSegments.sort((a, b) => a.start - b.start);
	if (invalidSegments.length > 0) {
		let lastEnd = 0;
		const underline = invalidSegments.reduce((acc, segment) => {
			const spaces = " ".repeat(segment.start - lastEnd);
			const arrows = "^".repeat(segment.length);
			lastEnd = segment.start + segment.length;
			return acc + spaces + arrows;
		}, "");
		throw new OpenAIError(`Path parameters result in path with invalid segments:\n${invalidSegments.map((e) => e.error).join("\n")}\n${path$3}\n${underline}`);
	}
	return path$3;
};
/**
* URI-encodes path params and ensures no unsafe /./ or /../ path segments are introduced.
*/
const path$1 = /* @__PURE__ */ createPathTagFunction(encodeURIPath);

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/chat/completions/messages.mjs
var Messages$1 = class extends APIResource {
	/**
	* Get the messages in a stored chat completion. Only Chat Completions that have
	* been created with the `store` parameter set to `true` will be returned.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const chatCompletionStoreMessage of client.chat.completions.messages.list(
	*   'completion_id',
	* )) {
	*   // ...
	* }
	* ```
	*/
	list(completionID, query = {}, options) {
		return this._client.getAPIList(path$1`/chat/completions/${completionID}/messages`, CursorPage, {
			query,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/parser.mjs
function isChatCompletionFunctionTool(tool$1) {
	return tool$1 !== void 0 && "function" in tool$1 && tool$1.function !== void 0;
}
function isAutoParsableResponseFormat(response_format) {
	return response_format?.["$brand"] === "auto-parseable-response-format";
}
function isAutoParsableTool$1(tool$1) {
	return tool$1?.["$brand"] === "auto-parseable-tool";
}
function maybeParseChatCompletion(completion, params) {
	if (!params || !hasAutoParseableInput$1(params)) return {
		...completion,
		choices: completion.choices.map((choice) => {
			assertToolCallsAreChatCompletionFunctionToolCalls(choice.message.tool_calls);
			return {
				...choice,
				message: {
					...choice.message,
					parsed: null,
					...choice.message.tool_calls ? { tool_calls: choice.message.tool_calls } : void 0
				}
			};
		})
	};
	return parseChatCompletion(completion, params);
}
function parseChatCompletion(completion, params) {
	const choices = completion.choices.map((choice) => {
		if (choice.finish_reason === "length") throw new LengthFinishReasonError();
		if (choice.finish_reason === "content_filter") throw new ContentFilterFinishReasonError();
		assertToolCallsAreChatCompletionFunctionToolCalls(choice.message.tool_calls);
		return {
			...choice,
			message: {
				...choice.message,
				...choice.message.tool_calls ? { tool_calls: choice.message.tool_calls?.map((toolCall) => parseToolCall$1(params, toolCall)) ?? void 0 } : void 0,
				parsed: choice.message.content && !choice.message.refusal ? parseResponseFormat(params, choice.message.content) : null
			}
		};
	});
	return {
		...completion,
		choices
	};
}
function parseResponseFormat(params, content) {
	if (params.response_format?.type !== "json_schema") return null;
	if (params.response_format?.type === "json_schema") {
		if ("$parseRaw" in params.response_format) return params.response_format.$parseRaw(content);
		return JSON.parse(content);
	}
	return null;
}
function parseToolCall$1(params, toolCall) {
	const inputTool = params.tools?.find((inputTool$1) => isChatCompletionFunctionTool(inputTool$1) && inputTool$1.function?.name === toolCall.function.name);
	return {
		...toolCall,
		function: {
			...toolCall.function,
			parsed_arguments: isAutoParsableTool$1(inputTool) ? inputTool.$parseRaw(toolCall.function.arguments) : inputTool?.function.strict ? JSON.parse(toolCall.function.arguments) : null
		}
	};
}
function shouldParseToolCall(params, toolCall) {
	if (!params || !("tools" in params) || !params.tools) return false;
	const inputTool = params.tools?.find((inputTool$1) => isChatCompletionFunctionTool(inputTool$1) && inputTool$1.function?.name === toolCall.function.name);
	return isChatCompletionFunctionTool(inputTool) && (isAutoParsableTool$1(inputTool) || inputTool?.function.strict || false);
}
function hasAutoParseableInput$1(params) {
	if (isAutoParsableResponseFormat(params.response_format)) return true;
	return params.tools?.some((t) => isAutoParsableTool$1(t) || t.type === "function" && t.function.strict === true) ?? false;
}
function assertToolCallsAreChatCompletionFunctionToolCalls(toolCalls) {
	for (const toolCall of toolCalls || []) if (toolCall.type !== "function") throw new OpenAIError(`Currently only \`function\` tool calls are supported; Received \`${toolCall.type}\``);
}
function validateInputTools(tools) {
	for (const tool$1 of tools ?? []) {
		if (tool$1.type !== "function") throw new OpenAIError(`Currently only \`function\` tool types support auto-parsing; Received \`${tool$1.type}\``);
		if (tool$1.function.strict !== true) throw new OpenAIError(`The \`${tool$1.function.name}\` tool is not marked with \`strict: true\`. Only strict function tools can be auto-parsed`);
	}
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/chatCompletionUtils.mjs
const isAssistantMessage = (message) => {
	return message?.role === "assistant";
};
const isToolMessage = (message) => {
	return message?.role === "tool";
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/EventStream.mjs
var _EventStream_instances, _EventStream_connectedPromise, _EventStream_resolveConnectedPromise, _EventStream_rejectConnectedPromise, _EventStream_endPromise, _EventStream_resolveEndPromise, _EventStream_rejectEndPromise, _EventStream_listeners, _EventStream_ended, _EventStream_errored, _EventStream_aborted, _EventStream_catchingPromiseCreated, _EventStream_handleError;
var EventStream = class {
	constructor() {
		_EventStream_instances.add(this);
		this.controller = new AbortController();
		_EventStream_connectedPromise.set(this, void 0);
		_EventStream_resolveConnectedPromise.set(this, () => {});
		_EventStream_rejectConnectedPromise.set(this, () => {});
		_EventStream_endPromise.set(this, void 0);
		_EventStream_resolveEndPromise.set(this, () => {});
		_EventStream_rejectEndPromise.set(this, () => {});
		_EventStream_listeners.set(this, {});
		_EventStream_ended.set(this, false);
		_EventStream_errored.set(this, false);
		_EventStream_aborted.set(this, false);
		_EventStream_catchingPromiseCreated.set(this, false);
		__classPrivateFieldSet(this, _EventStream_connectedPromise, new Promise((resolve, reject) => {
			__classPrivateFieldSet(this, _EventStream_resolveConnectedPromise, resolve, "f");
			__classPrivateFieldSet(this, _EventStream_rejectConnectedPromise, reject, "f");
		}), "f");
		__classPrivateFieldSet(this, _EventStream_endPromise, new Promise((resolve, reject) => {
			__classPrivateFieldSet(this, _EventStream_resolveEndPromise, resolve, "f");
			__classPrivateFieldSet(this, _EventStream_rejectEndPromise, reject, "f");
		}), "f");
		__classPrivateFieldGet(this, _EventStream_connectedPromise, "f").catch(() => {});
		__classPrivateFieldGet(this, _EventStream_endPromise, "f").catch(() => {});
	}
	_run(executor) {
		setTimeout(() => {
			executor().then(() => {
				this._emitFinal();
				this._emit("end");
			}, __classPrivateFieldGet(this, _EventStream_instances, "m", _EventStream_handleError).bind(this));
		}, 0);
	}
	_connected() {
		if (this.ended) return;
		__classPrivateFieldGet(this, _EventStream_resolveConnectedPromise, "f").call(this);
		this._emit("connect");
	}
	get ended() {
		return __classPrivateFieldGet(this, _EventStream_ended, "f");
	}
	get errored() {
		return __classPrivateFieldGet(this, _EventStream_errored, "f");
	}
	get aborted() {
		return __classPrivateFieldGet(this, _EventStream_aborted, "f");
	}
	abort() {
		this.controller.abort();
	}
	/**
	* Adds the listener function to the end of the listeners array for the event.
	* No checks are made to see if the listener has already been added. Multiple calls passing
	* the same combination of event and listener will result in the listener being added, and
	* called, multiple times.
	* @returns this ChatCompletionStream, so that calls can be chained
	*/
	on(event, listener) {
		(__classPrivateFieldGet(this, _EventStream_listeners, "f")[event] || (__classPrivateFieldGet(this, _EventStream_listeners, "f")[event] = [])).push({ listener });
		return this;
	}
	/**
	* Removes the specified listener from the listener array for the event.
	* off() will remove, at most, one instance of a listener from the listener array. If any single
	* listener has been added multiple times to the listener array for the specified event, then
	* off() must be called multiple times to remove each instance.
	* @returns this ChatCompletionStream, so that calls can be chained
	*/
	off(event, listener) {
		const listeners = __classPrivateFieldGet(this, _EventStream_listeners, "f")[event];
		if (!listeners) return this;
		const index = listeners.findIndex((l) => l.listener === listener);
		if (index >= 0) listeners.splice(index, 1);
		return this;
	}
	/**
	* Adds a one-time listener function for the event. The next time the event is triggered,
	* this listener is removed and then invoked.
	* @returns this ChatCompletionStream, so that calls can be chained
	*/
	once(event, listener) {
		(__classPrivateFieldGet(this, _EventStream_listeners, "f")[event] || (__classPrivateFieldGet(this, _EventStream_listeners, "f")[event] = [])).push({
			listener,
			once: true
		});
		return this;
	}
	/**
	* This is similar to `.once()`, but returns a Promise that resolves the next time
	* the event is triggered, instead of calling a listener callback.
	* @returns a Promise that resolves the next time given event is triggered,
	* or rejects if an error is emitted.  (If you request the 'error' event,
	* returns a promise that resolves with the error).
	*
	* Example:
	*
	*   const message = await stream.emitted('message') // rejects if the stream errors
	*/
	emitted(event) {
		return new Promise((resolve, reject) => {
			__classPrivateFieldSet(this, _EventStream_catchingPromiseCreated, true, "f");
			if (event !== "error") this.once("error", reject);
			this.once(event, resolve);
		});
	}
	async done() {
		__classPrivateFieldSet(this, _EventStream_catchingPromiseCreated, true, "f");
		await __classPrivateFieldGet(this, _EventStream_endPromise, "f");
	}
	_emit(event, ...args) {
		if (__classPrivateFieldGet(this, _EventStream_ended, "f")) return;
		if (event === "end") {
			__classPrivateFieldSet(this, _EventStream_ended, true, "f");
			__classPrivateFieldGet(this, _EventStream_resolveEndPromise, "f").call(this);
		}
		const listeners = __classPrivateFieldGet(this, _EventStream_listeners, "f")[event];
		if (listeners) {
			__classPrivateFieldGet(this, _EventStream_listeners, "f")[event] = listeners.filter((l) => !l.once);
			listeners.forEach(({ listener }) => listener(...args));
		}
		if (event === "abort") {
			const error = args[0];
			if (!__classPrivateFieldGet(this, _EventStream_catchingPromiseCreated, "f") && !listeners?.length) Promise.reject(error);
			__classPrivateFieldGet(this, _EventStream_rejectConnectedPromise, "f").call(this, error);
			__classPrivateFieldGet(this, _EventStream_rejectEndPromise, "f").call(this, error);
			this._emit("end");
			return;
		}
		if (event === "error") {
			const error = args[0];
			if (!__classPrivateFieldGet(this, _EventStream_catchingPromiseCreated, "f") && !listeners?.length) Promise.reject(error);
			__classPrivateFieldGet(this, _EventStream_rejectConnectedPromise, "f").call(this, error);
			__classPrivateFieldGet(this, _EventStream_rejectEndPromise, "f").call(this, error);
			this._emit("end");
		}
	}
	_emitFinal() {}
};
_EventStream_connectedPromise = /* @__PURE__ */ new WeakMap(), _EventStream_resolveConnectedPromise = /* @__PURE__ */ new WeakMap(), _EventStream_rejectConnectedPromise = /* @__PURE__ */ new WeakMap(), _EventStream_endPromise = /* @__PURE__ */ new WeakMap(), _EventStream_resolveEndPromise = /* @__PURE__ */ new WeakMap(), _EventStream_rejectEndPromise = /* @__PURE__ */ new WeakMap(), _EventStream_listeners = /* @__PURE__ */ new WeakMap(), _EventStream_ended = /* @__PURE__ */ new WeakMap(), _EventStream_errored = /* @__PURE__ */ new WeakMap(), _EventStream_aborted = /* @__PURE__ */ new WeakMap(), _EventStream_catchingPromiseCreated = /* @__PURE__ */ new WeakMap(), _EventStream_instances = /* @__PURE__ */ new WeakSet(), _EventStream_handleError = function _EventStream_handleError$1(error) {
	__classPrivateFieldSet(this, _EventStream_errored, true, "f");
	if (error instanceof Error && error.name === "AbortError") error = new APIUserAbortError();
	if (error instanceof APIUserAbortError) {
		__classPrivateFieldSet(this, _EventStream_aborted, true, "f");
		return this._emit("abort", error);
	}
	if (error instanceof OpenAIError) return this._emit("error", error);
	if (error instanceof Error) {
		const openAIError = new OpenAIError(error.message);
		openAIError.cause = error;
		return this._emit("error", openAIError);
	}
	return this._emit("error", new OpenAIError(String(error)));
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/RunnableFunction.mjs
function isRunnableFunctionWithParse(fn) {
	return typeof fn.parse === "function";
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/AbstractChatCompletionRunner.mjs
var _AbstractChatCompletionRunner_instances, _AbstractChatCompletionRunner_getFinalContent, _AbstractChatCompletionRunner_getFinalMessage, _AbstractChatCompletionRunner_getFinalFunctionToolCall, _AbstractChatCompletionRunner_getFinalFunctionToolCallResult, _AbstractChatCompletionRunner_calculateTotalUsage, _AbstractChatCompletionRunner_validateParams, _AbstractChatCompletionRunner_stringifyFunctionCallResult;
const DEFAULT_MAX_CHAT_COMPLETIONS = 10;
var AbstractChatCompletionRunner = class extends EventStream {
	constructor() {
		super(...arguments);
		_AbstractChatCompletionRunner_instances.add(this);
		this._chatCompletions = [];
		this.messages = [];
	}
	_addChatCompletion(chatCompletion) {
		this._chatCompletions.push(chatCompletion);
		this._emit("chatCompletion", chatCompletion);
		const message = chatCompletion.choices[0]?.message;
		if (message) this._addMessage(message);
		return chatCompletion;
	}
	_addMessage(message, emit = true) {
		if (!("content" in message)) message.content = null;
		this.messages.push(message);
		if (emit) {
			this._emit("message", message);
			if (isToolMessage(message) && message.content) this._emit("functionToolCallResult", message.content);
			else if (isAssistantMessage(message) && message.tool_calls) {
				for (const tool_call of message.tool_calls) if (tool_call.type === "function") this._emit("functionToolCall", tool_call.function);
			}
		}
	}
	/**
	* @returns a promise that resolves with the final ChatCompletion, or rejects
	* if an error occurred or the stream ended prematurely without producing a ChatCompletion.
	*/
	async finalChatCompletion() {
		await this.done();
		const completion = this._chatCompletions[this._chatCompletions.length - 1];
		if (!completion) throw new OpenAIError("stream ended without producing a ChatCompletion");
		return completion;
	}
	/**
	* @returns a promise that resolves with the content of the final ChatCompletionMessage, or rejects
	* if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
	*/
	async finalContent() {
		await this.done();
		return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalContent).call(this);
	}
	/**
	* @returns a promise that resolves with the the final assistant ChatCompletionMessage response,
	* or rejects if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
	*/
	async finalMessage() {
		await this.done();
		return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this);
	}
	/**
	* @returns a promise that resolves with the content of the final FunctionCall, or rejects
	* if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.
	*/
	async finalFunctionToolCall() {
		await this.done();
		return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionToolCall).call(this);
	}
	async finalFunctionToolCallResult() {
		await this.done();
		return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionToolCallResult).call(this);
	}
	async totalUsage() {
		await this.done();
		return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_calculateTotalUsage).call(this);
	}
	allChatCompletions() {
		return [...this._chatCompletions];
	}
	_emitFinal() {
		const completion = this._chatCompletions[this._chatCompletions.length - 1];
		if (completion) this._emit("finalChatCompletion", completion);
		const finalMessage = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this);
		if (finalMessage) this._emit("finalMessage", finalMessage);
		const finalContent = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalContent).call(this);
		if (finalContent) this._emit("finalContent", finalContent);
		const finalFunctionCall = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionToolCall).call(this);
		if (finalFunctionCall) this._emit("finalFunctionToolCall", finalFunctionCall);
		const finalFunctionCallResult = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalFunctionToolCallResult).call(this);
		if (finalFunctionCallResult != null) this._emit("finalFunctionToolCallResult", finalFunctionCallResult);
		if (this._chatCompletions.some((c) => c.usage)) this._emit("totalUsage", __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_calculateTotalUsage).call(this));
	}
	async _createChatCompletion(client, params, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		__classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_validateParams).call(this, params);
		const chatCompletion = await client.chat.completions.create({
			...params,
			stream: false
		}, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		return this._addChatCompletion(parseChatCompletion(chatCompletion, params));
	}
	async _runChatCompletion(client, params, options) {
		for (const message of params.messages) this._addMessage(message, false);
		return await this._createChatCompletion(client, params, options);
	}
	async _runTools(client, params, options) {
		const role = "tool";
		const { tool_choice = "auto", stream, ...restParams } = params;
		const singleFunctionToCall = typeof tool_choice !== "string" && tool_choice.type === "function" && tool_choice?.function?.name;
		const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};
		const inputTools = params.tools.map((tool$1) => {
			if (isAutoParsableTool$1(tool$1)) {
				if (!tool$1.$callback) throw new OpenAIError("Tool given to `.runTools()` that does not have an associated function");
				return {
					type: "function",
					function: {
						function: tool$1.$callback,
						name: tool$1.function.name,
						description: tool$1.function.description || "",
						parameters: tool$1.function.parameters,
						parse: tool$1.$parseRaw,
						strict: true
					}
				};
			}
			return tool$1;
		});
		const functionsByName = {};
		for (const f of inputTools) if (f.type === "function") functionsByName[f.function.name || f.function.function.name] = f.function;
		const tools = "tools" in params ? inputTools.map((t) => t.type === "function" ? {
			type: "function",
			function: {
				name: t.function.name || t.function.function.name,
				parameters: t.function.parameters,
				description: t.function.description,
				strict: t.function.strict
			}
		} : t) : void 0;
		for (const message of params.messages) this._addMessage(message, false);
		for (let i = 0; i < maxChatCompletions; ++i) {
			const message = (await this._createChatCompletion(client, {
				...restParams,
				tool_choice,
				tools,
				messages: [...this.messages]
			}, options)).choices[0]?.message;
			if (!message) throw new OpenAIError(`missing message in ChatCompletion response`);
			if (!message.tool_calls?.length) return;
			for (const tool_call of message.tool_calls) {
				if (tool_call.type !== "function") continue;
				const tool_call_id = tool_call.id;
				const { name, arguments: args } = tool_call.function;
				const fn = functionsByName[name];
				if (!fn) {
					const content$1 = `Invalid tool_call: ${JSON.stringify(name)}. Available options are: ${Object.keys(functionsByName).map((name$1) => JSON.stringify(name$1)).join(", ")}. Please try again`;
					this._addMessage({
						role,
						tool_call_id,
						content: content$1
					});
					continue;
				} else if (singleFunctionToCall && singleFunctionToCall !== name) {
					const content$1 = `Invalid tool_call: ${JSON.stringify(name)}. ${JSON.stringify(singleFunctionToCall)} requested. Please try again`;
					this._addMessage({
						role,
						tool_call_id,
						content: content$1
					});
					continue;
				}
				let parsed;
				try {
					parsed = isRunnableFunctionWithParse(fn) ? await fn.parse(args) : args;
				} catch (error) {
					const content$1 = error instanceof Error ? error.message : String(error);
					this._addMessage({
						role,
						tool_call_id,
						content: content$1
					});
					continue;
				}
				const rawContent = await fn.function(parsed, this);
				const content = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_stringifyFunctionCallResult).call(this, rawContent);
				this._addMessage({
					role,
					tool_call_id,
					content
				});
				if (singleFunctionToCall) return;
			}
		}
	}
};
_AbstractChatCompletionRunner_instances = /* @__PURE__ */ new WeakSet(), _AbstractChatCompletionRunner_getFinalContent = function _AbstractChatCompletionRunner_getFinalContent$1() {
	return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, "m", _AbstractChatCompletionRunner_getFinalMessage).call(this).content ?? null;
}, _AbstractChatCompletionRunner_getFinalMessage = function _AbstractChatCompletionRunner_getFinalMessage$1() {
	let i = this.messages.length;
	while (i-- > 0) {
		const message = this.messages[i];
		if (isAssistantMessage(message)) return {
			...message,
			content: message.content ?? null,
			refusal: message.refusal ?? null
		};
	}
	throw new OpenAIError("stream ended without producing a ChatCompletionMessage with role=assistant");
}, _AbstractChatCompletionRunner_getFinalFunctionToolCall = function _AbstractChatCompletionRunner_getFinalFunctionToolCall$1() {
	for (let i = this.messages.length - 1; i >= 0; i--) {
		const message = this.messages[i];
		if (isAssistantMessage(message) && message?.tool_calls?.length) return message.tool_calls.filter((x) => x.type === "function").at(-1)?.function;
	}
}, _AbstractChatCompletionRunner_getFinalFunctionToolCallResult = function _AbstractChatCompletionRunner_getFinalFunctionToolCallResult$1() {
	for (let i = this.messages.length - 1; i >= 0; i--) {
		const message = this.messages[i];
		if (isToolMessage(message) && message.content != null && typeof message.content === "string" && this.messages.some((x) => x.role === "assistant" && x.tool_calls?.some((y) => y.type === "function" && y.id === message.tool_call_id))) return message.content;
	}
}, _AbstractChatCompletionRunner_calculateTotalUsage = function _AbstractChatCompletionRunner_calculateTotalUsage$1() {
	const total = {
		completion_tokens: 0,
		prompt_tokens: 0,
		total_tokens: 0
	};
	for (const { usage } of this._chatCompletions) if (usage) {
		total.completion_tokens += usage.completion_tokens;
		total.prompt_tokens += usage.prompt_tokens;
		total.total_tokens += usage.total_tokens;
	}
	return total;
}, _AbstractChatCompletionRunner_validateParams = function _AbstractChatCompletionRunner_validateParams$1(params) {
	if (params.n != null && params.n > 1) throw new OpenAIError("ChatCompletion convenience helpers only support n=1 at this time. To use n>1, please use chat.completions.create() directly.");
}, _AbstractChatCompletionRunner_stringifyFunctionCallResult = function _AbstractChatCompletionRunner_stringifyFunctionCallResult$1(rawContent) {
	return typeof rawContent === "string" ? rawContent : rawContent === void 0 ? "undefined" : JSON.stringify(rawContent);
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/ChatCompletionRunner.mjs
var ChatCompletionRunner = class ChatCompletionRunner extends AbstractChatCompletionRunner {
	static runTools(client, params, options) {
		const runner = new ChatCompletionRunner();
		const opts = {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "runTools"
			}
		};
		runner._run(() => runner._runTools(client, params, opts));
		return runner;
	}
	_addMessage(message, emit = true) {
		super._addMessage(message, emit);
		if (isAssistantMessage(message) && message.content) this._emit("content", message.content);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/_vendor/partial-json-parser/parser.mjs
const STR = 1;
const NUM = 2;
const ARR = 4;
const OBJ = 8;
const NULL = 16;
const BOOL = 32;
const NAN = 64;
const INFINITY = 128;
const MINUS_INFINITY = 256;
const INF = INFINITY | MINUS_INFINITY;
const SPECIAL = 496;
const ATOM = NUM | 497;
const COLLECTION = ARR | OBJ;
const Allow = {
	STR,
	NUM,
	ARR,
	OBJ,
	NULL,
	BOOL,
	NAN,
	INFINITY,
	MINUS_INFINITY,
	INF,
	SPECIAL,
	ATOM,
	COLLECTION,
	ALL: ATOM | COLLECTION
};
var PartialJSON = class extends Error {};
var MalformedJSON = class extends Error {};
/**
* Parse incomplete JSON
* @param {string} jsonString Partial JSON to be parsed
* @param {number} allowPartial Specify what types are allowed to be partial, see {@link Allow} for details
* @returns The parsed JSON
* @throws {PartialJSON} If the JSON is incomplete (related to the `allow` parameter)
* @throws {MalformedJSON} If the JSON is malformed
*/
function parseJSON(jsonString, allowPartial = Allow.ALL) {
	if (typeof jsonString !== "string") throw new TypeError(`expecting str, got ${typeof jsonString}`);
	if (!jsonString.trim()) throw new Error(`${jsonString} is empty`);
	return _parseJSON(jsonString.trim(), allowPartial);
}
const _parseJSON = (jsonString, allow) => {
	const length = jsonString.length;
	let index = 0;
	const markPartialJSON = (msg) => {
		throw new PartialJSON(`${msg} at position ${index}`);
	};
	const throwMalformedError = (msg) => {
		throw new MalformedJSON(`${msg} at position ${index}`);
	};
	const parseAny = () => {
		skipBlank();
		if (index >= length) markPartialJSON("Unexpected end of input");
		if (jsonString[index] === "\"") return parseStr();
		if (jsonString[index] === "{") return parseObj();
		if (jsonString[index] === "[") return parseArr();
		if (jsonString.substring(index, index + 4) === "null" || Allow.NULL & allow && length - index < 4 && "null".startsWith(jsonString.substring(index))) {
			index += 4;
			return null;
		}
		if (jsonString.substring(index, index + 4) === "true" || Allow.BOOL & allow && length - index < 4 && "true".startsWith(jsonString.substring(index))) {
			index += 4;
			return true;
		}
		if (jsonString.substring(index, index + 5) === "false" || Allow.BOOL & allow && length - index < 5 && "false".startsWith(jsonString.substring(index))) {
			index += 5;
			return false;
		}
		if (jsonString.substring(index, index + 8) === "Infinity" || Allow.INFINITY & allow && length - index < 8 && "Infinity".startsWith(jsonString.substring(index))) {
			index += 8;
			return Infinity;
		}
		if (jsonString.substring(index, index + 9) === "-Infinity" || Allow.MINUS_INFINITY & allow && 1 < length - index && length - index < 9 && "-Infinity".startsWith(jsonString.substring(index))) {
			index += 9;
			return -Infinity;
		}
		if (jsonString.substring(index, index + 3) === "NaN" || Allow.NAN & allow && length - index < 3 && "NaN".startsWith(jsonString.substring(index))) {
			index += 3;
			return NaN;
		}
		return parseNum();
	};
	const parseStr = () => {
		const start = index;
		let escape$1 = false;
		index++;
		while (index < length && (jsonString[index] !== "\"" || escape$1 && jsonString[index - 1] === "\\")) {
			escape$1 = jsonString[index] === "\\" ? !escape$1 : false;
			index++;
		}
		if (jsonString.charAt(index) == "\"") try {
			return JSON.parse(jsonString.substring(start, ++index - Number(escape$1)));
		} catch (e) {
			throwMalformedError(String(e));
		}
		else if (Allow.STR & allow) try {
			return JSON.parse(jsonString.substring(start, index - Number(escape$1)) + "\"");
		} catch (e) {
			return JSON.parse(jsonString.substring(start, jsonString.lastIndexOf("\\")) + "\"");
		}
		markPartialJSON("Unterminated string literal");
	};
	const parseObj = () => {
		index++;
		skipBlank();
		const obj = {};
		try {
			while (jsonString[index] !== "}") {
				skipBlank();
				if (index >= length && Allow.OBJ & allow) return obj;
				const key = parseStr();
				skipBlank();
				index++;
				try {
					const value = parseAny();
					Object.defineProperty(obj, key, {
						value,
						writable: true,
						enumerable: true,
						configurable: true
					});
				} catch (e) {
					if (Allow.OBJ & allow) return obj;
					else throw e;
				}
				skipBlank();
				if (jsonString[index] === ",") index++;
			}
		} catch (e) {
			if (Allow.OBJ & allow) return obj;
			else markPartialJSON("Expected '}' at end of object");
		}
		index++;
		return obj;
	};
	const parseArr = () => {
		index++;
		const arr = [];
		try {
			while (jsonString[index] !== "]") {
				arr.push(parseAny());
				skipBlank();
				if (jsonString[index] === ",") index++;
			}
		} catch (e) {
			if (Allow.ARR & allow) return arr;
			markPartialJSON("Expected ']' at end of array");
		}
		index++;
		return arr;
	};
	const parseNum = () => {
		if (index === 0) {
			if (jsonString === "-" && Allow.NUM & allow) markPartialJSON("Not sure what '-' is");
			try {
				return JSON.parse(jsonString);
			} catch (e) {
				if (Allow.NUM & allow) try {
					if ("." === jsonString[jsonString.length - 1]) return JSON.parse(jsonString.substring(0, jsonString.lastIndexOf(".")));
					return JSON.parse(jsonString.substring(0, jsonString.lastIndexOf("e")));
				} catch (e$1) {}
				throwMalformedError(String(e));
			}
		}
		const start = index;
		if (jsonString[index] === "-") index++;
		while (jsonString[index] && !",]}".includes(jsonString[index])) index++;
		if (index == length && !(Allow.NUM & allow)) markPartialJSON("Unterminated number literal");
		try {
			return JSON.parse(jsonString.substring(start, index));
		} catch (e) {
			if (jsonString.substring(start, index) === "-" && Allow.NUM & allow) markPartialJSON("Not sure what '-' is");
			try {
				return JSON.parse(jsonString.substring(start, jsonString.lastIndexOf("e")));
			} catch (e$1) {
				throwMalformedError(String(e$1));
			}
		}
	};
	const skipBlank = () => {
		while (index < length && " \n\r	".includes(jsonString[index])) index++;
	};
	return parseAny();
};
const partialParse = (input) => parseJSON(input, Allow.ALL ^ Allow.NUM);

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/ChatCompletionStream.mjs
var _ChatCompletionStream_instances, _ChatCompletionStream_params, _ChatCompletionStream_choiceEventStates, _ChatCompletionStream_currentChatCompletionSnapshot, _ChatCompletionStream_beginRequest, _ChatCompletionStream_getChoiceEventState, _ChatCompletionStream_addChunk, _ChatCompletionStream_emitToolCallDoneEvent, _ChatCompletionStream_emitContentDoneEvents, _ChatCompletionStream_endRequest, _ChatCompletionStream_getAutoParseableResponseFormat, _ChatCompletionStream_accumulateChatCompletion;
var ChatCompletionStream = class ChatCompletionStream extends AbstractChatCompletionRunner {
	constructor(params) {
		super();
		_ChatCompletionStream_instances.add(this);
		_ChatCompletionStream_params.set(this, void 0);
		_ChatCompletionStream_choiceEventStates.set(this, void 0);
		_ChatCompletionStream_currentChatCompletionSnapshot.set(this, void 0);
		__classPrivateFieldSet(this, _ChatCompletionStream_params, params, "f");
		__classPrivateFieldSet(this, _ChatCompletionStream_choiceEventStates, [], "f");
	}
	get currentChatCompletionSnapshot() {
		return __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
	}
	/**
	* Intended for use on the frontend, consuming a stream produced with
	* `.toReadableStream()` on the backend.
	*
	* Note that messages sent to the model do not appear in `.on('message')`
	* in this context.
	*/
	static fromReadableStream(stream) {
		const runner = new ChatCompletionStream(null);
		runner._run(() => runner._fromReadableStream(stream));
		return runner;
	}
	static createChatCompletion(client, params, options) {
		const runner = new ChatCompletionStream(params);
		runner._run(() => runner._runChatCompletion(client, {
			...params,
			stream: true
		}, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "stream"
			}
		}));
		return runner;
	}
	async _createChatCompletion(client, params, options) {
		super._createChatCompletion;
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_beginRequest).call(this);
		const stream = await client.chat.completions.create({
			...params,
			stream: true
		}, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		for await (const chunk of stream) __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_addChunk).call(this, chunk);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
	}
	async _fromReadableStream(readableStream, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_beginRequest).call(this);
		this._connected();
		const stream = Stream.fromReadableStream(readableStream, this.controller);
		let chatId;
		for await (const chunk of stream) {
			if (chatId && chatId !== chunk.id) this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
			__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_addChunk).call(this, chunk);
			chatId = chunk.id;
		}
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_endRequest).call(this));
	}
	[(_ChatCompletionStream_params = /* @__PURE__ */ new WeakMap(), _ChatCompletionStream_choiceEventStates = /* @__PURE__ */ new WeakMap(), _ChatCompletionStream_currentChatCompletionSnapshot = /* @__PURE__ */ new WeakMap(), _ChatCompletionStream_instances = /* @__PURE__ */ new WeakSet(), _ChatCompletionStream_beginRequest = function _ChatCompletionStream_beginRequest$1() {
		if (this.ended) return;
		__classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, void 0, "f");
	}, _ChatCompletionStream_getChoiceEventState = function _ChatCompletionStream_getChoiceEventState$1(choice) {
		let state = __classPrivateFieldGet(this, _ChatCompletionStream_choiceEventStates, "f")[choice.index];
		if (state) return state;
		state = {
			content_done: false,
			refusal_done: false,
			logprobs_content_done: false,
			logprobs_refusal_done: false,
			done_tool_calls: /* @__PURE__ */ new Set(),
			current_tool_call_index: null
		};
		__classPrivateFieldGet(this, _ChatCompletionStream_choiceEventStates, "f")[choice.index] = state;
		return state;
	}, _ChatCompletionStream_addChunk = function _ChatCompletionStream_addChunk$1(chunk) {
		if (this.ended) return;
		const completion = __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_accumulateChatCompletion).call(this, chunk);
		this._emit("chunk", chunk, completion);
		for (const choice of chunk.choices) {
			const choiceSnapshot = completion.choices[choice.index];
			if (choice.delta.content != null && choiceSnapshot.message?.role === "assistant" && choiceSnapshot.message?.content) {
				this._emit("content", choice.delta.content, choiceSnapshot.message.content);
				this._emit("content.delta", {
					delta: choice.delta.content,
					snapshot: choiceSnapshot.message.content,
					parsed: choiceSnapshot.message.parsed
				});
			}
			if (choice.delta.refusal != null && choiceSnapshot.message?.role === "assistant" && choiceSnapshot.message?.refusal) this._emit("refusal.delta", {
				delta: choice.delta.refusal,
				snapshot: choiceSnapshot.message.refusal
			});
			if (choice.logprobs?.content != null && choiceSnapshot.message?.role === "assistant") this._emit("logprobs.content.delta", {
				content: choice.logprobs?.content,
				snapshot: choiceSnapshot.logprobs?.content ?? []
			});
			if (choice.logprobs?.refusal != null && choiceSnapshot.message?.role === "assistant") this._emit("logprobs.refusal.delta", {
				refusal: choice.logprobs?.refusal,
				snapshot: choiceSnapshot.logprobs?.refusal ?? []
			});
			const state = __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_getChoiceEventState).call(this, choiceSnapshot);
			if (choiceSnapshot.finish_reason) {
				__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_emitContentDoneEvents).call(this, choiceSnapshot);
				if (state.current_tool_call_index != null) __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_emitToolCallDoneEvent).call(this, choiceSnapshot, state.current_tool_call_index);
			}
			for (const toolCall of choice.delta.tool_calls ?? []) {
				if (state.current_tool_call_index !== toolCall.index) {
					__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_emitContentDoneEvents).call(this, choiceSnapshot);
					if (state.current_tool_call_index != null) __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_emitToolCallDoneEvent).call(this, choiceSnapshot, state.current_tool_call_index);
				}
				state.current_tool_call_index = toolCall.index;
			}
			for (const toolCallDelta of choice.delta.tool_calls ?? []) {
				const toolCallSnapshot = choiceSnapshot.message.tool_calls?.[toolCallDelta.index];
				if (!toolCallSnapshot?.type) continue;
				if (toolCallSnapshot?.type === "function") this._emit("tool_calls.function.arguments.delta", {
					name: toolCallSnapshot.function?.name,
					index: toolCallDelta.index,
					arguments: toolCallSnapshot.function.arguments,
					parsed_arguments: toolCallSnapshot.function.parsed_arguments,
					arguments_delta: toolCallDelta.function?.arguments ?? ""
				});
				else assertNever$1(toolCallSnapshot?.type);
			}
		}
	}, _ChatCompletionStream_emitToolCallDoneEvent = function _ChatCompletionStream_emitToolCallDoneEvent$1(choiceSnapshot, toolCallIndex) {
		if (__classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_getChoiceEventState).call(this, choiceSnapshot).done_tool_calls.has(toolCallIndex)) return;
		const toolCallSnapshot = choiceSnapshot.message.tool_calls?.[toolCallIndex];
		if (!toolCallSnapshot) throw new Error("no tool call snapshot");
		if (!toolCallSnapshot.type) throw new Error("tool call snapshot missing `type`");
		if (toolCallSnapshot.type === "function") {
			const inputTool = __classPrivateFieldGet(this, _ChatCompletionStream_params, "f")?.tools?.find((tool$1) => isChatCompletionFunctionTool(tool$1) && tool$1.function.name === toolCallSnapshot.function.name);
			this._emit("tool_calls.function.arguments.done", {
				name: toolCallSnapshot.function.name,
				index: toolCallIndex,
				arguments: toolCallSnapshot.function.arguments,
				parsed_arguments: isAutoParsableTool$1(inputTool) ? inputTool.$parseRaw(toolCallSnapshot.function.arguments) : inputTool?.function.strict ? JSON.parse(toolCallSnapshot.function.arguments) : null
			});
		} else assertNever$1(toolCallSnapshot.type);
	}, _ChatCompletionStream_emitContentDoneEvents = function _ChatCompletionStream_emitContentDoneEvents$1(choiceSnapshot) {
		const state = __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_getChoiceEventState).call(this, choiceSnapshot);
		if (choiceSnapshot.message.content && !state.content_done) {
			state.content_done = true;
			const responseFormat = __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_getAutoParseableResponseFormat).call(this);
			this._emit("content.done", {
				content: choiceSnapshot.message.content,
				parsed: responseFormat ? responseFormat.$parseRaw(choiceSnapshot.message.content) : null
			});
		}
		if (choiceSnapshot.message.refusal && !state.refusal_done) {
			state.refusal_done = true;
			this._emit("refusal.done", { refusal: choiceSnapshot.message.refusal });
		}
		if (choiceSnapshot.logprobs?.content && !state.logprobs_content_done) {
			state.logprobs_content_done = true;
			this._emit("logprobs.content.done", { content: choiceSnapshot.logprobs.content });
		}
		if (choiceSnapshot.logprobs?.refusal && !state.logprobs_refusal_done) {
			state.logprobs_refusal_done = true;
			this._emit("logprobs.refusal.done", { refusal: choiceSnapshot.logprobs.refusal });
		}
	}, _ChatCompletionStream_endRequest = function _ChatCompletionStream_endRequest$1() {
		if (this.ended) throw new OpenAIError(`stream has ended, this shouldn't happen`);
		const snapshot = __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
		if (!snapshot) throw new OpenAIError(`request ended without sending any chunks`);
		__classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, void 0, "f");
		__classPrivateFieldSet(this, _ChatCompletionStream_choiceEventStates, [], "f");
		return finalizeChatCompletion(snapshot, __classPrivateFieldGet(this, _ChatCompletionStream_params, "f"));
	}, _ChatCompletionStream_getAutoParseableResponseFormat = function _ChatCompletionStream_getAutoParseableResponseFormat$1() {
		const responseFormat = __classPrivateFieldGet(this, _ChatCompletionStream_params, "f")?.response_format;
		if (isAutoParsableResponseFormat(responseFormat)) return responseFormat;
		return null;
	}, _ChatCompletionStream_accumulateChatCompletion = function _ChatCompletionStream_accumulateChatCompletion$1(chunk) {
		var _a$2, _b, _c, _d;
		let snapshot = __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, "f");
		const { choices, ...rest } = chunk;
		if (!snapshot) snapshot = __classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, {
			...rest,
			choices: []
		}, "f");
		else Object.assign(snapshot, rest);
		for (const { delta, finish_reason, index, logprobs = null, ...other } of chunk.choices) {
			let choice = snapshot.choices[index];
			if (!choice) choice = snapshot.choices[index] = {
				finish_reason,
				index,
				message: {},
				logprobs,
				...other
			};
			if (logprobs) if (!choice.logprobs) choice.logprobs = Object.assign({}, logprobs);
			else {
				const { content: content$1, refusal: refusal$1, ...rest$2 } = logprobs;
				assertIsEmpty(rest$2);
				Object.assign(choice.logprobs, rest$2);
				if (content$1) {
					(_a$2 = choice.logprobs).content ?? (_a$2.content = []);
					choice.logprobs.content.push(...content$1);
				}
				if (refusal$1) {
					(_b = choice.logprobs).refusal ?? (_b.refusal = []);
					choice.logprobs.refusal.push(...refusal$1);
				}
			}
			if (finish_reason) {
				choice.finish_reason = finish_reason;
				if (__classPrivateFieldGet(this, _ChatCompletionStream_params, "f") && hasAutoParseableInput$1(__classPrivateFieldGet(this, _ChatCompletionStream_params, "f"))) {
					if (finish_reason === "length") throw new LengthFinishReasonError();
					if (finish_reason === "content_filter") throw new ContentFilterFinishReasonError();
				}
			}
			Object.assign(choice, other);
			if (!delta) continue;
			const { content, refusal, function_call, role, tool_calls, ...rest$1 } = delta;
			assertIsEmpty(rest$1);
			Object.assign(choice.message, rest$1);
			if (refusal) choice.message.refusal = (choice.message.refusal || "") + refusal;
			if (role) choice.message.role = role;
			if (function_call) if (!choice.message.function_call) choice.message.function_call = function_call;
			else {
				if (function_call.name) choice.message.function_call.name = function_call.name;
				if (function_call.arguments) {
					(_c = choice.message.function_call).arguments ?? (_c.arguments = "");
					choice.message.function_call.arguments += function_call.arguments;
				}
			}
			if (content) {
				choice.message.content = (choice.message.content || "") + content;
				if (!choice.message.refusal && __classPrivateFieldGet(this, _ChatCompletionStream_instances, "m", _ChatCompletionStream_getAutoParseableResponseFormat).call(this)) choice.message.parsed = partialParse(choice.message.content);
			}
			if (tool_calls) {
				if (!choice.message.tool_calls) choice.message.tool_calls = [];
				for (const { index: index$1, id, type, function: fn, ...rest$2 } of tool_calls) {
					const tool_call = (_d = choice.message.tool_calls)[index$1] ?? (_d[index$1] = {});
					Object.assign(tool_call, rest$2);
					if (id) tool_call.id = id;
					if (type) tool_call.type = type;
					if (fn) tool_call.function ?? (tool_call.function = {
						name: fn.name ?? "",
						arguments: ""
					});
					if (fn?.name) tool_call.function.name = fn.name;
					if (fn?.arguments) {
						tool_call.function.arguments += fn.arguments;
						if (shouldParseToolCall(__classPrivateFieldGet(this, _ChatCompletionStream_params, "f"), tool_call)) tool_call.function.parsed_arguments = partialParse(tool_call.function.arguments);
					}
				}
			}
		}
		return snapshot;
	}, Symbol.asyncIterator)]() {
		const pushQueue = [];
		const readQueue = [];
		let done = false;
		this.on("chunk", (chunk) => {
			const reader = readQueue.shift();
			if (reader) reader.resolve(chunk);
			else pushQueue.push(chunk);
		});
		this.on("end", () => {
			done = true;
			for (const reader of readQueue) reader.resolve(void 0);
			readQueue.length = 0;
		});
		this.on("abort", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		this.on("error", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		return {
			next: async () => {
				if (!pushQueue.length) {
					if (done) return {
						value: void 0,
						done: true
					};
					return new Promise((resolve, reject) => readQueue.push({
						resolve,
						reject
					})).then((chunk) => chunk ? {
						value: chunk,
						done: false
					} : {
						value: void 0,
						done: true
					});
				}
				return {
					value: pushQueue.shift(),
					done: false
				};
			},
			return: async () => {
				this.abort();
				return {
					value: void 0,
					done: true
				};
			}
		};
	}
	toReadableStream() {
		return new Stream(this[Symbol.asyncIterator].bind(this), this.controller).toReadableStream();
	}
};
function finalizeChatCompletion(snapshot, params) {
	const { id, choices, created, model, system_fingerprint, ...rest } = snapshot;
	return maybeParseChatCompletion({
		...rest,
		id,
		choices: choices.map(({ message, finish_reason, index, logprobs, ...choiceRest }) => {
			if (!finish_reason) throw new OpenAIError(`missing finish_reason for choice ${index}`);
			const { content = null, function_call, tool_calls, ...messageRest } = message;
			const role = message.role;
			if (!role) throw new OpenAIError(`missing role for choice ${index}`);
			if (function_call) {
				const { arguments: args, name } = function_call;
				if (args == null) throw new OpenAIError(`missing function_call.arguments for choice ${index}`);
				if (!name) throw new OpenAIError(`missing function_call.name for choice ${index}`);
				return {
					...choiceRest,
					message: {
						content,
						function_call: {
							arguments: args,
							name
						},
						role,
						refusal: message.refusal ?? null
					},
					finish_reason,
					index,
					logprobs
				};
			}
			if (tool_calls) return {
				...choiceRest,
				index,
				finish_reason,
				logprobs,
				message: {
					...messageRest,
					role,
					content,
					refusal: message.refusal ?? null,
					tool_calls: tool_calls.map((tool_call, i) => {
						const { function: fn, type, id: id$1, ...toolRest } = tool_call;
						const { arguments: args, name, ...fnRest } = fn || {};
						if (id$1 == null) throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].id\n${str(snapshot)}`);
						if (type == null) throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].type\n${str(snapshot)}`);
						if (name == null) throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].function.name\n${str(snapshot)}`);
						if (args == null) throw new OpenAIError(`missing choices[${index}].tool_calls[${i}].function.arguments\n${str(snapshot)}`);
						return {
							...toolRest,
							id: id$1,
							type,
							function: {
								...fnRest,
								name,
								arguments: args
							}
						};
					})
				}
			};
			return {
				...choiceRest,
				message: {
					...messageRest,
					content,
					role,
					refusal: message.refusal ?? null
				},
				finish_reason,
				index,
				logprobs
			};
		}),
		created,
		model,
		object: "chat.completion",
		...system_fingerprint ? { system_fingerprint } : {}
	}, params);
}
function str(x) {
	return JSON.stringify(x);
}
/**
* Ensures the given argument is an empty object, useful for
* asserting that all known properties on an object have been
* destructured.
*/
function assertIsEmpty(obj) {}
function assertNever$1(_x) {}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/ChatCompletionStreamingRunner.mjs
var ChatCompletionStreamingRunner = class ChatCompletionStreamingRunner extends ChatCompletionStream {
	static fromReadableStream(stream) {
		const runner = new ChatCompletionStreamingRunner(null);
		runner._run(() => runner._fromReadableStream(stream));
		return runner;
	}
	static runTools(client, params, options) {
		const runner = new ChatCompletionStreamingRunner(params);
		const opts = {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "runTools"
			}
		};
		runner._run(() => runner._runTools(client, params, opts));
		return runner;
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/chat/completions/completions.mjs
var Completions$1 = class extends APIResource {
	constructor() {
		super(...arguments);
		this.messages = new Messages$1(this._client);
	}
	create(body, options) {
		return this._client.post("/chat/completions", {
			body,
			...options,
			stream: body.stream ?? false
		});
	}
	/**
	* Get a stored chat completion. Only Chat Completions that have been created with
	* the `store` parameter set to `true` will be returned.
	*
	* @example
	* ```ts
	* const chatCompletion =
	*   await client.chat.completions.retrieve('completion_id');
	* ```
	*/
	retrieve(completionID, options) {
		return this._client.get(path$1`/chat/completions/${completionID}`, options);
	}
	/**
	* Modify a stored chat completion. Only Chat Completions that have been created
	* with the `store` parameter set to `true` can be modified. Currently, the only
	* supported modification is to update the `metadata` field.
	*
	* @example
	* ```ts
	* const chatCompletion = await client.chat.completions.update(
	*   'completion_id',
	*   { metadata: { foo: 'string' } },
	* );
	* ```
	*/
	update(completionID, body, options) {
		return this._client.post(path$1`/chat/completions/${completionID}`, {
			body,
			...options
		});
	}
	/**
	* List stored Chat Completions. Only Chat Completions that have been stored with
	* the `store` parameter set to `true` will be returned.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const chatCompletion of client.chat.completions.list()) {
	*   // ...
	* }
	* ```
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/chat/completions", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete a stored chat completion. Only Chat Completions that have been created
	* with the `store` parameter set to `true` can be deleted.
	*
	* @example
	* ```ts
	* const chatCompletionDeleted =
	*   await client.chat.completions.delete('completion_id');
	* ```
	*/
	delete(completionID, options) {
		return this._client.delete(path$1`/chat/completions/${completionID}`, options);
	}
	parse(body, options) {
		validateInputTools(body.tools);
		return this._client.chat.completions.create(body, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "chat.completions.parse"
			}
		})._thenUnwrap((completion) => parseChatCompletion(completion, body));
	}
	runTools(body, options) {
		if (body.stream) return ChatCompletionStreamingRunner.runTools(this._client, body, options);
		return ChatCompletionRunner.runTools(this._client, body, options);
	}
	/**
	* Creates a chat completion stream
	*/
	stream(body, options) {
		return ChatCompletionStream.createChatCompletion(this._client, body, options);
	}
};
Completions$1.Messages = Messages$1;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/chat/chat.mjs
var Chat = class extends APIResource {
	constructor() {
		super(...arguments);
		this.completions = new Completions$1(this._client);
	}
};
Chat.Completions = Completions$1;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/headers.mjs
const brand_privateNullableHeaders = /* @__PURE__ */ Symbol("brand.privateNullableHeaders");
function* iterateHeaders(headers) {
	if (!headers) return;
	if (brand_privateNullableHeaders in headers) {
		const { values, nulls } = headers;
		yield* values.entries();
		for (const name of nulls) yield [name, null];
		return;
	}
	let shouldClear = false;
	let iter;
	if (headers instanceof Headers) iter = headers.entries();
	else if (isReadonlyArray(headers)) iter = headers;
	else {
		shouldClear = true;
		iter = Object.entries(headers ?? {});
	}
	for (let row of iter) {
		const name = row[0];
		if (typeof name !== "string") throw new TypeError("expected header name to be a string");
		const values = isReadonlyArray(row[1]) ? row[1] : [row[1]];
		let didClear = false;
		for (const value of values) {
			if (value === void 0) continue;
			if (shouldClear && !didClear) {
				didClear = true;
				yield [name, null];
			}
			yield [name, value];
		}
	}
}
const buildHeaders = (newHeaders) => {
	const targetHeaders = new Headers();
	const nullHeaders = /* @__PURE__ */ new Set();
	for (const headers of newHeaders) {
		const seenHeaders = /* @__PURE__ */ new Set();
		for (const [name, value] of iterateHeaders(headers)) {
			const lowerName = name.toLowerCase();
			if (!seenHeaders.has(lowerName)) {
				targetHeaders.delete(name);
				seenHeaders.add(lowerName);
			}
			if (value === null) {
				targetHeaders.delete(name);
				nullHeaders.add(lowerName);
			} else {
				targetHeaders.append(name, value);
				nullHeaders.delete(lowerName);
			}
		}
	}
	return {
		[brand_privateNullableHeaders]: true,
		values: targetHeaders,
		nulls: nullHeaders
	};
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/audio/speech.mjs
var Speech = class extends APIResource {
	/**
	* Generates audio from the input text.
	*
	* @example
	* ```ts
	* const speech = await client.audio.speech.create({
	*   input: 'input',
	*   model: 'string',
	*   voice: 'ash',
	* });
	*
	* const content = await speech.blob();
	* console.log(content);
	* ```
	*/
	create(body, options) {
		return this._client.post("/audio/speech", {
			body,
			...options,
			headers: buildHeaders([{ Accept: "application/octet-stream" }, options?.headers]),
			__binaryResponse: true
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/audio/transcriptions.mjs
var Transcriptions = class extends APIResource {
	create(body, options) {
		return this._client.post("/audio/transcriptions", multipartFormRequestOptions({
			body,
			...options,
			stream: body.stream ?? false,
			__metadata: { model: body.model }
		}, this._client));
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/audio/translations.mjs
var Translations = class extends APIResource {
	create(body, options) {
		return this._client.post("/audio/translations", multipartFormRequestOptions({
			body,
			...options,
			__metadata: { model: body.model }
		}, this._client));
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/audio/audio.mjs
var Audio = class extends APIResource {
	constructor() {
		super(...arguments);
		this.transcriptions = new Transcriptions(this._client);
		this.translations = new Translations(this._client);
		this.speech = new Speech(this._client);
	}
};
Audio.Transcriptions = Transcriptions;
Audio.Translations = Translations;
Audio.Speech = Speech;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/batches.mjs
var Batches = class extends APIResource {
	/**
	* Creates and executes a batch from an uploaded file of requests
	*/
	create(body, options) {
		return this._client.post("/batches", {
			body,
			...options
		});
	}
	/**
	* Retrieves a batch.
	*/
	retrieve(batchID, options) {
		return this._client.get(path$1`/batches/${batchID}`, options);
	}
	/**
	* List your organization's batches.
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/batches", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Cancels an in-progress batch. The batch will be in status `cancelling` for up to
	* 10 minutes, before changing to `cancelled`, where it will have partial results
	* (if any) available in the output file.
	*/
	cancel(batchID, options) {
		return this._client.post(path$1`/batches/${batchID}/cancel`, options);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/assistants.mjs
var Assistants = class extends APIResource {
	/**
	* Create an assistant with a model and instructions.
	*
	* @example
	* ```ts
	* const assistant = await client.beta.assistants.create({
	*   model: 'gpt-4o',
	* });
	* ```
	*/
	create(body, options) {
		return this._client.post("/assistants", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieves an assistant.
	*
	* @example
	* ```ts
	* const assistant = await client.beta.assistants.retrieve(
	*   'assistant_id',
	* );
	* ```
	*/
	retrieve(assistantID, options) {
		return this._client.get(path$1`/assistants/${assistantID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Modifies an assistant.
	*
	* @example
	* ```ts
	* const assistant = await client.beta.assistants.update(
	*   'assistant_id',
	* );
	* ```
	*/
	update(assistantID, body, options) {
		return this._client.post(path$1`/assistants/${assistantID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of assistants.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const assistant of client.beta.assistants.list()) {
	*   // ...
	* }
	* ```
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/assistants", CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Delete an assistant.
	*
	* @example
	* ```ts
	* const assistantDeleted =
	*   await client.beta.assistants.delete('assistant_id');
	* ```
	*/
	delete(assistantID, options) {
		return this._client.delete(path$1`/assistants/${assistantID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/realtime/sessions.mjs
var Sessions$1 = class extends APIResource {
	/**
	* Create an ephemeral API token for use in client-side applications with the
	* Realtime API. Can be configured with the same session parameters as the
	* `session.update` client event.
	*
	* It responds with a session object, plus a `client_secret` key which contains a
	* usable ephemeral API token that can be used to authenticate browser clients for
	* the Realtime API.
	*
	* @example
	* ```ts
	* const session =
	*   await client.beta.realtime.sessions.create();
	* ```
	*/
	create(body, options) {
		return this._client.post("/realtime/sessions", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/realtime/transcription-sessions.mjs
var TranscriptionSessions = class extends APIResource {
	/**
	* Create an ephemeral API token for use in client-side applications with the
	* Realtime API specifically for realtime transcriptions. Can be configured with
	* the same session parameters as the `transcription_session.update` client event.
	*
	* It responds with a session object, plus a `client_secret` key which contains a
	* usable ephemeral API token that can be used to authenticate browser clients for
	* the Realtime API.
	*
	* @example
	* ```ts
	* const transcriptionSession =
	*   await client.beta.realtime.transcriptionSessions.create();
	* ```
	*/
	create(body, options) {
		return this._client.post("/realtime/transcription_sessions", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/realtime/realtime.mjs
/**
* @deprecated Realtime has now launched and is generally available. The old beta API is now deprecated.
*/
var Realtime$1 = class extends APIResource {
	constructor() {
		super(...arguments);
		this.sessions = new Sessions$1(this._client);
		this.transcriptionSessions = new TranscriptionSessions(this._client);
	}
};
Realtime$1.Sessions = Sessions$1;
Realtime$1.TranscriptionSessions = TranscriptionSessions;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/chatkit/sessions.mjs
var Sessions = class extends APIResource {
	/**
	* Create a ChatKit session
	*
	* @example
	* ```ts
	* const chatSession =
	*   await client.beta.chatkit.sessions.create({
	*     user: 'x',
	*     workflow: { id: 'id' },
	*   });
	* ```
	*/
	create(body, options) {
		return this._client.post("/chatkit/sessions", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
	/**
	* Cancel a ChatKit session
	*
	* @example
	* ```ts
	* const chatSession =
	*   await client.beta.chatkit.sessions.cancel('cksess_123');
	* ```
	*/
	cancel(sessionID, options) {
		return this._client.post(path$1`/chatkit/sessions/${sessionID}/cancel`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/chatkit/threads.mjs
var Threads$1 = class extends APIResource {
	/**
	* Retrieve a ChatKit thread
	*
	* @example
	* ```ts
	* const chatkitThread =
	*   await client.beta.chatkit.threads.retrieve('cthr_123');
	* ```
	*/
	retrieve(threadID, options) {
		return this._client.get(path$1`/chatkit/threads/${threadID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
	/**
	* List ChatKit threads
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const chatkitThread of client.beta.chatkit.threads.list()) {
	*   // ...
	* }
	* ```
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/chatkit/threads", ConversationCursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
	/**
	* Delete a ChatKit thread
	*
	* @example
	* ```ts
	* const thread = await client.beta.chatkit.threads.delete(
	*   'cthr_123',
	* );
	* ```
	*/
	delete(threadID, options) {
		return this._client.delete(path$1`/chatkit/threads/${threadID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
	/**
	* List ChatKit thread items
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const thread of client.beta.chatkit.threads.listItems(
	*   'cthr_123',
	* )) {
	*   // ...
	* }
	* ```
	*/
	listItems(threadID, query = {}, options) {
		return this._client.getAPIList(path$1`/chatkit/threads/${threadID}/items`, ConversationCursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "chatkit_beta=v1" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/chatkit/chatkit.mjs
var ChatKit = class extends APIResource {
	constructor() {
		super(...arguments);
		this.sessions = new Sessions(this._client);
		this.threads = new Threads$1(this._client);
	}
};
ChatKit.Sessions = Sessions;
ChatKit.Threads = Threads$1;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/threads/messages.mjs
/**
* @deprecated The Assistants API is deprecated in favor of the Responses API
*/
var Messages = class extends APIResource {
	/**
	* Create a message.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	create(threadID, body, options) {
		return this._client.post(path$1`/threads/${threadID}/messages`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieve a message.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	retrieve(messageID, params, options) {
		const { thread_id } = params;
		return this._client.get(path$1`/threads/${thread_id}/messages/${messageID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Modifies a message.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	update(messageID, params, options) {
		const { thread_id, ...body } = params;
		return this._client.post(path$1`/threads/${thread_id}/messages/${messageID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of messages for a given thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	list(threadID, query = {}, options) {
		return this._client.getAPIList(path$1`/threads/${threadID}/messages`, CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Deletes a message.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	delete(messageID, params, options) {
		const { thread_id } = params;
		return this._client.delete(path$1`/threads/${thread_id}/messages/${messageID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/threads/runs/steps.mjs
/**
* @deprecated The Assistants API is deprecated in favor of the Responses API
*/
var Steps = class extends APIResource {
	/**
	* Retrieves a run step.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	retrieve(stepID, params, options) {
		const { thread_id, run_id, ...query } = params;
		return this._client.get(path$1`/threads/${thread_id}/runs/${run_id}/steps/${stepID}`, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of run steps belonging to a run.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	list(runID, params, options) {
		const { thread_id, ...query } = params;
		return this._client.getAPIList(path$1`/threads/${thread_id}/runs/${runID}/steps`, CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/base64.mjs
/**
* Converts a Base64 encoded string to a Float32Array.
* @param base64Str - The Base64 encoded string.
* @returns An Array of numbers interpreted as Float32 values.
*/
const toFloat32Array = (base64Str) => {
	if (typeof Buffer !== "undefined") {
		const buf = Buffer.from(base64Str, "base64");
		return Array.from(new Float32Array(buf.buffer, buf.byteOffset, buf.length / Float32Array.BYTES_PER_ELEMENT));
	} else {
		const binaryStr = atob(base64Str);
		const len = binaryStr.length;
		const bytes = new Uint8Array(len);
		for (let i = 0; i < len; i++) bytes[i] = binaryStr.charCodeAt(i);
		return Array.from(new Float32Array(bytes.buffer));
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/internal/utils/env.mjs
/**
* Read an environment variable.
*
* Trims beginning and trailing whitespace.
*
* Will return undefined if the environment variable doesn't exist or cannot be accessed.
*/
const readEnv = (env) => {
	if (typeof globalThis.process !== "undefined") return globalThis.process.env?.[env]?.trim() ?? void 0;
	if (typeof globalThis.Deno !== "undefined") return globalThis.Deno.env?.get?.(env)?.trim();
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/AssistantStream.mjs
var _AssistantStream_instances, _a$1, _AssistantStream_events, _AssistantStream_runStepSnapshots, _AssistantStream_messageSnapshots, _AssistantStream_messageSnapshot, _AssistantStream_finalRun, _AssistantStream_currentContentIndex, _AssistantStream_currentContent, _AssistantStream_currentToolCallIndex, _AssistantStream_currentToolCall, _AssistantStream_currentEvent, _AssistantStream_currentRunSnapshot, _AssistantStream_currentRunStepSnapshot, _AssistantStream_addEvent, _AssistantStream_endRequest, _AssistantStream_handleMessage, _AssistantStream_handleRunStep, _AssistantStream_handleEvent, _AssistantStream_accumulateRunStep, _AssistantStream_accumulateMessage, _AssistantStream_accumulateContent, _AssistantStream_handleRun;
var AssistantStream = class extends EventStream {
	constructor() {
		super(...arguments);
		_AssistantStream_instances.add(this);
		_AssistantStream_events.set(this, []);
		_AssistantStream_runStepSnapshots.set(this, {});
		_AssistantStream_messageSnapshots.set(this, {});
		_AssistantStream_messageSnapshot.set(this, void 0);
		_AssistantStream_finalRun.set(this, void 0);
		_AssistantStream_currentContentIndex.set(this, void 0);
		_AssistantStream_currentContent.set(this, void 0);
		_AssistantStream_currentToolCallIndex.set(this, void 0);
		_AssistantStream_currentToolCall.set(this, void 0);
		_AssistantStream_currentEvent.set(this, void 0);
		_AssistantStream_currentRunSnapshot.set(this, void 0);
		_AssistantStream_currentRunStepSnapshot.set(this, void 0);
	}
	[(_AssistantStream_events = /* @__PURE__ */ new WeakMap(), _AssistantStream_runStepSnapshots = /* @__PURE__ */ new WeakMap(), _AssistantStream_messageSnapshots = /* @__PURE__ */ new WeakMap(), _AssistantStream_messageSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_finalRun = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentContentIndex = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentContent = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentToolCallIndex = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentToolCall = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentEvent = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentRunSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_currentRunStepSnapshot = /* @__PURE__ */ new WeakMap(), _AssistantStream_instances = /* @__PURE__ */ new WeakSet(), Symbol.asyncIterator)]() {
		const pushQueue = [];
		const readQueue = [];
		let done = false;
		this.on("event", (event) => {
			const reader = readQueue.shift();
			if (reader) reader.resolve(event);
			else pushQueue.push(event);
		});
		this.on("end", () => {
			done = true;
			for (const reader of readQueue) reader.resolve(void 0);
			readQueue.length = 0;
		});
		this.on("abort", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		this.on("error", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		return {
			next: async () => {
				if (!pushQueue.length) {
					if (done) return {
						value: void 0,
						done: true
					};
					return new Promise((resolve, reject) => readQueue.push({
						resolve,
						reject
					})).then((chunk) => chunk ? {
						value: chunk,
						done: false
					} : {
						value: void 0,
						done: true
					});
				}
				return {
					value: pushQueue.shift(),
					done: false
				};
			},
			return: async () => {
				this.abort();
				return {
					value: void 0,
					done: true
				};
			}
		};
	}
	static fromReadableStream(stream) {
		const runner = new _a$1();
		runner._run(() => runner._fromReadableStream(stream));
		return runner;
	}
	async _fromReadableStream(readableStream, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		this._connected();
		const stream = Stream.fromReadableStream(readableStream, this.controller);
		for await (const event of stream) __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
	}
	toReadableStream() {
		return new Stream(this[Symbol.asyncIterator].bind(this), this.controller).toReadableStream();
	}
	static createToolAssistantStream(runId, runs, params, options) {
		const runner = new _a$1();
		runner._run(() => runner._runToolAssistantStream(runId, runs, params, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "stream"
			}
		}));
		return runner;
	}
	async _createToolAssistantStream(run, runId, params, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		const body = {
			...params,
			stream: true
		};
		const stream = await run.submitToolOutputs(runId, body, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		for await (const event of stream) __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
	}
	static createThreadAssistantStream(params, thread, options) {
		const runner = new _a$1();
		runner._run(() => runner._threadAssistantStream(params, thread, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "stream"
			}
		}));
		return runner;
	}
	static createAssistantStream(threadId, runs, params, options) {
		const runner = new _a$1();
		runner._run(() => runner._runAssistantStream(threadId, runs, params, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "stream"
			}
		}));
		return runner;
	}
	currentEvent() {
		return __classPrivateFieldGet(this, _AssistantStream_currentEvent, "f");
	}
	currentRun() {
		return __classPrivateFieldGet(this, _AssistantStream_currentRunSnapshot, "f");
	}
	currentMessageSnapshot() {
		return __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f");
	}
	currentRunStepSnapshot() {
		return __classPrivateFieldGet(this, _AssistantStream_currentRunStepSnapshot, "f");
	}
	async finalRunSteps() {
		await this.done();
		return Object.values(__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f"));
	}
	async finalMessages() {
		await this.done();
		return Object.values(__classPrivateFieldGet(this, _AssistantStream_messageSnapshots, "f"));
	}
	async finalRun() {
		await this.done();
		if (!__classPrivateFieldGet(this, _AssistantStream_finalRun, "f")) throw Error("Final run was not received.");
		return __classPrivateFieldGet(this, _AssistantStream_finalRun, "f");
	}
	async _createThreadAssistantStream(thread, params, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		const body = {
			...params,
			stream: true
		};
		const stream = await thread.createAndRun(body, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		for await (const event of stream) __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
	}
	async _createAssistantStream(run, threadId, params, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		const body = {
			...params,
			stream: true
		};
		const stream = await run.create(threadId, body, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		for await (const event of stream) __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_addEvent).call(this, event);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_endRequest).call(this));
	}
	static accumulateDelta(acc, delta) {
		for (const [key, deltaValue] of Object.entries(delta)) {
			if (!acc.hasOwnProperty(key)) {
				acc[key] = deltaValue;
				continue;
			}
			let accValue = acc[key];
			if (accValue === null || accValue === void 0) {
				acc[key] = deltaValue;
				continue;
			}
			if (key === "index" || key === "type") {
				acc[key] = deltaValue;
				continue;
			}
			if (typeof accValue === "string" && typeof deltaValue === "string") accValue += deltaValue;
			else if (typeof accValue === "number" && typeof deltaValue === "number") accValue += deltaValue;
			else if (isObj(accValue) && isObj(deltaValue)) accValue = this.accumulateDelta(accValue, deltaValue);
			else if (Array.isArray(accValue) && Array.isArray(deltaValue)) {
				if (accValue.every((x) => typeof x === "string" || typeof x === "number")) {
					accValue.push(...deltaValue);
					continue;
				}
				for (const deltaEntry of deltaValue) {
					if (!isObj(deltaEntry)) throw new Error(`Expected array delta entry to be an object but got: ${deltaEntry}`);
					const index = deltaEntry["index"];
					if (index == null) {
						console.error(deltaEntry);
						throw new Error("Expected array delta entry to have an `index` property");
					}
					if (typeof index !== "number") throw new Error(`Expected array delta entry \`index\` property to be a number but got ${index}`);
					const accEntry = accValue[index];
					if (accEntry == null) accValue.push(deltaEntry);
					else accValue[index] = this.accumulateDelta(accEntry, deltaEntry);
				}
				continue;
			} else throw Error(`Unhandled record type: ${key}, deltaValue: ${deltaValue}, accValue: ${accValue}`);
			acc[key] = accValue;
		}
		return acc;
	}
	_addRun(run) {
		return run;
	}
	async _threadAssistantStream(params, thread, options) {
		return await this._createThreadAssistantStream(thread, params, options);
	}
	async _runAssistantStream(threadId, runs, params, options) {
		return await this._createAssistantStream(runs, threadId, params, options);
	}
	async _runToolAssistantStream(runId, runs, params, options) {
		return await this._createToolAssistantStream(runs, runId, params, options);
	}
};
_a$1 = AssistantStream, _AssistantStream_addEvent = function _AssistantStream_addEvent$1(event) {
	if (this.ended) return;
	__classPrivateFieldSet(this, _AssistantStream_currentEvent, event, "f");
	__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_handleEvent).call(this, event);
	switch (event.event) {
		case "thread.created": break;
		case "thread.run.created":
		case "thread.run.queued":
		case "thread.run.in_progress":
		case "thread.run.requires_action":
		case "thread.run.completed":
		case "thread.run.incomplete":
		case "thread.run.failed":
		case "thread.run.cancelling":
		case "thread.run.cancelled":
		case "thread.run.expired":
			__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_handleRun).call(this, event);
			break;
		case "thread.run.step.created":
		case "thread.run.step.in_progress":
		case "thread.run.step.delta":
		case "thread.run.step.completed":
		case "thread.run.step.failed":
		case "thread.run.step.cancelled":
		case "thread.run.step.expired":
			__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_handleRunStep).call(this, event);
			break;
		case "thread.message.created":
		case "thread.message.in_progress":
		case "thread.message.delta":
		case "thread.message.completed":
		case "thread.message.incomplete":
			__classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_handleMessage).call(this, event);
			break;
		case "error": throw new Error("Encountered an error event in event processing - errors should be processed earlier");
		default: assertNever(event);
	}
}, _AssistantStream_endRequest = function _AssistantStream_endRequest$1() {
	if (this.ended) throw new OpenAIError(`stream has ended, this shouldn't happen`);
	if (!__classPrivateFieldGet(this, _AssistantStream_finalRun, "f")) throw Error("Final run has not been received");
	return __classPrivateFieldGet(this, _AssistantStream_finalRun, "f");
}, _AssistantStream_handleMessage = function _AssistantStream_handleMessage$1(event) {
	const [accumulatedMessage, newContent] = __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_accumulateMessage).call(this, event, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f"));
	__classPrivateFieldSet(this, _AssistantStream_messageSnapshot, accumulatedMessage, "f");
	__classPrivateFieldGet(this, _AssistantStream_messageSnapshots, "f")[accumulatedMessage.id] = accumulatedMessage;
	for (const content of newContent) {
		const snapshotContent = accumulatedMessage.content[content.index];
		if (snapshotContent?.type == "text") this._emit("textCreated", snapshotContent.text);
	}
	switch (event.event) {
		case "thread.message.created":
			this._emit("messageCreated", event.data);
			break;
		case "thread.message.in_progress": break;
		case "thread.message.delta":
			this._emit("messageDelta", event.data.delta, accumulatedMessage);
			if (event.data.delta.content) for (const content of event.data.delta.content) {
				if (content.type == "text" && content.text) {
					let textDelta = content.text;
					let snapshot = accumulatedMessage.content[content.index];
					if (snapshot && snapshot.type == "text") this._emit("textDelta", textDelta, snapshot.text);
					else throw Error("The snapshot associated with this text delta is not text or missing");
				}
				if (content.index != __classPrivateFieldGet(this, _AssistantStream_currentContentIndex, "f")) {
					if (__classPrivateFieldGet(this, _AssistantStream_currentContent, "f")) switch (__classPrivateFieldGet(this, _AssistantStream_currentContent, "f").type) {
						case "text":
							this._emit("textDone", __classPrivateFieldGet(this, _AssistantStream_currentContent, "f").text, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f"));
							break;
						case "image_file":
							this._emit("imageFileDone", __classPrivateFieldGet(this, _AssistantStream_currentContent, "f").image_file, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f"));
							break;
					}
					__classPrivateFieldSet(this, _AssistantStream_currentContentIndex, content.index, "f");
				}
				__classPrivateFieldSet(this, _AssistantStream_currentContent, accumulatedMessage.content[content.index], "f");
			}
			break;
		case "thread.message.completed":
		case "thread.message.incomplete":
			if (__classPrivateFieldGet(this, _AssistantStream_currentContentIndex, "f") !== void 0) {
				const currentContent = event.data.content[__classPrivateFieldGet(this, _AssistantStream_currentContentIndex, "f")];
				if (currentContent) switch (currentContent.type) {
					case "image_file":
						this._emit("imageFileDone", currentContent.image_file, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f"));
						break;
					case "text":
						this._emit("textDone", currentContent.text, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f"));
						break;
				}
			}
			if (__classPrivateFieldGet(this, _AssistantStream_messageSnapshot, "f")) this._emit("messageDone", event.data);
			__classPrivateFieldSet(this, _AssistantStream_messageSnapshot, void 0, "f");
	}
}, _AssistantStream_handleRunStep = function _AssistantStream_handleRunStep$1(event) {
	const accumulatedRunStep = __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_accumulateRunStep).call(this, event);
	__classPrivateFieldSet(this, _AssistantStream_currentRunStepSnapshot, accumulatedRunStep, "f");
	switch (event.event) {
		case "thread.run.step.created":
			this._emit("runStepCreated", event.data);
			break;
		case "thread.run.step.delta":
			const delta = event.data.delta;
			if (delta.step_details && delta.step_details.type == "tool_calls" && delta.step_details.tool_calls && accumulatedRunStep.step_details.type == "tool_calls") for (const toolCall of delta.step_details.tool_calls) if (toolCall.index == __classPrivateFieldGet(this, _AssistantStream_currentToolCallIndex, "f")) this._emit("toolCallDelta", toolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index]);
			else {
				if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f")) this._emit("toolCallDone", __classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f"));
				__classPrivateFieldSet(this, _AssistantStream_currentToolCallIndex, toolCall.index, "f");
				__classPrivateFieldSet(this, _AssistantStream_currentToolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index], "f");
				if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f")) this._emit("toolCallCreated", __classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f"));
			}
			this._emit("runStepDelta", event.data.delta, accumulatedRunStep);
			break;
		case "thread.run.step.completed":
		case "thread.run.step.failed":
		case "thread.run.step.cancelled":
		case "thread.run.step.expired":
			__classPrivateFieldSet(this, _AssistantStream_currentRunStepSnapshot, void 0, "f");
			if (event.data.step_details.type == "tool_calls") {
				if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f")) {
					this._emit("toolCallDone", __classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f"));
					__classPrivateFieldSet(this, _AssistantStream_currentToolCall, void 0, "f");
				}
			}
			this._emit("runStepDone", event.data, accumulatedRunStep);
			break;
		case "thread.run.step.in_progress": break;
	}
}, _AssistantStream_handleEvent = function _AssistantStream_handleEvent$1(event) {
	__classPrivateFieldGet(this, _AssistantStream_events, "f").push(event);
	this._emit("event", event);
}, _AssistantStream_accumulateRunStep = function _AssistantStream_accumulateRunStep$1(event) {
	switch (event.event) {
		case "thread.run.step.created":
			__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = event.data;
			return event.data;
		case "thread.run.step.delta":
			let snapshot = __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
			if (!snapshot) throw Error("Received a RunStepDelta before creation of a snapshot");
			let data = event.data;
			if (data.delta) {
				const accumulated = _a$1.accumulateDelta(snapshot, data.delta);
				__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = accumulated;
			}
			return __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
		case "thread.run.step.completed":
		case "thread.run.step.failed":
		case "thread.run.step.cancelled":
		case "thread.run.step.expired":
		case "thread.run.step.in_progress":
			__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id] = event.data;
			break;
	}
	if (__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id]) return __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, "f")[event.data.id];
	throw new Error("No snapshot available");
}, _AssistantStream_accumulateMessage = function _AssistantStream_accumulateMessage$1(event, snapshot) {
	let newContent = [];
	switch (event.event) {
		case "thread.message.created": return [event.data, newContent];
		case "thread.message.delta":
			if (!snapshot) throw Error("Received a delta with no existing snapshot (there should be one from message creation)");
			let data = event.data;
			if (data.delta.content) for (const contentElement of data.delta.content) if (contentElement.index in snapshot.content) {
				let currentContent = snapshot.content[contentElement.index];
				snapshot.content[contentElement.index] = __classPrivateFieldGet(this, _AssistantStream_instances, "m", _AssistantStream_accumulateContent).call(this, contentElement, currentContent);
			} else {
				snapshot.content[contentElement.index] = contentElement;
				newContent.push(contentElement);
			}
			return [snapshot, newContent];
		case "thread.message.in_progress":
		case "thread.message.completed":
		case "thread.message.incomplete": if (snapshot) return [snapshot, newContent];
		else throw Error("Received thread message event with no existing snapshot");
	}
	throw Error("Tried to accumulate a non-message event");
}, _AssistantStream_accumulateContent = function _AssistantStream_accumulateContent$1(contentElement, currentContent) {
	return _a$1.accumulateDelta(currentContent, contentElement);
}, _AssistantStream_handleRun = function _AssistantStream_handleRun$1(event) {
	__classPrivateFieldSet(this, _AssistantStream_currentRunSnapshot, event.data, "f");
	switch (event.event) {
		case "thread.run.created": break;
		case "thread.run.queued": break;
		case "thread.run.in_progress": break;
		case "thread.run.requires_action":
		case "thread.run.cancelled":
		case "thread.run.failed":
		case "thread.run.completed":
		case "thread.run.expired":
		case "thread.run.incomplete":
			__classPrivateFieldSet(this, _AssistantStream_finalRun, event.data, "f");
			if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f")) {
				this._emit("toolCallDone", __classPrivateFieldGet(this, _AssistantStream_currentToolCall, "f"));
				__classPrivateFieldSet(this, _AssistantStream_currentToolCall, void 0, "f");
			}
			break;
		case "thread.run.cancelling": break;
	}
};
function assertNever(_x) {}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/threads/runs/runs.mjs
/**
* @deprecated The Assistants API is deprecated in favor of the Responses API
*/
var Runs$1 = class extends APIResource {
	constructor() {
		super(...arguments);
		this.steps = new Steps(this._client);
	}
	create(threadID, params, options) {
		const { include, ...body } = params;
		return this._client.post(path$1`/threads/${threadID}/runs`, {
			query: { include },
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers]),
			stream: params.stream ?? false
		});
	}
	/**
	* Retrieves a run.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	retrieve(runID, params, options) {
		const { thread_id } = params;
		return this._client.get(path$1`/threads/${thread_id}/runs/${runID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Modifies a run.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	update(runID, params, options) {
		const { thread_id, ...body } = params;
		return this._client.post(path$1`/threads/${thread_id}/runs/${runID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of runs belonging to a thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	list(threadID, query = {}, options) {
		return this._client.getAPIList(path$1`/threads/${threadID}/runs`, CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Cancels a run that is `in_progress`.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	cancel(runID, params, options) {
		const { thread_id } = params;
		return this._client.post(path$1`/threads/${thread_id}/runs/${runID}/cancel`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* A helper to create a run an poll for a terminal state. More information on Run
	* lifecycles can be found here:
	* https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
	*/
	async createAndPoll(threadId, body, options) {
		const run = await this.create(threadId, body, options);
		return await this.poll(run.id, { thread_id: threadId }, options);
	}
	/**
	* Create a Run stream
	*
	* @deprecated use `stream` instead
	*/
	createAndStream(threadId, body, options) {
		return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);
	}
	/**
	* A helper to poll a run status until it reaches a terminal state. More
	* information on Run lifecycles can be found here:
	* https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
	*/
	async poll(runId, params, options) {
		const headers = buildHeaders([options?.headers, {
			"X-Stainless-Poll-Helper": "true",
			"X-Stainless-Custom-Poll-Interval": options?.pollIntervalMs?.toString() ?? void 0
		}]);
		while (true) {
			const { data: run, response } = await this.retrieve(runId, params, {
				...options,
				headers: {
					...options?.headers,
					...headers
				}
			}).withResponse();
			switch (run.status) {
				case "queued":
				case "in_progress":
				case "cancelling":
					let sleepInterval = 5e3;
					if (options?.pollIntervalMs) sleepInterval = options.pollIntervalMs;
					else {
						const headerInterval = response.headers.get("openai-poll-after-ms");
						if (headerInterval) {
							const headerIntervalMs = parseInt(headerInterval);
							if (!isNaN(headerIntervalMs)) sleepInterval = headerIntervalMs;
						}
					}
					await sleep(sleepInterval);
					break;
				case "requires_action":
				case "incomplete":
				case "cancelled":
				case "completed":
				case "failed":
				case "expired": return run;
			}
		}
	}
	/**
	* Create a Run stream
	*/
	stream(threadId, body, options) {
		return AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);
	}
	submitToolOutputs(runID, params, options) {
		const { thread_id, ...body } = params;
		return this._client.post(path$1`/threads/${thread_id}/runs/${runID}/submit_tool_outputs`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers]),
			stream: params.stream ?? false
		});
	}
	/**
	* A helper to submit a tool output to a run and poll for a terminal run state.
	* More information on Run lifecycles can be found here:
	* https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
	*/
	async submitToolOutputsAndPoll(runId, params, options) {
		const run = await this.submitToolOutputs(runId, params, options);
		return await this.poll(run.id, params, options);
	}
	/**
	* Submit the tool outputs from a previous run and stream the run to a terminal
	* state. More information on Run lifecycles can be found here:
	* https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
	*/
	submitToolOutputsStream(runId, params, options) {
		return AssistantStream.createToolAssistantStream(runId, this._client.beta.threads.runs, params, options);
	}
};
Runs$1.Steps = Steps;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/threads/threads.mjs
/**
* @deprecated The Assistants API is deprecated in favor of the Responses API
*/
var Threads = class extends APIResource {
	constructor() {
		super(...arguments);
		this.runs = new Runs$1(this._client);
		this.messages = new Messages(this._client);
	}
	/**
	* Create a thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	create(body = {}, options) {
		return this._client.post("/threads", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieves a thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	retrieve(threadID, options) {
		return this._client.get(path$1`/threads/${threadID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Modifies a thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	update(threadID, body, options) {
		return this._client.post(path$1`/threads/${threadID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Delete a thread.
	*
	* @deprecated The Assistants API is deprecated in favor of the Responses API
	*/
	delete(threadID, options) {
		return this._client.delete(path$1`/threads/${threadID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	createAndRun(body, options) {
		return this._client.post("/threads/runs", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers]),
			stream: body.stream ?? false
		});
	}
	/**
	* A helper to create a thread, start a run and then poll for a terminal state.
	* More information on Run lifecycles can be found here:
	* https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps
	*/
	async createAndRunPoll(body, options) {
		const run = await this.createAndRun(body, options);
		return await this.runs.poll(run.id, { thread_id: run.thread_id }, options);
	}
	/**
	* Create a thread and stream the run back
	*/
	createAndRunStream(body, options) {
		return AssistantStream.createThreadAssistantStream(body, this._client.beta.threads, options);
	}
};
Threads.Runs = Runs$1;
Threads.Messages = Messages;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/beta/beta.mjs
var Beta = class extends APIResource {
	constructor() {
		super(...arguments);
		this.realtime = new Realtime$1(this._client);
		this.chatkit = new ChatKit(this._client);
		this.assistants = new Assistants(this._client);
		this.threads = new Threads(this._client);
	}
};
Beta.Realtime = Realtime$1;
Beta.ChatKit = ChatKit;
Beta.Assistants = Assistants;
Beta.Threads = Threads;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/completions.mjs
var Completions = class extends APIResource {
	create(body, options) {
		return this._client.post("/completions", {
			body,
			...options,
			stream: body.stream ?? false
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/containers/files/content.mjs
var Content = class extends APIResource {
	/**
	* Retrieve Container File Content
	*/
	retrieve(fileID, params, options) {
		const { container_id } = params;
		return this._client.get(path$1`/containers/${container_id}/files/${fileID}/content`, {
			...options,
			headers: buildHeaders([{ Accept: "application/binary" }, options?.headers]),
			__binaryResponse: true
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/containers/files/files.mjs
var Files$2 = class extends APIResource {
	constructor() {
		super(...arguments);
		this.content = new Content(this._client);
	}
	/**
	* Create a Container File
	*
	* You can send either a multipart/form-data request with the raw file content, or
	* a JSON request with a file ID.
	*/
	create(containerID, body, options) {
		return this._client.post(path$1`/containers/${containerID}/files`, multipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
	/**
	* Retrieve Container File
	*/
	retrieve(fileID, params, options) {
		const { container_id } = params;
		return this._client.get(path$1`/containers/${container_id}/files/${fileID}`, options);
	}
	/**
	* List Container files
	*/
	list(containerID, query = {}, options) {
		return this._client.getAPIList(path$1`/containers/${containerID}/files`, CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete Container File
	*/
	delete(fileID, params, options) {
		const { container_id } = params;
		return this._client.delete(path$1`/containers/${container_id}/files/${fileID}`, {
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
};
Files$2.Content = Content;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/containers/containers.mjs
var Containers = class extends APIResource {
	constructor() {
		super(...arguments);
		this.files = new Files$2(this._client);
	}
	/**
	* Create Container
	*/
	create(body, options) {
		return this._client.post("/containers", {
			body,
			...options
		});
	}
	/**
	* Retrieve Container
	*/
	retrieve(containerID, options) {
		return this._client.get(path$1`/containers/${containerID}`, options);
	}
	/**
	* List Containers
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/containers", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete Container
	*/
	delete(containerID, options) {
		return this._client.delete(path$1`/containers/${containerID}`, {
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
};
Containers.Files = Files$2;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/conversations/items.mjs
var Items = class extends APIResource {
	/**
	* Create items in a conversation with the given ID.
	*/
	create(conversationID, params, options) {
		const { include, ...body } = params;
		return this._client.post(path$1`/conversations/${conversationID}/items`, {
			query: { include },
			body,
			...options
		});
	}
	/**
	* Get a single item from a conversation with the given IDs.
	*/
	retrieve(itemID, params, options) {
		const { conversation_id, ...query } = params;
		return this._client.get(path$1`/conversations/${conversation_id}/items/${itemID}`, {
			query,
			...options
		});
	}
	/**
	* List all items for a conversation with the given ID.
	*/
	list(conversationID, query = {}, options) {
		return this._client.getAPIList(path$1`/conversations/${conversationID}/items`, ConversationCursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete an item from a conversation with the given IDs.
	*/
	delete(itemID, params, options) {
		const { conversation_id } = params;
		return this._client.delete(path$1`/conversations/${conversation_id}/items/${itemID}`, options);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/conversations/conversations.mjs
var Conversations = class extends APIResource {
	constructor() {
		super(...arguments);
		this.items = new Items(this._client);
	}
	/**
	* Create a conversation.
	*/
	create(body = {}, options) {
		return this._client.post("/conversations", {
			body,
			...options
		});
	}
	/**
	* Get a conversation
	*/
	retrieve(conversationID, options) {
		return this._client.get(path$1`/conversations/${conversationID}`, options);
	}
	/**
	* Update a conversation
	*/
	update(conversationID, body, options) {
		return this._client.post(path$1`/conversations/${conversationID}`, {
			body,
			...options
		});
	}
	/**
	* Delete a conversation. Items in the conversation will not be deleted.
	*/
	delete(conversationID, options) {
		return this._client.delete(path$1`/conversations/${conversationID}`, options);
	}
};
Conversations.Items = Items;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/embeddings.mjs
var Embeddings = class extends APIResource {
	/**
	* Creates an embedding vector representing the input text.
	*
	* @example
	* ```ts
	* const createEmbeddingResponse =
	*   await client.embeddings.create({
	*     input: 'The quick brown fox jumped over the lazy dog',
	*     model: 'text-embedding-3-small',
	*   });
	* ```
	*/
	create(body, options) {
		const hasUserProvidedEncodingFormat = !!body.encoding_format;
		let encoding_format = hasUserProvidedEncodingFormat ? body.encoding_format : "base64";
		if (hasUserProvidedEncodingFormat) loggerFor(this._client).debug("embeddings/user defined encoding_format:", body.encoding_format);
		const response = this._client.post("/embeddings", {
			body: {
				...body,
				encoding_format
			},
			...options
		});
		if (hasUserProvidedEncodingFormat) return response;
		loggerFor(this._client).debug("embeddings/decoding base64 embeddings from base64");
		return response._thenUnwrap((response$1) => {
			if (response$1 && response$1.data) response$1.data.forEach((embeddingBase64Obj) => {
				const embeddingBase64Str = embeddingBase64Obj.embedding;
				embeddingBase64Obj.embedding = toFloat32Array(embeddingBase64Str);
			});
			return response$1;
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/evals/runs/output-items.mjs
var OutputItems = class extends APIResource {
	/**
	* Get an evaluation run output item by ID.
	*/
	retrieve(outputItemID, params, options) {
		const { eval_id, run_id } = params;
		return this._client.get(path$1`/evals/${eval_id}/runs/${run_id}/output_items/${outputItemID}`, options);
	}
	/**
	* Get a list of output items for an evaluation run.
	*/
	list(runID, params, options) {
		const { eval_id, ...query } = params;
		return this._client.getAPIList(path$1`/evals/${eval_id}/runs/${runID}/output_items`, CursorPage, {
			query,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/evals/runs/runs.mjs
var Runs = class extends APIResource {
	constructor() {
		super(...arguments);
		this.outputItems = new OutputItems(this._client);
	}
	/**
	* Kicks off a new run for a given evaluation, specifying the data source, and what
	* model configuration to use to test. The datasource will be validated against the
	* schema specified in the config of the evaluation.
	*/
	create(evalID, body, options) {
		return this._client.post(path$1`/evals/${evalID}/runs`, {
			body,
			...options
		});
	}
	/**
	* Get an evaluation run by ID.
	*/
	retrieve(runID, params, options) {
		const { eval_id } = params;
		return this._client.get(path$1`/evals/${eval_id}/runs/${runID}`, options);
	}
	/**
	* Get a list of runs for an evaluation.
	*/
	list(evalID, query = {}, options) {
		return this._client.getAPIList(path$1`/evals/${evalID}/runs`, CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete an eval run.
	*/
	delete(runID, params, options) {
		const { eval_id } = params;
		return this._client.delete(path$1`/evals/${eval_id}/runs/${runID}`, options);
	}
	/**
	* Cancel an ongoing evaluation run.
	*/
	cancel(runID, params, options) {
		const { eval_id } = params;
		return this._client.post(path$1`/evals/${eval_id}/runs/${runID}`, options);
	}
};
Runs.OutputItems = OutputItems;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/evals/evals.mjs
var Evals = class extends APIResource {
	constructor() {
		super(...arguments);
		this.runs = new Runs(this._client);
	}
	/**
	* Create the structure of an evaluation that can be used to test a model's
	* performance. An evaluation is a set of testing criteria and the config for a
	* data source, which dictates the schema of the data used in the evaluation. After
	* creating an evaluation, you can run it on different models and model parameters.
	* We support several types of graders and datasources. For more information, see
	* the [Evals guide](https://platform.openai.com/docs/guides/evals).
	*/
	create(body, options) {
		return this._client.post("/evals", {
			body,
			...options
		});
	}
	/**
	* Get an evaluation by ID.
	*/
	retrieve(evalID, options) {
		return this._client.get(path$1`/evals/${evalID}`, options);
	}
	/**
	* Update certain properties of an evaluation.
	*/
	update(evalID, body, options) {
		return this._client.post(path$1`/evals/${evalID}`, {
			body,
			...options
		});
	}
	/**
	* List evaluations for a project.
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/evals", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete an evaluation.
	*/
	delete(evalID, options) {
		return this._client.delete(path$1`/evals/${evalID}`, options);
	}
};
Evals.Runs = Runs;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/files.mjs
var Files$1 = class extends APIResource {
	/**
	* Upload a file that can be used across various endpoints. Individual files can be
	* up to 512 MB, and the size of all files uploaded by one organization can be up
	* to 1 TB.
	*
	* - The Assistants API supports files up to 2 million tokens and of specific file
	*   types. See the
	*   [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools)
	*   for details.
	* - The Fine-tuning API only supports `.jsonl` files. The input also has certain
	*   required formats for fine-tuning
	*   [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input)
	*   or
	*   [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)
	*   models.
	* - The Batch API only supports `.jsonl` files up to 200 MB in size. The input
	*   also has a specific required
	*   [format](https://platform.openai.com/docs/api-reference/batch/request-input).
	*
	* Please [contact us](https://help.openai.com/) if you need to increase these
	* storage limits.
	*/
	create(body, options) {
		return this._client.post("/files", multipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
	/**
	* Returns information about a specific file.
	*/
	retrieve(fileID, options) {
		return this._client.get(path$1`/files/${fileID}`, options);
	}
	/**
	* Returns a list of files.
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/files", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete a file and remove it from all vector stores.
	*/
	delete(fileID, options) {
		return this._client.delete(path$1`/files/${fileID}`, options);
	}
	/**
	* Returns the contents of the specified file.
	*/
	content(fileID, options) {
		return this._client.get(path$1`/files/${fileID}/content`, {
			...options,
			headers: buildHeaders([{ Accept: "application/binary" }, options?.headers]),
			__binaryResponse: true
		});
	}
	/**
	* Waits for the given file to be processed, default timeout is 30 mins.
	*/
	async waitForProcessing(id, { pollInterval = 5e3, maxWait = 1800 * 1e3 } = {}) {
		const TERMINAL_STATES = new Set([
			"processed",
			"error",
			"deleted"
		]);
		const start = Date.now();
		let file = await this.retrieve(id);
		while (!file.status || !TERMINAL_STATES.has(file.status)) {
			await sleep(pollInterval);
			file = await this.retrieve(id);
			if (Date.now() - start > maxWait) throw new APIConnectionTimeoutError({ message: `Giving up on waiting for file ${id} to finish processing after ${maxWait} milliseconds.` });
		}
		return file;
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/methods.mjs
var Methods = class extends APIResource {};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/alpha/graders.mjs
var Graders$1 = class extends APIResource {
	/**
	* Run a grader.
	*
	* @example
	* ```ts
	* const response = await client.fineTuning.alpha.graders.run({
	*   grader: {
	*     input: 'input',
	*     name: 'name',
	*     operation: 'eq',
	*     reference: 'reference',
	*     type: 'string_check',
	*   },
	*   model_sample: 'model_sample',
	* });
	* ```
	*/
	run(body, options) {
		return this._client.post("/fine_tuning/alpha/graders/run", {
			body,
			...options
		});
	}
	/**
	* Validate a grader.
	*
	* @example
	* ```ts
	* const response =
	*   await client.fineTuning.alpha.graders.validate({
	*     grader: {
	*       input: 'input',
	*       name: 'name',
	*       operation: 'eq',
	*       reference: 'reference',
	*       type: 'string_check',
	*     },
	*   });
	* ```
	*/
	validate(body, options) {
		return this._client.post("/fine_tuning/alpha/graders/validate", {
			body,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/alpha/alpha.mjs
var Alpha = class extends APIResource {
	constructor() {
		super(...arguments);
		this.graders = new Graders$1(this._client);
	}
};
Alpha.Graders = Graders$1;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/checkpoints/permissions.mjs
var Permissions = class extends APIResource {
	/**
	* **NOTE:** Calling this endpoint requires an [admin API key](../admin-api-keys).
	*
	* This enables organization owners to share fine-tuned models with other projects
	* in their organization.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const permissionCreateResponse of client.fineTuning.checkpoints.permissions.create(
	*   'ft:gpt-4o-mini-2024-07-18:org:weather:B7R9VjQd',
	*   { project_ids: ['string'] },
	* )) {
	*   // ...
	* }
	* ```
	*/
	create(fineTunedModelCheckpoint, body, options) {
		return this._client.getAPIList(path$1`/fine_tuning/checkpoints/${fineTunedModelCheckpoint}/permissions`, Page, {
			body,
			method: "post",
			...options
		});
	}
	/**
	* **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).
	*
	* Organization owners can use this endpoint to view all permissions for a
	* fine-tuned model checkpoint.
	*
	* @example
	* ```ts
	* const permission =
	*   await client.fineTuning.checkpoints.permissions.retrieve(
	*     'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	*   );
	* ```
	*/
	retrieve(fineTunedModelCheckpoint, query = {}, options) {
		return this._client.get(path$1`/fine_tuning/checkpoints/${fineTunedModelCheckpoint}/permissions`, {
			query,
			...options
		});
	}
	/**
	* **NOTE:** This endpoint requires an [admin API key](../admin-api-keys).
	*
	* Organization owners can use this endpoint to delete a permission for a
	* fine-tuned model checkpoint.
	*
	* @example
	* ```ts
	* const permission =
	*   await client.fineTuning.checkpoints.permissions.delete(
	*     'cp_zc4Q7MP6XxulcVzj4MZdwsAB',
	*     {
	*       fine_tuned_model_checkpoint:
	*         'ft:gpt-4o-mini-2024-07-18:org:weather:B7R9VjQd',
	*     },
	*   );
	* ```
	*/
	delete(permissionID, params, options) {
		const { fine_tuned_model_checkpoint } = params;
		return this._client.delete(path$1`/fine_tuning/checkpoints/${fine_tuned_model_checkpoint}/permissions/${permissionID}`, options);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/checkpoints/checkpoints.mjs
var Checkpoints$1 = class extends APIResource {
	constructor() {
		super(...arguments);
		this.permissions = new Permissions(this._client);
	}
};
Checkpoints$1.Permissions = Permissions;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs
var Checkpoints = class extends APIResource {
	/**
	* List checkpoints for a fine-tuning job.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const fineTuningJobCheckpoint of client.fineTuning.jobs.checkpoints.list(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* )) {
	*   // ...
	* }
	* ```
	*/
	list(fineTuningJobID, query = {}, options) {
		return this._client.getAPIList(path$1`/fine_tuning/jobs/${fineTuningJobID}/checkpoints`, CursorPage, {
			query,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/jobs/jobs.mjs
var Jobs = class extends APIResource {
	constructor() {
		super(...arguments);
		this.checkpoints = new Checkpoints(this._client);
	}
	/**
	* Creates a fine-tuning job which begins the process of creating a new model from
	* a given dataset.
	*
	* Response includes details of the enqueued job including job status and the name
	* of the fine-tuned models once complete.
	*
	* [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)
	*
	* @example
	* ```ts
	* const fineTuningJob = await client.fineTuning.jobs.create({
	*   model: 'gpt-4o-mini',
	*   training_file: 'file-abc123',
	* });
	* ```
	*/
	create(body, options) {
		return this._client.post("/fine_tuning/jobs", {
			body,
			...options
		});
	}
	/**
	* Get info about a fine-tuning job.
	*
	* [Learn more about fine-tuning](https://platform.openai.com/docs/guides/model-optimization)
	*
	* @example
	* ```ts
	* const fineTuningJob = await client.fineTuning.jobs.retrieve(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* );
	* ```
	*/
	retrieve(fineTuningJobID, options) {
		return this._client.get(path$1`/fine_tuning/jobs/${fineTuningJobID}`, options);
	}
	/**
	* List your organization's fine-tuning jobs
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const fineTuningJob of client.fineTuning.jobs.list()) {
	*   // ...
	* }
	* ```
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/fine_tuning/jobs", CursorPage, {
			query,
			...options
		});
	}
	/**
	* Immediately cancel a fine-tune job.
	*
	* @example
	* ```ts
	* const fineTuningJob = await client.fineTuning.jobs.cancel(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* );
	* ```
	*/
	cancel(fineTuningJobID, options) {
		return this._client.post(path$1`/fine_tuning/jobs/${fineTuningJobID}/cancel`, options);
	}
	/**
	* Get status updates for a fine-tuning job.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const fineTuningJobEvent of client.fineTuning.jobs.listEvents(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* )) {
	*   // ...
	* }
	* ```
	*/
	listEvents(fineTuningJobID, query = {}, options) {
		return this._client.getAPIList(path$1`/fine_tuning/jobs/${fineTuningJobID}/events`, CursorPage, {
			query,
			...options
		});
	}
	/**
	* Pause a fine-tune job.
	*
	* @example
	* ```ts
	* const fineTuningJob = await client.fineTuning.jobs.pause(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* );
	* ```
	*/
	pause(fineTuningJobID, options) {
		return this._client.post(path$1`/fine_tuning/jobs/${fineTuningJobID}/pause`, options);
	}
	/**
	* Resume a fine-tune job.
	*
	* @example
	* ```ts
	* const fineTuningJob = await client.fineTuning.jobs.resume(
	*   'ft-AF1WoRqd3aJAHsqc9NY7iL8F',
	* );
	* ```
	*/
	resume(fineTuningJobID, options) {
		return this._client.post(path$1`/fine_tuning/jobs/${fineTuningJobID}/resume`, options);
	}
};
Jobs.Checkpoints = Checkpoints;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/fine-tuning/fine-tuning.mjs
var FineTuning = class extends APIResource {
	constructor() {
		super(...arguments);
		this.methods = new Methods(this._client);
		this.jobs = new Jobs(this._client);
		this.checkpoints = new Checkpoints$1(this._client);
		this.alpha = new Alpha(this._client);
	}
};
FineTuning.Methods = Methods;
FineTuning.Jobs = Jobs;
FineTuning.Checkpoints = Checkpoints$1;
FineTuning.Alpha = Alpha;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/graders/grader-models.mjs
var GraderModels = class extends APIResource {};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/graders/graders.mjs
var Graders = class extends APIResource {
	constructor() {
		super(...arguments);
		this.graderModels = new GraderModels(this._client);
	}
};
Graders.GraderModels = GraderModels;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/images.mjs
var Images = class extends APIResource {
	/**
	* Creates a variation of a given image. This endpoint only supports `dall-e-2`.
	*
	* @example
	* ```ts
	* const imagesResponse = await client.images.createVariation({
	*   image: fs.createReadStream('otter.png'),
	* });
	* ```
	*/
	createVariation(body, options) {
		return this._client.post("/images/variations", multipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
	edit(body, options) {
		return this._client.post("/images/edits", multipartFormRequestOptions({
			body,
			...options,
			stream: body.stream ?? false
		}, this._client));
	}
	generate(body, options) {
		return this._client.post("/images/generations", {
			body,
			...options,
			stream: body.stream ?? false
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/models.mjs
var Models = class extends APIResource {
	/**
	* Retrieves a model instance, providing basic information about the model such as
	* the owner and permissioning.
	*/
	retrieve(model, options) {
		return this._client.get(path$1`/models/${model}`, options);
	}
	/**
	* Lists the currently available models, and provides basic information about each
	* one such as the owner and availability.
	*/
	list(options) {
		return this._client.getAPIList("/models", Page, options);
	}
	/**
	* Delete a fine-tuned model. You must have the Owner role in your organization to
	* delete a model.
	*/
	delete(model, options) {
		return this._client.delete(path$1`/models/${model}`, options);
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/moderations.mjs
var Moderations = class extends APIResource {
	/**
	* Classifies if text and/or image inputs are potentially harmful. Learn more in
	* the [moderation guide](https://platform.openai.com/docs/guides/moderation).
	*/
	create(body, options) {
		return this._client.post("/moderations", {
			body,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/realtime/calls.mjs
var Calls = class extends APIResource {
	/**
	* Accept an incoming SIP call and configure the realtime session that will handle
	* it.
	*
	* @example
	* ```ts
	* await client.realtime.calls.accept('call_id', {
	*   type: 'realtime',
	* });
	* ```
	*/
	accept(callID, body, options) {
		return this._client.post(path$1`/realtime/calls/${callID}/accept`, {
			body,
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
	/**
	* End an active Realtime API call, whether it was initiated over SIP or WebRTC.
	*
	* @example
	* ```ts
	* await client.realtime.calls.hangup('call_id');
	* ```
	*/
	hangup(callID, options) {
		return this._client.post(path$1`/realtime/calls/${callID}/hangup`, {
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
	/**
	* Transfer an active SIP call to a new destination using the SIP REFER verb.
	*
	* @example
	* ```ts
	* await client.realtime.calls.refer('call_id', {
	*   target_uri: 'tel:+14155550123',
	* });
	* ```
	*/
	refer(callID, body, options) {
		return this._client.post(path$1`/realtime/calls/${callID}/refer`, {
			body,
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
	/**
	* Decline an incoming SIP call by returning a SIP status code to the caller.
	*
	* @example
	* ```ts
	* await client.realtime.calls.reject('call_id');
	* ```
	*/
	reject(callID, body = {}, options) {
		return this._client.post(path$1`/realtime/calls/${callID}/reject`, {
			body,
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/realtime/client-secrets.mjs
var ClientSecrets = class extends APIResource {
	/**
	* Create a Realtime client secret with an associated session configuration.
	*
	* @example
	* ```ts
	* const clientSecret =
	*   await client.realtime.clientSecrets.create();
	* ```
	*/
	create(body, options) {
		return this._client.post("/realtime/client_secrets", {
			body,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/realtime/realtime.mjs
var Realtime = class extends APIResource {
	constructor() {
		super(...arguments);
		this.clientSecrets = new ClientSecrets(this._client);
		this.calls = new Calls(this._client);
	}
};
Realtime.ClientSecrets = ClientSecrets;
Realtime.Calls = Calls;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/ResponsesParser.mjs
function maybeParseResponse(response, params) {
	if (!params || !hasAutoParseableInput(params)) return {
		...response,
		output_parsed: null,
		output: response.output.map((item) => {
			if (item.type === "function_call") return {
				...item,
				parsed_arguments: null
			};
			if (item.type === "message") return {
				...item,
				content: item.content.map((content) => ({
					...content,
					parsed: null
				}))
			};
			else return item;
		})
	};
	return parseResponse(response, params);
}
function parseResponse(response, params) {
	const output = response.output.map((item) => {
		if (item.type === "function_call") return {
			...item,
			parsed_arguments: parseToolCall(params, item)
		};
		if (item.type === "message") {
			const content = item.content.map((content$1) => {
				if (content$1.type === "output_text") return {
					...content$1,
					parsed: parseTextFormat(params, content$1.text)
				};
				return content$1;
			});
			return {
				...item,
				content
			};
		}
		return item;
	});
	const parsed = Object.assign({}, response, { output });
	if (!Object.getOwnPropertyDescriptor(response, "output_text")) addOutputText(parsed);
	Object.defineProperty(parsed, "output_parsed", {
		enumerable: true,
		get() {
			for (const output$1 of parsed.output) {
				if (output$1.type !== "message") continue;
				for (const content of output$1.content) if (content.type === "output_text" && content.parsed !== null) return content.parsed;
			}
			return null;
		}
	});
	return parsed;
}
function parseTextFormat(params, content) {
	if (params.text?.format?.type !== "json_schema") return null;
	if ("$parseRaw" in params.text?.format) return (params.text?.format).$parseRaw(content);
	return JSON.parse(content);
}
function hasAutoParseableInput(params) {
	if (isAutoParsableResponseFormat(params.text?.format)) return true;
	return false;
}
function isAutoParsableTool(tool$1) {
	return tool$1?.["$brand"] === "auto-parseable-tool";
}
function getInputToolByName(input_tools, name) {
	return input_tools.find((tool$1) => tool$1.type === "function" && tool$1.name === name);
}
function parseToolCall(params, toolCall) {
	const inputTool = getInputToolByName(params.tools ?? [], toolCall.name);
	return {
		...toolCall,
		...toolCall,
		parsed_arguments: isAutoParsableTool(inputTool) ? inputTool.$parseRaw(toolCall.arguments) : inputTool?.strict ? JSON.parse(toolCall.arguments) : null
	};
}
function addOutputText(rsp) {
	const texts = [];
	for (const output of rsp.output) {
		if (output.type !== "message") continue;
		for (const content of output.content) if (content.type === "output_text") texts.push(content.text);
	}
	rsp.output_text = texts.join("");
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/responses/ResponseStream.mjs
var _ResponseStream_instances, _ResponseStream_params, _ResponseStream_currentResponseSnapshot, _ResponseStream_finalResponse, _ResponseStream_beginRequest, _ResponseStream_addEvent, _ResponseStream_endRequest, _ResponseStream_accumulateResponse;
var ResponseStream = class ResponseStream extends EventStream {
	constructor(params) {
		super();
		_ResponseStream_instances.add(this);
		_ResponseStream_params.set(this, void 0);
		_ResponseStream_currentResponseSnapshot.set(this, void 0);
		_ResponseStream_finalResponse.set(this, void 0);
		__classPrivateFieldSet(this, _ResponseStream_params, params, "f");
	}
	static createResponse(client, params, options) {
		const runner = new ResponseStream(params);
		runner._run(() => runner._createOrRetrieveResponse(client, params, {
			...options,
			headers: {
				...options?.headers,
				"X-Stainless-Helper-Method": "stream"
			}
		}));
		return runner;
	}
	async _createOrRetrieveResponse(client, params, options) {
		const signal = options?.signal;
		if (signal) {
			if (signal.aborted) this.controller.abort();
			signal.addEventListener("abort", () => this.controller.abort());
		}
		__classPrivateFieldGet(this, _ResponseStream_instances, "m", _ResponseStream_beginRequest).call(this);
		let stream;
		let starting_after = null;
		if ("response_id" in params) {
			stream = await client.responses.retrieve(params.response_id, { stream: true }, {
				...options,
				signal: this.controller.signal,
				stream: true
			});
			starting_after = params.starting_after ?? null;
		} else stream = await client.responses.create({
			...params,
			stream: true
		}, {
			...options,
			signal: this.controller.signal
		});
		this._connected();
		for await (const event of stream) __classPrivateFieldGet(this, _ResponseStream_instances, "m", _ResponseStream_addEvent).call(this, event, starting_after);
		if (stream.controller.signal?.aborted) throw new APIUserAbortError();
		return __classPrivateFieldGet(this, _ResponseStream_instances, "m", _ResponseStream_endRequest).call(this);
	}
	[(_ResponseStream_params = /* @__PURE__ */ new WeakMap(), _ResponseStream_currentResponseSnapshot = /* @__PURE__ */ new WeakMap(), _ResponseStream_finalResponse = /* @__PURE__ */ new WeakMap(), _ResponseStream_instances = /* @__PURE__ */ new WeakSet(), _ResponseStream_beginRequest = function _ResponseStream_beginRequest$1() {
		if (this.ended) return;
		__classPrivateFieldSet(this, _ResponseStream_currentResponseSnapshot, void 0, "f");
	}, _ResponseStream_addEvent = function _ResponseStream_addEvent$1(event, starting_after) {
		if (this.ended) return;
		const maybeEmit = (name, event$1) => {
			if (starting_after == null || event$1.sequence_number > starting_after) this._emit(name, event$1);
		};
		const response = __classPrivateFieldGet(this, _ResponseStream_instances, "m", _ResponseStream_accumulateResponse).call(this, event);
		maybeEmit("event", event);
		switch (event.type) {
			case "response.output_text.delta": {
				const output = response.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				if (output.type === "message") {
					const content = output.content[event.content_index];
					if (!content) throw new OpenAIError(`missing content at index ${event.content_index}`);
					if (content.type !== "output_text") throw new OpenAIError(`expected content to be 'output_text', got ${content.type}`);
					maybeEmit("response.output_text.delta", {
						...event,
						snapshot: content.text
					});
				}
				break;
			}
			case "response.function_call_arguments.delta": {
				const output = response.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				if (output.type === "function_call") maybeEmit("response.function_call_arguments.delta", {
					...event,
					snapshot: output.arguments
				});
				break;
			}
			default:
				maybeEmit(event.type, event);
				break;
		}
	}, _ResponseStream_endRequest = function _ResponseStream_endRequest$1() {
		if (this.ended) throw new OpenAIError(`stream has ended, this shouldn't happen`);
		const snapshot = __classPrivateFieldGet(this, _ResponseStream_currentResponseSnapshot, "f");
		if (!snapshot) throw new OpenAIError(`request ended without sending any events`);
		__classPrivateFieldSet(this, _ResponseStream_currentResponseSnapshot, void 0, "f");
		const parsedResponse = finalizeResponse(snapshot, __classPrivateFieldGet(this, _ResponseStream_params, "f"));
		__classPrivateFieldSet(this, _ResponseStream_finalResponse, parsedResponse, "f");
		return parsedResponse;
	}, _ResponseStream_accumulateResponse = function _ResponseStream_accumulateResponse$1(event) {
		let snapshot = __classPrivateFieldGet(this, _ResponseStream_currentResponseSnapshot, "f");
		if (!snapshot) {
			if (event.type !== "response.created") throw new OpenAIError(`When snapshot hasn't been set yet, expected 'response.created' event, got ${event.type}`);
			snapshot = __classPrivateFieldSet(this, _ResponseStream_currentResponseSnapshot, event.response, "f");
			return snapshot;
		}
		switch (event.type) {
			case "response.output_item.added":
				snapshot.output.push(event.item);
				break;
			case "response.content_part.added": {
				const output = snapshot.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				const type = output.type;
				const part = event.part;
				if (type === "message" && part.type !== "reasoning_text") output.content.push(part);
				else if (type === "reasoning" && part.type === "reasoning_text") {
					if (!output.content) output.content = [];
					output.content.push(part);
				}
				break;
			}
			case "response.output_text.delta": {
				const output = snapshot.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				if (output.type === "message") {
					const content = output.content[event.content_index];
					if (!content) throw new OpenAIError(`missing content at index ${event.content_index}`);
					if (content.type !== "output_text") throw new OpenAIError(`expected content to be 'output_text', got ${content.type}`);
					content.text += event.delta;
				}
				break;
			}
			case "response.function_call_arguments.delta": {
				const output = snapshot.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				if (output.type === "function_call") output.arguments += event.delta;
				break;
			}
			case "response.reasoning_text.delta": {
				const output = snapshot.output[event.output_index];
				if (!output) throw new OpenAIError(`missing output at index ${event.output_index}`);
				if (output.type === "reasoning") {
					const content = output.content?.[event.content_index];
					if (!content) throw new OpenAIError(`missing content at index ${event.content_index}`);
					if (content.type !== "reasoning_text") throw new OpenAIError(`expected content to be 'reasoning_text', got ${content.type}`);
					content.text += event.delta;
				}
				break;
			}
			case "response.completed":
				__classPrivateFieldSet(this, _ResponseStream_currentResponseSnapshot, event.response, "f");
				break;
		}
		return snapshot;
	}, Symbol.asyncIterator)]() {
		const pushQueue = [];
		const readQueue = [];
		let done = false;
		this.on("event", (event) => {
			const reader = readQueue.shift();
			if (reader) reader.resolve(event);
			else pushQueue.push(event);
		});
		this.on("end", () => {
			done = true;
			for (const reader of readQueue) reader.resolve(void 0);
			readQueue.length = 0;
		});
		this.on("abort", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		this.on("error", (err) => {
			done = true;
			for (const reader of readQueue) reader.reject(err);
			readQueue.length = 0;
		});
		return {
			next: async () => {
				if (!pushQueue.length) {
					if (done) return {
						value: void 0,
						done: true
					};
					return new Promise((resolve, reject) => readQueue.push({
						resolve,
						reject
					})).then((event) => event ? {
						value: event,
						done: false
					} : {
						value: void 0,
						done: true
					});
				}
				return {
					value: pushQueue.shift(),
					done: false
				};
			},
			return: async () => {
				this.abort();
				return {
					value: void 0,
					done: true
				};
			}
		};
	}
	/**
	* @returns a promise that resolves with the final Response, or rejects
	* if an error occurred or the stream ended prematurely without producing a REsponse.
	*/
	async finalResponse() {
		await this.done();
		const response = __classPrivateFieldGet(this, _ResponseStream_finalResponse, "f");
		if (!response) throw new OpenAIError("stream ended without producing a ChatCompletion");
		return response;
	}
};
function finalizeResponse(snapshot, params) {
	return maybeParseResponse(snapshot, params);
}

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/responses/input-items.mjs
var InputItems = class extends APIResource {
	/**
	* Returns a list of input items for a given response.
	*
	* @example
	* ```ts
	* // Automatically fetches more pages as needed.
	* for await (const responseItem of client.responses.inputItems.list(
	*   'response_id',
	* )) {
	*   // ...
	* }
	* ```
	*/
	list(responseID, query = {}, options) {
		return this._client.getAPIList(path$1`/responses/${responseID}/input_items`, CursorPage, {
			query,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/responses/input-tokens.mjs
var InputTokens = class extends APIResource {
	/**
	* Get input token counts
	*
	* @example
	* ```ts
	* const response = await client.responses.inputTokens.count();
	* ```
	*/
	count(body = {}, options) {
		return this._client.post("/responses/input_tokens", {
			body,
			...options
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/responses/responses.mjs
var Responses = class extends APIResource {
	constructor() {
		super(...arguments);
		this.inputItems = new InputItems(this._client);
		this.inputTokens = new InputTokens(this._client);
	}
	create(body, options) {
		return this._client.post("/responses", {
			body,
			...options,
			stream: body.stream ?? false
		})._thenUnwrap((rsp) => {
			if ("object" in rsp && rsp.object === "response") addOutputText(rsp);
			return rsp;
		});
	}
	retrieve(responseID, query = {}, options) {
		return this._client.get(path$1`/responses/${responseID}`, {
			query,
			...options,
			stream: query?.stream ?? false
		})._thenUnwrap((rsp) => {
			if ("object" in rsp && rsp.object === "response") addOutputText(rsp);
			return rsp;
		});
	}
	/**
	* Deletes a model response with the given ID.
	*
	* @example
	* ```ts
	* await client.responses.delete(
	*   'resp_677efb5139a88190b512bc3fef8e535d',
	* );
	* ```
	*/
	delete(responseID, options) {
		return this._client.delete(path$1`/responses/${responseID}`, {
			...options,
			headers: buildHeaders([{ Accept: "*/*" }, options?.headers])
		});
	}
	parse(body, options) {
		return this._client.responses.create(body, options)._thenUnwrap((response) => parseResponse(response, body));
	}
	/**
	* Creates a model response stream
	*/
	stream(body, options) {
		return ResponseStream.createResponse(this._client, body, options);
	}
	/**
	* Cancels a model response with the given ID. Only responses created with the
	* `background` parameter set to `true` can be cancelled.
	* [Learn more](https://platform.openai.com/docs/guides/background).
	*
	* @example
	* ```ts
	* const response = await client.responses.cancel(
	*   'resp_677efb5139a88190b512bc3fef8e535d',
	* );
	* ```
	*/
	cancel(responseID, options) {
		return this._client.post(path$1`/responses/${responseID}/cancel`, options);
	}
	/**
	* Compact conversation
	*
	* @example
	* ```ts
	* const compactedResponse = await client.responses.compact({
	*   model: 'gpt-5.2',
	* });
	* ```
	*/
	compact(body, options) {
		return this._client.post("/responses/compact", {
			body,
			...options
		});
	}
};
Responses.InputItems = InputItems;
Responses.InputTokens = InputTokens;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/uploads/parts.mjs
var Parts = class extends APIResource {
	/**
	* Adds a
	* [Part](https://platform.openai.com/docs/api-reference/uploads/part-object) to an
	* [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object.
	* A Part represents a chunk of bytes from the file you are trying to upload.
	*
	* Each Part can be at most 64 MB, and you can add Parts until you hit the Upload
	* maximum of 8 GB.
	*
	* It is possible to add multiple Parts in parallel. You can decide the intended
	* order of the Parts when you
	* [complete the Upload](https://platform.openai.com/docs/api-reference/uploads/complete).
	*/
	create(uploadID, body, options) {
		return this._client.post(path$1`/uploads/${uploadID}/parts`, multipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/uploads/uploads.mjs
var Uploads = class extends APIResource {
	constructor() {
		super(...arguments);
		this.parts = new Parts(this._client);
	}
	/**
	* Creates an intermediate
	* [Upload](https://platform.openai.com/docs/api-reference/uploads/object) object
	* that you can add
	* [Parts](https://platform.openai.com/docs/api-reference/uploads/part-object) to.
	* Currently, an Upload can accept at most 8 GB in total and expires after an hour
	* after you create it.
	*
	* Once you complete the Upload, we will create a
	* [File](https://platform.openai.com/docs/api-reference/files/object) object that
	* contains all the parts you uploaded. This File is usable in the rest of our
	* platform as a regular File object.
	*
	* For certain `purpose` values, the correct `mime_type` must be specified. Please
	* refer to documentation for the
	* [supported MIME types for your use case](https://platform.openai.com/docs/assistants/tools/file-search#supported-files).
	*
	* For guidance on the proper filename extensions for each purpose, please follow
	* the documentation on
	* [creating a File](https://platform.openai.com/docs/api-reference/files/create).
	*/
	create(body, options) {
		return this._client.post("/uploads", {
			body,
			...options
		});
	}
	/**
	* Cancels the Upload. No Parts may be added after an Upload is cancelled.
	*/
	cancel(uploadID, options) {
		return this._client.post(path$1`/uploads/${uploadID}/cancel`, options);
	}
	/**
	* Completes the
	* [Upload](https://platform.openai.com/docs/api-reference/uploads/object).
	*
	* Within the returned Upload object, there is a nested
	* [File](https://platform.openai.com/docs/api-reference/files/object) object that
	* is ready to use in the rest of the platform.
	*
	* You can specify the order of the Parts by passing in an ordered list of the Part
	* IDs.
	*
	* The number of bytes uploaded upon completion must match the number of bytes
	* initially specified when creating the Upload object. No Parts may be added after
	* an Upload is completed.
	*/
	complete(uploadID, body, options) {
		return this._client.post(path$1`/uploads/${uploadID}/complete`, {
			body,
			...options
		});
	}
};
Uploads.Parts = Parts;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/lib/Util.mjs
/**
* Like `Promise.allSettled()` but throws an error if any promises are rejected.
*/
const allSettledWithThrow = async (promises) => {
	const results = await Promise.allSettled(promises);
	const rejected = results.filter((result) => result.status === "rejected");
	if (rejected.length) {
		for (const result of rejected) console.error(result.reason);
		throw new Error(`${rejected.length} promise(s) failed - see the above errors`);
	}
	const values = [];
	for (const result of results) if (result.status === "fulfilled") values.push(result.value);
	return values;
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/vector-stores/file-batches.mjs
var FileBatches = class extends APIResource {
	/**
	* Create a vector store file batch.
	*/
	create(vectorStoreID, body, options) {
		return this._client.post(path$1`/vector_stores/${vectorStoreID}/file_batches`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieves a vector store file batch.
	*/
	retrieve(batchID, params, options) {
		const { vector_store_id } = params;
		return this._client.get(path$1`/vector_stores/${vector_store_id}/file_batches/${batchID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Cancel a vector store file batch. This attempts to cancel the processing of
	* files in this batch as soon as possible.
	*/
	cancel(batchID, params, options) {
		const { vector_store_id } = params;
		return this._client.post(path$1`/vector_stores/${vector_store_id}/file_batches/${batchID}/cancel`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Create a vector store batch and poll until all files have been processed.
	*/
	async createAndPoll(vectorStoreId, body, options) {
		const batch = await this.create(vectorStoreId, body);
		return await this.poll(vectorStoreId, batch.id, options);
	}
	/**
	* Returns a list of vector store files in a batch.
	*/
	listFiles(batchID, params, options) {
		const { vector_store_id, ...query } = params;
		return this._client.getAPIList(path$1`/vector_stores/${vector_store_id}/file_batches/${batchID}/files`, CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Wait for the given file batch to be processed.
	*
	* Note: this will return even if one of the files failed to process, you need to
	* check batch.file_counts.failed_count to handle this case.
	*/
	async poll(vectorStoreID, batchID, options) {
		const headers = buildHeaders([options?.headers, {
			"X-Stainless-Poll-Helper": "true",
			"X-Stainless-Custom-Poll-Interval": options?.pollIntervalMs?.toString() ?? void 0
		}]);
		while (true) {
			const { data: batch, response } = await this.retrieve(batchID, { vector_store_id: vectorStoreID }, {
				...options,
				headers
			}).withResponse();
			switch (batch.status) {
				case "in_progress":
					let sleepInterval = 5e3;
					if (options?.pollIntervalMs) sleepInterval = options.pollIntervalMs;
					else {
						const headerInterval = response.headers.get("openai-poll-after-ms");
						if (headerInterval) {
							const headerIntervalMs = parseInt(headerInterval);
							if (!isNaN(headerIntervalMs)) sleepInterval = headerIntervalMs;
						}
					}
					await sleep(sleepInterval);
					break;
				case "failed":
				case "cancelled":
				case "completed": return batch;
			}
		}
	}
	/**
	* Uploads the given files concurrently and then creates a vector store file batch.
	*
	* The concurrency limit is configurable using the `maxConcurrency` parameter.
	*/
	async uploadAndPoll(vectorStoreId, { files, fileIds = [] }, options) {
		if (files == null || files.length == 0) throw new Error(`No \`files\` provided to process. If you've already uploaded files you should use \`.createAndPoll()\` instead`);
		const configuredConcurrency = options?.maxConcurrency ?? 5;
		const concurrencyLimit = Math.min(configuredConcurrency, files.length);
		const client = this._client;
		const fileIterator = files.values();
		const allFileIds = [...fileIds];
		async function processFiles(iterator) {
			for (let item of iterator) {
				const fileObj = await client.files.create({
					file: item,
					purpose: "assistants"
				}, options);
				allFileIds.push(fileObj.id);
			}
		}
		await allSettledWithThrow(Array(concurrencyLimit).fill(fileIterator).map(processFiles));
		return await this.createAndPoll(vectorStoreId, { file_ids: allFileIds });
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/vector-stores/files.mjs
var Files = class extends APIResource {
	/**
	* Create a vector store file by attaching a
	* [File](https://platform.openai.com/docs/api-reference/files) to a
	* [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).
	*/
	create(vectorStoreID, body, options) {
		return this._client.post(path$1`/vector_stores/${vectorStoreID}/files`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieves a vector store file.
	*/
	retrieve(fileID, params, options) {
		const { vector_store_id } = params;
		return this._client.get(path$1`/vector_stores/${vector_store_id}/files/${fileID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Update attributes on a vector store file.
	*/
	update(fileID, params, options) {
		const { vector_store_id, ...body } = params;
		return this._client.post(path$1`/vector_stores/${vector_store_id}/files/${fileID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of vector store files.
	*/
	list(vectorStoreID, query = {}, options) {
		return this._client.getAPIList(path$1`/vector_stores/${vectorStoreID}/files`, CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Delete a vector store file. This will remove the file from the vector store but
	* the file itself will not be deleted. To delete the file, use the
	* [delete file](https://platform.openai.com/docs/api-reference/files/delete)
	* endpoint.
	*/
	delete(fileID, params, options) {
		const { vector_store_id } = params;
		return this._client.delete(path$1`/vector_stores/${vector_store_id}/files/${fileID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Attach a file to the given vector store and wait for it to be processed.
	*/
	async createAndPoll(vectorStoreId, body, options) {
		const file = await this.create(vectorStoreId, body, options);
		return await this.poll(vectorStoreId, file.id, options);
	}
	/**
	* Wait for the vector store file to finish processing.
	*
	* Note: this will return even if the file failed to process, you need to check
	* file.last_error and file.status to handle these cases
	*/
	async poll(vectorStoreID, fileID, options) {
		const headers = buildHeaders([options?.headers, {
			"X-Stainless-Poll-Helper": "true",
			"X-Stainless-Custom-Poll-Interval": options?.pollIntervalMs?.toString() ?? void 0
		}]);
		while (true) {
			const fileResponse = await this.retrieve(fileID, { vector_store_id: vectorStoreID }, {
				...options,
				headers
			}).withResponse();
			const file = fileResponse.data;
			switch (file.status) {
				case "in_progress":
					let sleepInterval = 5e3;
					if (options?.pollIntervalMs) sleepInterval = options.pollIntervalMs;
					else {
						const headerInterval = fileResponse.response.headers.get("openai-poll-after-ms");
						if (headerInterval) {
							const headerIntervalMs = parseInt(headerInterval);
							if (!isNaN(headerIntervalMs)) sleepInterval = headerIntervalMs;
						}
					}
					await sleep(sleepInterval);
					break;
				case "failed":
				case "completed": return file;
			}
		}
	}
	/**
	* Upload a file to the `files` API and then attach it to the given vector store.
	*
	* Note the file will be asynchronously processed (you can use the alternative
	* polling helper method to wait for processing to complete).
	*/
	async upload(vectorStoreId, file, options) {
		const fileInfo = await this._client.files.create({
			file,
			purpose: "assistants"
		}, options);
		return this.create(vectorStoreId, { file_id: fileInfo.id }, options);
	}
	/**
	* Add a file to a vector store and poll until processing is complete.
	*/
	async uploadAndPoll(vectorStoreId, file, options) {
		const fileInfo = await this.upload(vectorStoreId, file, options);
		return await this.poll(vectorStoreId, fileInfo.id, options);
	}
	/**
	* Retrieve the parsed contents of a vector store file.
	*/
	content(fileID, params, options) {
		const { vector_store_id } = params;
		return this._client.getAPIList(path$1`/vector_stores/${vector_store_id}/files/${fileID}/content`, Page, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/vector-stores/vector-stores.mjs
var VectorStores = class extends APIResource {
	constructor() {
		super(...arguments);
		this.files = new Files(this._client);
		this.fileBatches = new FileBatches(this._client);
	}
	/**
	* Create a vector store.
	*/
	create(body, options) {
		return this._client.post("/vector_stores", {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Retrieves a vector store.
	*/
	retrieve(vectorStoreID, options) {
		return this._client.get(path$1`/vector_stores/${vectorStoreID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Modifies a vector store.
	*/
	update(vectorStoreID, body, options) {
		return this._client.post(path$1`/vector_stores/${vectorStoreID}`, {
			body,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Returns a list of vector stores.
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/vector_stores", CursorPage, {
			query,
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Delete a vector store.
	*/
	delete(vectorStoreID, options) {
		return this._client.delete(path$1`/vector_stores/${vectorStoreID}`, {
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
	/**
	* Search a vector store for relevant chunks based on a query and file attributes
	* filter.
	*/
	search(vectorStoreID, body, options) {
		return this._client.getAPIList(path$1`/vector_stores/${vectorStoreID}/search`, Page, {
			body,
			method: "post",
			...options,
			headers: buildHeaders([{ "OpenAI-Beta": "assistants=v2" }, options?.headers])
		});
	}
};
VectorStores.Files = Files;
VectorStores.FileBatches = FileBatches;

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/videos.mjs
var Videos = class extends APIResource {
	/**
	* Create a video
	*/
	create(body, options) {
		return this._client.post("/videos", maybeMultipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
	/**
	* Retrieve a video
	*/
	retrieve(videoID, options) {
		return this._client.get(path$1`/videos/${videoID}`, options);
	}
	/**
	* List videos
	*/
	list(query = {}, options) {
		return this._client.getAPIList("/videos", ConversationCursorPage, {
			query,
			...options
		});
	}
	/**
	* Delete a video
	*/
	delete(videoID, options) {
		return this._client.delete(path$1`/videos/${videoID}`, options);
	}
	/**
	* Download video content
	*/
	downloadContent(videoID, query = {}, options) {
		return this._client.get(path$1`/videos/${videoID}/content`, {
			query,
			...options,
			headers: buildHeaders([{ Accept: "application/binary" }, options?.headers]),
			__binaryResponse: true
		});
	}
	/**
	* Create a video remix
	*/
	remix(videoID, body, options) {
		return this._client.post(path$1`/videos/${videoID}/remix`, maybeMultipartFormRequestOptions({
			body,
			...options
		}, this._client));
	}
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/resources/webhooks.mjs
var _Webhooks_instances, _Webhooks_validateSecret, _Webhooks_getRequiredHeader;
var Webhooks = class extends APIResource {
	constructor() {
		super(...arguments);
		_Webhooks_instances.add(this);
	}
	/**
	* Validates that the given payload was sent by OpenAI and parses the payload.
	*/
	async unwrap(payload, headers, secret = this._client.webhookSecret, tolerance = 300) {
		await this.verifySignature(payload, headers, secret, tolerance);
		return JSON.parse(payload);
	}
	/**
	* Validates whether or not the webhook payload was sent by OpenAI.
	*
	* An error will be raised if the webhook payload was not sent by OpenAI.
	*
	* @param payload - The webhook payload
	* @param headers - The webhook headers
	* @param secret - The webhook secret (optional, will use client secret if not provided)
	* @param tolerance - Maximum age of the webhook in seconds (default: 300 = 5 minutes)
	*/
	async verifySignature(payload, headers, secret = this._client.webhookSecret, tolerance = 300) {
		if (typeof crypto === "undefined" || typeof crypto.subtle.importKey !== "function" || typeof crypto.subtle.verify !== "function") throw new Error("Webhook signature verification is only supported when the `crypto` global is defined");
		__classPrivateFieldGet(this, _Webhooks_instances, "m", _Webhooks_validateSecret).call(this, secret);
		const headersObj = buildHeaders([headers]).values;
		const signatureHeader = __classPrivateFieldGet(this, _Webhooks_instances, "m", _Webhooks_getRequiredHeader).call(this, headersObj, "webhook-signature");
		const timestamp = __classPrivateFieldGet(this, _Webhooks_instances, "m", _Webhooks_getRequiredHeader).call(this, headersObj, "webhook-timestamp");
		const webhookId = __classPrivateFieldGet(this, _Webhooks_instances, "m", _Webhooks_getRequiredHeader).call(this, headersObj, "webhook-id");
		const timestampSeconds = parseInt(timestamp, 10);
		if (isNaN(timestampSeconds)) throw new InvalidWebhookSignatureError("Invalid webhook timestamp format");
		const nowSeconds = Math.floor(Date.now() / 1e3);
		if (nowSeconds - timestampSeconds > tolerance) throw new InvalidWebhookSignatureError("Webhook timestamp is too old");
		if (timestampSeconds > nowSeconds + tolerance) throw new InvalidWebhookSignatureError("Webhook timestamp is too new");
		const signatures = signatureHeader.split(" ").map((part) => part.startsWith("v1,") ? part.substring(3) : part);
		const decodedSecret = secret.startsWith("whsec_") ? Buffer.from(secret.replace("whsec_", ""), "base64") : Buffer.from(secret, "utf-8");
		const signedPayload = webhookId ? `${webhookId}.${timestamp}.${payload}` : `${timestamp}.${payload}`;
		const key = await crypto.subtle.importKey("raw", decodedSecret, {
			name: "HMAC",
			hash: "SHA-256"
		}, false, ["verify"]);
		for (const signature of signatures) try {
			const signatureBytes = Buffer.from(signature, "base64");
			if (await crypto.subtle.verify("HMAC", key, signatureBytes, new TextEncoder().encode(signedPayload))) return;
		} catch {
			continue;
		}
		throw new InvalidWebhookSignatureError("The given webhook signature does not match the expected signature");
	}
};
_Webhooks_instances = /* @__PURE__ */ new WeakSet(), _Webhooks_validateSecret = function _Webhooks_validateSecret$1(secret) {
	if (typeof secret !== "string" || secret.length === 0) throw new Error(`The webhook secret must either be set using the env var, OPENAI_WEBHOOK_SECRET, on the client class, OpenAI({ webhookSecret: '123' }), or passed to this function`);
}, _Webhooks_getRequiredHeader = function _Webhooks_getRequiredHeader$1(headers, name) {
	if (!headers) throw new Error(`Headers are required`);
	const value = headers.get(name);
	if (value === null || value === void 0) throw new Error(`Missing required header: ${name}`);
	return value;
};

//#endregion
//#region ../../node_modules/.pnpm/openai@6.16.0_zod@4.3.6/node_modules/openai/client.mjs
var _OpenAI_instances, _a, _OpenAI_encoder, _OpenAI_baseURLOverridden;
/**
* API Client for interfacing with the OpenAI API.
*/
var OpenAI = class {
	/**
	* API Client for interfacing with the OpenAI API.
	*
	* @param {string | undefined} [opts.apiKey=process.env['OPENAI_API_KEY'] ?? undefined]
	* @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]
	* @param {string | null | undefined} [opts.project=process.env['OPENAI_PROJECT_ID'] ?? null]
	* @param {string | null | undefined} [opts.webhookSecret=process.env['OPENAI_WEBHOOK_SECRET'] ?? null]
	* @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL'] ?? https://api.openai.com/v1] - Override the default base URL for the API.
	* @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.
	* @param {MergedRequestInit} [opts.fetchOptions] - Additional `RequestInit` options to be passed to `fetch` calls.
	* @param {Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.
	* @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.
	* @param {HeadersLike} opts.defaultHeaders - Default headers to include with every request to the API.
	* @param {Record<string, string | undefined>} opts.defaultQuery - Default query parameters to include with every request to the API.
	* @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.
	*/
	constructor({ baseURL = readEnv("OPENAI_BASE_URL"), apiKey = readEnv("OPENAI_API_KEY"), organization = readEnv("OPENAI_ORG_ID") ?? null, project = readEnv("OPENAI_PROJECT_ID") ?? null, webhookSecret = readEnv("OPENAI_WEBHOOK_SECRET") ?? null, ...opts } = {}) {
		_OpenAI_instances.add(this);
		_OpenAI_encoder.set(this, void 0);
		this.completions = new Completions(this);
		this.chat = new Chat(this);
		this.embeddings = new Embeddings(this);
		this.files = new Files$1(this);
		this.images = new Images(this);
		this.audio = new Audio(this);
		this.moderations = new Moderations(this);
		this.models = new Models(this);
		this.fineTuning = new FineTuning(this);
		this.graders = new Graders(this);
		this.vectorStores = new VectorStores(this);
		this.webhooks = new Webhooks(this);
		this.beta = new Beta(this);
		this.batches = new Batches(this);
		this.uploads = new Uploads(this);
		this.responses = new Responses(this);
		this.realtime = new Realtime(this);
		this.conversations = new Conversations(this);
		this.evals = new Evals(this);
		this.containers = new Containers(this);
		this.videos = new Videos(this);
		if (apiKey === void 0) throw new OpenAIError("Missing credentials. Please pass an `apiKey`, or set the `OPENAI_API_KEY` environment variable.");
		const options = {
			apiKey,
			organization,
			project,
			webhookSecret,
			...opts,
			baseURL: baseURL || `https://api.openai.com/v1`
		};
		if (!options.dangerouslyAllowBrowser && isRunningInBrowser()) throw new OpenAIError("It looks like you're running in a browser-like environment.\n\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\nIf you understand the risks and have appropriate mitigations in place,\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\n\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\n\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\n");
		this.baseURL = options.baseURL;
		this.timeout = options.timeout ?? _a.DEFAULT_TIMEOUT;
		this.logger = options.logger ?? console;
		const defaultLogLevel = "warn";
		this.logLevel = defaultLogLevel;
		this.logLevel = parseLogLevel(options.logLevel, "ClientOptions.logLevel", this) ?? parseLogLevel(readEnv("OPENAI_LOG"), "process.env['OPENAI_LOG']", this) ?? defaultLogLevel;
		this.fetchOptions = options.fetchOptions;
		this.maxRetries = options.maxRetries ?? 2;
		this.fetch = options.fetch ?? getDefaultFetch();
		__classPrivateFieldSet(this, _OpenAI_encoder, FallbackEncoder, "f");
		this._options = options;
		this.apiKey = typeof apiKey === "string" ? apiKey : "Missing Key";
		this.organization = organization;
		this.project = project;
		this.webhookSecret = webhookSecret;
	}
	/**
	* Create a new client instance re-using the same options given to the current client with optional overriding.
	*/
	withOptions(options) {
		return new this.constructor({
			...this._options,
			baseURL: this.baseURL,
			maxRetries: this.maxRetries,
			timeout: this.timeout,
			logger: this.logger,
			logLevel: this.logLevel,
			fetch: this.fetch,
			fetchOptions: this.fetchOptions,
			apiKey: this.apiKey,
			organization: this.organization,
			project: this.project,
			webhookSecret: this.webhookSecret,
			...options
		});
	}
	defaultQuery() {
		return this._options.defaultQuery;
	}
	validateHeaders({ values, nulls }) {}
	async authHeaders(opts) {
		return buildHeaders([{ Authorization: `Bearer ${this.apiKey}` }]);
	}
	stringifyQuery(query) {
		return stringify(query, { arrayFormat: "brackets" });
	}
	getUserAgent() {
		return `${this.constructor.name}/JS ${VERSION}`;
	}
	defaultIdempotencyKey() {
		return `stainless-node-retry-${uuid4()}`;
	}
	makeStatusError(status, error, message, headers) {
		return APIError.generate(status, error, message, headers);
	}
	async _callApiKey() {
		const apiKey = this._options.apiKey;
		if (typeof apiKey !== "function") return false;
		let token;
		try {
			token = await apiKey();
		} catch (err) {
			if (err instanceof OpenAIError) throw err;
			throw new OpenAIError(`Failed to get token from 'apiKey' function: ${err.message}`, { cause: err });
		}
		if (typeof token !== "string" || !token) throw new OpenAIError(`Expected 'apiKey' function argument to return a string but it returned ${token}`);
		this.apiKey = token;
		return true;
	}
	buildURL(path$2, query, defaultBaseURL) {
		const baseURL = !__classPrivateFieldGet(this, _OpenAI_instances, "m", _OpenAI_baseURLOverridden).call(this) && defaultBaseURL || this.baseURL;
		const url = isAbsoluteURL(path$2) ? new URL(path$2) : new URL(baseURL + (baseURL.endsWith("/") && path$2.startsWith("/") ? path$2.slice(1) : path$2));
		const defaultQuery = this.defaultQuery();
		if (!isEmptyObj(defaultQuery)) query = {
			...defaultQuery,
			...query
		};
		if (typeof query === "object" && query && !Array.isArray(query)) url.search = this.stringifyQuery(query);
		return url.toString();
	}
	/**
	* Used as a callback for mutating the given `FinalRequestOptions` object.
	*/
	async prepareOptions(options) {
		await this._callApiKey();
	}
	/**
	* Used as a callback for mutating the given `RequestInit` object.
	*
	* This is useful for cases where you want to add certain headers based off of
	* the request properties, e.g. `method` or `url`.
	*/
	async prepareRequest(request, { url, options }) {}
	get(path$2, opts) {
		return this.methodRequest("get", path$2, opts);
	}
	post(path$2, opts) {
		return this.methodRequest("post", path$2, opts);
	}
	patch(path$2, opts) {
		return this.methodRequest("patch", path$2, opts);
	}
	put(path$2, opts) {
		return this.methodRequest("put", path$2, opts);
	}
	delete(path$2, opts) {
		return this.methodRequest("delete", path$2, opts);
	}
	methodRequest(method, path$2, opts) {
		return this.request(Promise.resolve(opts).then((opts$1) => {
			return {
				method,
				path: path$2,
				...opts$1
			};
		}));
	}
	request(options, remainingRetries = null) {
		return new APIPromise(this, this.makeRequest(options, remainingRetries, void 0));
	}
	async makeRequest(optionsInput, retriesRemaining, retryOfRequestLogID) {
		const options = await optionsInput;
		const maxRetries = options.maxRetries ?? this.maxRetries;
		if (retriesRemaining == null) retriesRemaining = maxRetries;
		await this.prepareOptions(options);
		const { req, url, timeout } = await this.buildRequest(options, { retryCount: maxRetries - retriesRemaining });
		await this.prepareRequest(req, {
			url,
			options
		});
		/** Not an API request ID, just for correlating local log entries. */
		const requestLogID = "log_" + (Math.random() * (1 << 24) | 0).toString(16).padStart(6, "0");
		const retryLogStr = retryOfRequestLogID === void 0 ? "" : `, retryOf: ${retryOfRequestLogID}`;
		const startTime = Date.now();
		loggerFor(this).debug(`[${requestLogID}] sending request`, formatRequestDetails({
			retryOfRequestLogID,
			method: options.method,
			url,
			options,
			headers: req.headers
		}));
		if (options.signal?.aborted) throw new APIUserAbortError();
		const controller = new AbortController();
		const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);
		const headersTime = Date.now();
		if (response instanceof globalThis.Error) {
			const retryMessage = `retrying, ${retriesRemaining} attempts remaining`;
			if (options.signal?.aborted) throw new APIUserAbortError();
			const isTimeout = isAbortError(response) || /timed? ?out/i.test(String(response) + ("cause" in response ? String(response.cause) : ""));
			if (retriesRemaining) {
				loggerFor(this).info(`[${requestLogID}] connection ${isTimeout ? "timed out" : "failed"} - ${retryMessage}`);
				loggerFor(this).debug(`[${requestLogID}] connection ${isTimeout ? "timed out" : "failed"} (${retryMessage})`, formatRequestDetails({
					retryOfRequestLogID,
					url,
					durationMs: headersTime - startTime,
					message: response.message
				}));
				return this.retryRequest(options, retriesRemaining, retryOfRequestLogID ?? requestLogID);
			}
			loggerFor(this).info(`[${requestLogID}] connection ${isTimeout ? "timed out" : "failed"} - error; no more retries left`);
			loggerFor(this).debug(`[${requestLogID}] connection ${isTimeout ? "timed out" : "failed"} (error; no more retries left)`, formatRequestDetails({
				retryOfRequestLogID,
				url,
				durationMs: headersTime - startTime,
				message: response.message
			}));
			if (isTimeout) throw new APIConnectionTimeoutError();
			throw new APIConnectionError({ cause: response });
		}
		const responseInfo = `[${requestLogID}${retryLogStr}${[...response.headers.entries()].filter(([name]) => name === "x-request-id").map(([name, value]) => ", " + name + ": " + JSON.stringify(value)).join("")}] ${req.method} ${url} ${response.ok ? "succeeded" : "failed"} with status ${response.status} in ${headersTime - startTime}ms`;
		if (!response.ok) {
			const shouldRetry = await this.shouldRetry(response);
			if (retriesRemaining && shouldRetry) {
				const retryMessage$1 = `retrying, ${retriesRemaining} attempts remaining`;
				await CancelReadableStream(response.body);
				loggerFor(this).info(`${responseInfo} - ${retryMessage$1}`);
				loggerFor(this).debug(`[${requestLogID}] response error (${retryMessage$1})`, formatRequestDetails({
					retryOfRequestLogID,
					url: response.url,
					status: response.status,
					headers: response.headers,
					durationMs: headersTime - startTime
				}));
				return this.retryRequest(options, retriesRemaining, retryOfRequestLogID ?? requestLogID, response.headers);
			}
			const retryMessage = shouldRetry ? `error; no more retries left` : `error; not retryable`;
			loggerFor(this).info(`${responseInfo} - ${retryMessage}`);
			const errText = await response.text().catch((err) => castToError(err).message);
			const errJSON = safeJSON(errText);
			const errMessage = errJSON ? void 0 : errText;
			loggerFor(this).debug(`[${requestLogID}] response error (${retryMessage})`, formatRequestDetails({
				retryOfRequestLogID,
				url: response.url,
				status: response.status,
				headers: response.headers,
				message: errMessage,
				durationMs: Date.now() - startTime
			}));
			throw this.makeStatusError(response.status, errJSON, errMessage, response.headers);
		}
		loggerFor(this).info(responseInfo);
		loggerFor(this).debug(`[${requestLogID}] response start`, formatRequestDetails({
			retryOfRequestLogID,
			url: response.url,
			status: response.status,
			headers: response.headers,
			durationMs: headersTime - startTime
		}));
		return {
			response,
			options,
			controller,
			requestLogID,
			retryOfRequestLogID,
			startTime
		};
	}
	getAPIList(path$2, Page$1, opts) {
		return this.requestAPIList(Page$1, {
			method: "get",
			path: path$2,
			...opts
		});
	}
	requestAPIList(Page$1, options) {
		const request = this.makeRequest(options, null, void 0);
		return new PagePromise(this, request, Page$1);
	}
	async fetchWithTimeout(url, init, ms, controller) {
		const { signal, method, ...options } = init || {};
		if (signal) signal.addEventListener("abort", () => controller.abort());
		const timeout = setTimeout(() => controller.abort(), ms);
		const isReadableBody = globalThis.ReadableStream && options.body instanceof globalThis.ReadableStream || typeof options.body === "object" && options.body !== null && Symbol.asyncIterator in options.body;
		const fetchOptions = {
			signal: controller.signal,
			...isReadableBody ? { duplex: "half" } : {},
			method: "GET",
			...options
		};
		if (method) fetchOptions.method = method.toUpperCase();
		try {
			return await this.fetch.call(void 0, url, fetchOptions);
		} finally {
			clearTimeout(timeout);
		}
	}
	async shouldRetry(response) {
		const shouldRetryHeader = response.headers.get("x-should-retry");
		if (shouldRetryHeader === "true") return true;
		if (shouldRetryHeader === "false") return false;
		if (response.status === 408) return true;
		if (response.status === 409) return true;
		if (response.status === 429) return true;
		if (response.status >= 500) return true;
		return false;
	}
	async retryRequest(options, retriesRemaining, requestLogID, responseHeaders) {
		let timeoutMillis;
		const retryAfterMillisHeader = responseHeaders?.get("retry-after-ms");
		if (retryAfterMillisHeader) {
			const timeoutMs = parseFloat(retryAfterMillisHeader);
			if (!Number.isNaN(timeoutMs)) timeoutMillis = timeoutMs;
		}
		const retryAfterHeader = responseHeaders?.get("retry-after");
		if (retryAfterHeader && !timeoutMillis) {
			const timeoutSeconds = parseFloat(retryAfterHeader);
			if (!Number.isNaN(timeoutSeconds)) timeoutMillis = timeoutSeconds * 1e3;
			else timeoutMillis = Date.parse(retryAfterHeader) - Date.now();
		}
		if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1e3)) {
			const maxRetries = options.maxRetries ?? this.maxRetries;
			timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);
		}
		await sleep(timeoutMillis);
		return this.makeRequest(options, retriesRemaining - 1, requestLogID);
	}
	calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries) {
		const initialRetryDelay = .5;
		const maxRetryDelay = 8;
		const numRetries = maxRetries - retriesRemaining;
		return Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay) * (1 - Math.random() * .25) * 1e3;
	}
	async buildRequest(inputOptions, { retryCount = 0 } = {}) {
		const options = { ...inputOptions };
		const { method, path: path$2, query, defaultBaseURL } = options;
		const url = this.buildURL(path$2, query, defaultBaseURL);
		if ("timeout" in options) validatePositiveInteger("timeout", options.timeout);
		options.timeout = options.timeout ?? this.timeout;
		const { bodyHeaders, body } = this.buildBody({ options });
		return {
			req: {
				method,
				headers: await this.buildHeaders({
					options: inputOptions,
					method,
					bodyHeaders,
					retryCount
				}),
				...options.signal && { signal: options.signal },
				...globalThis.ReadableStream && body instanceof globalThis.ReadableStream && { duplex: "half" },
				...body && { body },
				...this.fetchOptions ?? {},
				...options.fetchOptions ?? {}
			},
			url,
			timeout: options.timeout
		};
	}
	async buildHeaders({ options, method, bodyHeaders, retryCount }) {
		let idempotencyHeaders = {};
		if (this.idempotencyHeader && method !== "get") {
			if (!options.idempotencyKey) options.idempotencyKey = this.defaultIdempotencyKey();
			idempotencyHeaders[this.idempotencyHeader] = options.idempotencyKey;
		}
		const headers = buildHeaders([
			idempotencyHeaders,
			{
				Accept: "application/json",
				"User-Agent": this.getUserAgent(),
				"X-Stainless-Retry-Count": String(retryCount),
				...options.timeout ? { "X-Stainless-Timeout": String(Math.trunc(options.timeout / 1e3)) } : {},
				...getPlatformHeaders(),
				"OpenAI-Organization": this.organization,
				"OpenAI-Project": this.project
			},
			await this.authHeaders(options),
			this._options.defaultHeaders,
			bodyHeaders,
			options.headers
		]);
		this.validateHeaders(headers);
		return headers.values;
	}
	buildBody({ options: { body, headers: rawHeaders } }) {
		if (!body) return {
			bodyHeaders: void 0,
			body: void 0
		};
		const headers = buildHeaders([rawHeaders]);
		if (ArrayBuffer.isView(body) || body instanceof ArrayBuffer || body instanceof DataView || typeof body === "string" && headers.values.has("content-type") || globalThis.Blob && body instanceof globalThis.Blob || body instanceof FormData || body instanceof URLSearchParams || globalThis.ReadableStream && body instanceof globalThis.ReadableStream) return {
			bodyHeaders: void 0,
			body
		};
		else if (typeof body === "object" && (Symbol.asyncIterator in body || Symbol.iterator in body && "next" in body && typeof body.next === "function")) return {
			bodyHeaders: void 0,
			body: ReadableStreamFrom(body)
		};
		else return __classPrivateFieldGet(this, _OpenAI_encoder, "f").call(this, {
			body,
			headers
		});
	}
};
_a = OpenAI, _OpenAI_encoder = /* @__PURE__ */ new WeakMap(), _OpenAI_instances = /* @__PURE__ */ new WeakSet(), _OpenAI_baseURLOverridden = function _OpenAI_baseURLOverridden$1() {
	return this.baseURL !== "https://api.openai.com/v1";
};
OpenAI.OpenAI = _a;
OpenAI.DEFAULT_TIMEOUT = 6e5;
OpenAI.OpenAIError = OpenAIError;
OpenAI.APIError = APIError;
OpenAI.APIConnectionError = APIConnectionError;
OpenAI.APIConnectionTimeoutError = APIConnectionTimeoutError;
OpenAI.APIUserAbortError = APIUserAbortError;
OpenAI.NotFoundError = NotFoundError;
OpenAI.ConflictError = ConflictError;
OpenAI.RateLimitError = RateLimitError;
OpenAI.BadRequestError = BadRequestError;
OpenAI.AuthenticationError = AuthenticationError;
OpenAI.InternalServerError = InternalServerError;
OpenAI.PermissionDeniedError = PermissionDeniedError;
OpenAI.UnprocessableEntityError = UnprocessableEntityError;
OpenAI.InvalidWebhookSignatureError = InvalidWebhookSignatureError;
OpenAI.toFile = toFile;
OpenAI.Completions = Completions;
OpenAI.Chat = Chat;
OpenAI.Embeddings = Embeddings;
OpenAI.Files = Files$1;
OpenAI.Images = Images;
OpenAI.Audio = Audio;
OpenAI.Moderations = Moderations;
OpenAI.Models = Models;
OpenAI.FineTuning = FineTuning;
OpenAI.Graders = Graders;
OpenAI.VectorStores = VectorStores;
OpenAI.Webhooks = Webhooks;
OpenAI.Beta = Beta;
OpenAI.Batches = Batches;
OpenAI.Uploads = Uploads;
OpenAI.Responses = Responses;
OpenAI.Realtime = Realtime;
OpenAI.Conversations = Conversations;
OpenAI.Evals = Evals;
OpenAI.Containers = Containers;
OpenAI.Videos = Videos;

//#endregion
//#region ../../node_modules/.pnpm/@langchain+openai@1.2.3_@langchain+core@1.1.16_openai@6.16.0_zod@4.3.6__/node_modules/@langchain/openai/dist/tools/computerUse.js
const ComputerUseScreenshotActionSchema = z.object({ type: z.literal("screenshot") });
const ComputerUseClickActionSchema = z.object({
	type: z.literal("click"),
	x: z.number(),
	y: z.number(),
	button: z.enum([
		"left",
		"right",
		"wheel",
		"back",
		"forward"
	]).default("left")
});
const ComputerUseDoubleClickActionSchema = z.object({
	type: z.literal("double_click"),
	x: z.number(),
	y: z.number(),
	button: z.enum([
		"left",
		"right",
		"wheel",
		"back",
		"forward"
	]).default("left")
});
const ComputerUseDragActionSchema = z.object({
	type: z.literal("drag"),
	path: z.array(z.object({
		x: z.number(),
		y: z.number()
	}))
});
const ComputerUseKeypressActionSchema = z.object({
	type: z.literal("keypress"),
	keys: z.array(z.string())
});
const ComputerUseMoveActionSchema = z.object({
	type: z.literal("move"),
	x: z.number(),
	y: z.number()
});
const ComputerUseScrollActionSchema = z.object({
	type: z.literal("scroll"),
	x: z.number(),
	y: z.number(),
	scroll_x: z.number(),
	scroll_y: z.number()
});
const ComputerUseTypeActionSchema = z.object({
	type: z.literal("type"),
	text: z.string()
});
const ComputerUseWaitActionSchema = z.object({
	type: z.literal("wait"),
	duration: z.number().optional()
});
const ComputerUseActionUnionSchema = z.discriminatedUnion("type", [
	ComputerUseScreenshotActionSchema,
	ComputerUseClickActionSchema,
	ComputerUseDoubleClickActionSchema,
	ComputerUseDragActionSchema,
	ComputerUseKeypressActionSchema,
	ComputerUseMoveActionSchema,
	ComputerUseScrollActionSchema,
	ComputerUseTypeActionSchema,
	ComputerUseWaitActionSchema
]);
const ComputerUseActionSchema = z.object({ action: ComputerUseActionUnionSchema });

//#endregion
//#region ../../node_modules/.pnpm/@langchain+openai@1.2.3_@langchain+core@1.1.16_openai@6.16.0_zod@4.3.6__/node_modules/@langchain/openai/dist/tools/localShell.js
const LocalShellExecActionSchema = z.object({
	type: z.literal("exec"),
	command: z.array(z.string()),
	env: z.record(z.string(), z.string()).optional(),
	working_directory: z.string().optional(),
	timeout_ms: z.number().optional(),
	user: z.string().optional()
});
const LocalShellActionSchema = z.discriminatedUnion("type", [LocalShellExecActionSchema]);

//#endregion
//#region ../../node_modules/.pnpm/@langchain+openai@1.2.3_@langchain+core@1.1.16_openai@6.16.0_zod@4.3.6__/node_modules/@langchain/openai/dist/tools/shell.js
const ShellActionSchema = z.object({
	commands: z.array(z.string()).describe("Array of shell commands to execute"),
	timeout_ms: z.number().optional().describe("Optional timeout in milliseconds for the commands"),
	max_output_length: z.number().optional().describe("Optional maximum number of characters to return from each command")
});

//#endregion
//#region ../../node_modules/.pnpm/@langchain+openai@1.2.3_@langchain+core@1.1.16_openai@6.16.0_zod@4.3.6__/node_modules/@langchain/openai/dist/tools/applyPatch.js
const ApplyPatchCreateFileOperationSchema = z.object({
	type: z.literal("create_file"),
	path: z.string(),
	diff: z.string()
});
const ApplyPatchUpdateFileOperationSchema = z.object({
	type: z.literal("update_file"),
	path: z.string(),
	diff: z.string()
});
const ApplyPatchDeleteFileOperationSchema = z.object({
	type: z.literal("delete_file"),
	path: z.string()
});
const ApplyPatchOperationSchema = z.discriminatedUnion("type", [
	ApplyPatchCreateFileOperationSchema,
	ApplyPatchUpdateFileOperationSchema,
	ApplyPatchDeleteFileOperationSchema
]);

//#endregion
//#region src/middleware/summarization.ts
/**
* Summarization middleware with backend support for conversation history offloading.
*
* This module extends the base LangChain summarization middleware with additional
* backend-based features for persisting conversation history before summarization.
*
* ## Usage
*
* ```typescript
* import { createSummarizationMiddleware } from "@anthropic/deepagents";
* import { FilesystemBackend } from "@anthropic/deepagents";
*
* const backend = new FilesystemBackend({ rootDir: "/data" });
*
* const middleware = createSummarizationMiddleware({
*   model: "gpt-4o-mini",
*   backend,
*   trigger: { type: "fraction", value: 0.85 },
*   keep: { type: "fraction", value: 0.10 },
* });
*
* const agent = createDeepAgent({ middleware: [middleware] });
* ```
*
* ## Storage
*
* Offloaded messages are stored as markdown at `/conversation_history/{thread_id}.md`.
*
* Each summarization event appends a new section to this file, creating a running log
* of all evicted messages.
*
* ## Relationship to LangChain Summarization Middleware
*
* The base `summarizationMiddleware` from `langchain` provides core summarization
* functionality. This middleware adds:
* - Backend-based conversation history offloading
* - Tool argument truncation for old messages
*
* For simple use cases without backend offloading, use `summarizationMiddleware`
* from `langchain` directly.
*/
/**
* State schema for summarization middleware.
*/
const SummarizationStateSchema = z$1.object({ _summarizationSessionId: z$1.string().optional() });

//#endregion
//#region src/backends/store.ts
/**
* Backend that stores files in LangGraph's BaseStore (persistent).
*
* Uses LangGraph's Store for persistent, cross-conversation storage.
* Files are organized via namespaces and persist across all threads.
*
* The namespace can include an optional assistant_id for multi-agent isolation.
*/
var StoreBackend = class {
	stateAndStore;
	constructor(stateAndStore) {
		this.stateAndStore = stateAndStore;
	}
	/**
	* Get the store instance.
	*
	* @returns BaseStore instance
	* @throws Error if no store is available
	*/
	getStore() {
		const store = this.stateAndStore.store;
		if (!store) throw new Error("Store is required but not available in StateAndStore");
		return store;
	}
	/**
	* Get the namespace for store operations.
	*
	* If an assistant_id is available in stateAndStore, return
	* [assistant_id, "filesystem"] to provide per-assistant isolation.
	* Otherwise return ["filesystem"].
	*/
	getNamespace() {
		const namespace = "filesystem";
		const assistantId = this.stateAndStore.assistantId;
		if (assistantId) return [assistantId, namespace];
		return [namespace];
	}
	/**
	* Convert a store Item to FileData format.
	*
	* @param storeItem - The store Item containing file data
	* @returns FileData object
	* @throws Error if required fields are missing or have incorrect types
	*/
	convertStoreItemToFileData(storeItem) {
		const value = storeItem.value;
		if (!value.content || !Array.isArray(value.content) || typeof value.created_at !== "string" || typeof value.modified_at !== "string") throw new Error(`Store item does not contain valid FileData fields. Got keys: ${Object.keys(value).join(", ")}`);
		return {
			content: value.content,
			created_at: value.created_at,
			modified_at: value.modified_at
		};
	}
	/**
	* Convert FileData to a value suitable for store.put().
	*
	* @param fileData - The FileData to convert
	* @returns Object with content, created_at, and modified_at fields
	*/
	convertFileDataToStoreValue(fileData) {
		return {
			content: fileData.content,
			created_at: fileData.created_at,
			modified_at: fileData.modified_at
		};
	}
	/**
	* Search store with automatic pagination to retrieve all results.
	*
	* @param store - The store to search
	* @param namespace - Hierarchical path prefix to search within
	* @param options - Optional query, filter, and page_size
	* @returns List of all items matching the search criteria
	*/
	async searchStorePaginated(store, namespace, options = {}) {
		const { query, filter, pageSize = 100 } = options;
		const allItems = [];
		let offset = 0;
		while (true) {
			const pageItems = await store.search(namespace, {
				query,
				filter,
				limit: pageSize,
				offset
			});
			if (!pageItems || pageItems.length === 0) break;
			allItems.push(...pageItems);
			if (pageItems.length < pageSize) break;
			offset += pageSize;
		}
		return allItems;
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(path$2) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const infos = [];
		const subdirs = /* @__PURE__ */ new Set();
		const normalizedPath = path$2.endsWith("/") ? path$2 : path$2 + "/";
		for (const item of items) {
			const itemKey = String(item.key);
			if (!itemKey.startsWith(normalizedPath)) continue;
			const relative = itemKey.substring(normalizedPath.length);
			if (relative.includes("/")) {
				const subdirName = relative.split("/")[0];
				subdirs.add(normalizedPath + subdirName + "/");
				continue;
			}
			try {
				const fd = this.convertStoreItemToFileData(item);
				const size = fd.content.join("\n").length;
				infos.push({
					path: itemKey,
					is_dir: false,
					size,
					modified_at: fd.modified_at
				});
			} catch {
				continue;
			}
		}
		for (const subdir of Array.from(subdirs).sort()) infos.push({
			path: subdir,
			is_dir: true,
			size: 0,
			modified_at: ""
		});
		infos.sort((a, b) => a.path.localeCompare(b.path));
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit$1 = 500) {
		try {
			return formatReadResponse(await this.readRaw(filePath), offset, limit$1);
		} catch (e) {
			return `Error: ${e.message}`;
		}
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const item = await store.get(namespace, filePath);
		if (!item) throw new Error(`File '${filePath}' not found`);
		return this.convertStoreItemToFileData(item);
	}
	/**
	* Create a new file with content.
	* Returns WriteResult. External storage sets filesUpdate=null.
	*/
	async write(filePath, content) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		if (await store.get(namespace, filePath)) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		const fileData = createFileData(content);
		const storeValue = this.convertFileDataToStoreValue(fileData);
		await store.put(namespace, filePath, storeValue);
		return {
			path: filePath,
			filesUpdate: null
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult. External storage sets filesUpdate=null.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const item = await store.get(namespace, filePath);
		if (!item) return { error: `Error: File '${filePath}' not found` };
		try {
			const fileData = this.convertStoreItemToFileData(item);
			const result = performStringReplacement(fileDataToString(fileData), oldString, newString, replaceAll);
			if (typeof result === "string") return { error: result };
			const [newContent, occurrences] = result;
			const newFileData = updateFileData(fileData, newContent);
			const storeValue = this.convertFileDataToStoreValue(newFileData);
			await store.put(namespace, filePath, storeValue);
			return {
				path: filePath,
				filesUpdate: null,
				occurrences
			};
		} catch (e) {
			return { error: `Error: ${e.message}` };
		}
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, path$2 = "/", glob = null) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const files = {};
		for (const item of items) try {
			files[item.key] = this.convertStoreItemToFileData(item);
		} catch {
			continue;
		}
		return grepMatchesFromFiles(files, pattern, path$2, glob);
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, path$2 = "/") {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const items = await this.searchStorePaginated(store, namespace);
		const files = {};
		for (const item of items) try {
			files[item.key] = this.convertStoreItemToFileData(item);
		} catch {
			continue;
		}
		const result = globSearchFiles(files, pattern, path$2);
		if (result === "No files found") return [];
		const paths = result.split("\n");
		const infos = [];
		for (const p of paths) {
			const fd = files[p];
			const size = fd ? fd.content.join("\n").length : 0;
			infos.push({
				path: p,
				is_dir: false,
				size,
				modified_at: fd?.modified_at || ""
			});
		}
		return infos;
	}
	/**
	* Upload multiple files.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const responses = [];
		for (const [path$2, content] of files) try {
			const fileData = createFileData(new TextDecoder().decode(content));
			const storeValue = this.convertFileDataToStoreValue(fileData);
			await store.put(namespace, path$2, storeValue);
			responses.push({
				path: path$2,
				error: null
			});
		} catch {
			responses.push({
				path: path$2,
				error: "invalid_path"
			});
		}
		return responses;
	}
	/**
	* Download multiple files.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const store = this.getStore();
		const namespace = this.getNamespace();
		const responses = [];
		for (const path$2 of paths) try {
			const item = await store.get(namespace, path$2);
			if (!item) {
				responses.push({
					path: path$2,
					content: null,
					error: "file_not_found"
				});
				continue;
			}
			const contentStr = fileDataToString(this.convertStoreItemToFileData(item));
			const content = new TextEncoder().encode(contentStr);
			responses.push({
				path: path$2,
				content,
				error: null
			});
		} catch {
			responses.push({
				path: path$2,
				content: null,
				error: "file_not_found"
			});
		}
		return responses;
	}
};

//#endregion
//#region src/backends/filesystem.ts
/**
* FilesystemBackend: Read and write files directly from the filesystem.
*
* Security and search upgrades:
* - Secure path resolution with root containment when in virtual_mode (sandboxed to cwd)
* - Prevent symlink-following on file I/O using O_NOFOLLOW when available
* - Ripgrep-powered grep with JSON parsing, plus regex fallback
*   and optional glob include filtering, while preserving virtual path behavior
*/
const SUPPORTS_NOFOLLOW = fs$1.constants.O_NOFOLLOW !== void 0;
/**
* Backend that reads and writes files directly from the filesystem.
*
* Files are accessed using their actual filesystem paths. Relative paths are
* resolved relative to the current working directory. Content is read/written
* as plain text, and metadata (timestamps) are derived from filesystem stats.
*/
var FilesystemBackend = class {
	cwd;
	virtualMode;
	maxFileSizeBytes;
	constructor(options = {}) {
		const { rootDir, virtualMode = false, maxFileSizeMb = 10 } = options;
		this.cwd = rootDir ? path.resolve(rootDir) : process.cwd();
		this.virtualMode = virtualMode;
		this.maxFileSizeBytes = maxFileSizeMb * 1024 * 1024;
	}
	/**
	* Resolve a file path with security checks.
	*
	* When virtualMode=true, treat incoming paths as virtual absolute paths under
	* this.cwd, disallow traversal (.., ~) and ensure resolved path stays within root.
	* When virtualMode=false, preserve legacy behavior: absolute paths are allowed
	* as-is; relative paths resolve under cwd.
	*
	* @param key - File path (absolute, relative, or virtual when virtualMode=true)
	* @returns Resolved absolute path string
	* @throws Error if path traversal detected or path outside root
	*/
	resolvePath(key) {
		if (this.virtualMode) {
			const vpath = key.startsWith("/") ? key : "/" + key;
			if (vpath.includes("..") || vpath.startsWith("~")) throw new Error("Path traversal not allowed");
			const full = path.resolve(this.cwd, vpath.substring(1));
			const relative = path.relative(this.cwd, full);
			if (relative.startsWith("..") || path.isAbsolute(relative)) throw new Error(`Path: ${full} outside root directory: ${this.cwd}`);
			return full;
		}
		if (path.isAbsolute(key)) return key;
		return path.resolve(this.cwd, key);
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param dirPath - Absolute directory path to list files from
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*          Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(dirPath) {
		try {
			const resolvedPath = this.resolvePath(dirPath);
			if (!(await fs.stat(resolvedPath)).isDirectory()) return [];
			const entries = await fs.readdir(resolvedPath, { withFileTypes: true });
			const results = [];
			const cwdStr = this.cwd.endsWith(path.sep) ? this.cwd : this.cwd + path.sep;
			for (const entry of entries) {
				const fullPath = path.join(resolvedPath, entry.name);
				try {
					const entryStat = await fs.stat(fullPath);
					const isFile = entryStat.isFile();
					const isDir = entryStat.isDirectory();
					if (!this.virtualMode) {
						if (isFile) results.push({
							path: fullPath,
							is_dir: false,
							size: entryStat.size,
							modified_at: entryStat.mtime.toISOString()
						});
						else if (isDir) results.push({
							path: fullPath + path.sep,
							is_dir: true,
							size: 0,
							modified_at: entryStat.mtime.toISOString()
						});
					} else {
						let relativePath;
						if (fullPath.startsWith(cwdStr)) relativePath = fullPath.substring(cwdStr.length);
						else if (fullPath.startsWith(this.cwd)) relativePath = fullPath.substring(this.cwd.length).replace(/^[/\\]/, "");
						else relativePath = fullPath;
						relativePath = relativePath.split(path.sep).join("/");
						const virtPath = "/" + relativePath;
						if (isFile) results.push({
							path: virtPath,
							is_dir: false,
							size: entryStat.size,
							modified_at: entryStat.mtime.toISOString()
						});
						else if (isDir) results.push({
							path: virtPath + "/",
							is_dir: true,
							size: 0,
							modified_at: entryStat.mtime.toISOString()
						});
					}
				} catch {
					continue;
				}
			}
			results.sort((a, b) => a.path.localeCompare(b.path));
			return results;
		} catch {
			return [];
		}
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute or relative file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit$1 = 500) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			let content;
			if (SUPPORTS_NOFOLLOW) {
				if (!(await fs.stat(resolvedPath)).isFile()) return `Error: File '${filePath}' not found`;
				const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
				try {
					content = await fd.readFile({ encoding: "utf-8" });
				} finally {
					await fd.close();
				}
			} else {
				const stat = await fs.lstat(resolvedPath);
				if (stat.isSymbolicLink()) return `Error: Symlinks are not allowed: ${filePath}`;
				if (!stat.isFile()) return `Error: File '${filePath}' not found`;
				content = await fs.readFile(resolvedPath, "utf-8");
			}
			const emptyMsg = checkEmptyContent(content);
			if (emptyMsg) return emptyMsg;
			const lines = content.split("\n");
			const startIdx = offset;
			const endIdx = Math.min(startIdx + limit$1, lines.length);
			if (startIdx >= lines.length) return `Error: Line offset ${offset} exceeds file length (${lines.length} lines)`;
			return formatContentWithLineNumbers(lines.slice(startIdx, endIdx), startIdx + 1);
		} catch (e) {
			return `Error reading file '${filePath}': ${e.message}`;
		}
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const resolvedPath = this.resolvePath(filePath);
		let content;
		let stat;
		if (SUPPORTS_NOFOLLOW) {
			stat = await fs.stat(resolvedPath);
			if (!stat.isFile()) throw new Error(`File '${filePath}' not found`);
			const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
			try {
				content = await fd.readFile({ encoding: "utf-8" });
			} finally {
				await fd.close();
			}
		} else {
			stat = await fs.lstat(resolvedPath);
			if (stat.isSymbolicLink()) throw new Error(`Symlinks are not allowed: ${filePath}`);
			if (!stat.isFile()) throw new Error(`File '${filePath}' not found`);
			content = await fs.readFile(resolvedPath, "utf-8");
		}
		return {
			content: content.split("\n"),
			created_at: stat.ctime.toISOString(),
			modified_at: stat.mtime.toISOString()
		};
	}
	/**
	* Create a new file with content.
	* Returns WriteResult. External storage sets filesUpdate=null.
	*/
	async write(filePath, content) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			try {
				if ((await fs.lstat(resolvedPath)).isSymbolicLink()) return { error: `Cannot write to ${filePath} because it is a symlink. Symlinks are not allowed.` };
				return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
			} catch {}
			await fs.mkdir(path.dirname(resolvedPath), { recursive: true });
			if (SUPPORTS_NOFOLLOW) {
				const flags = fs$1.constants.O_WRONLY | fs$1.constants.O_CREAT | fs$1.constants.O_TRUNC | fs$1.constants.O_NOFOLLOW;
				const fd = await fs.open(resolvedPath, flags, 420);
				try {
					await fd.writeFile(content, "utf-8");
				} finally {
					await fd.close();
				}
			} else await fs.writeFile(resolvedPath, content, "utf-8");
			return {
				path: filePath,
				filesUpdate: null
			};
		} catch (e) {
			return { error: `Error writing file '${filePath}': ${e.message}` };
		}
	}
	/**
	* Edit a file by replacing string occurrences.
	* Returns EditResult. External storage sets filesUpdate=null.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		try {
			const resolvedPath = this.resolvePath(filePath);
			let content;
			if (SUPPORTS_NOFOLLOW) {
				if (!(await fs.stat(resolvedPath)).isFile()) return { error: `Error: File '${filePath}' not found` };
				const fd = await fs.open(resolvedPath, fs$1.constants.O_RDONLY | fs$1.constants.O_NOFOLLOW);
				try {
					content = await fd.readFile({ encoding: "utf-8" });
				} finally {
					await fd.close();
				}
			} else {
				const stat = await fs.lstat(resolvedPath);
				if (stat.isSymbolicLink()) return { error: `Error: Symlinks are not allowed: ${filePath}` };
				if (!stat.isFile()) return { error: `Error: File '${filePath}' not found` };
				content = await fs.readFile(resolvedPath, "utf-8");
			}
			const result = performStringReplacement(content, oldString, newString, replaceAll);
			if (typeof result === "string") return { error: result };
			const [newContent, occurrences] = result;
			if (SUPPORTS_NOFOLLOW) {
				const flags = fs$1.constants.O_WRONLY | fs$1.constants.O_TRUNC | fs$1.constants.O_NOFOLLOW;
				const fd = await fs.open(resolvedPath, flags);
				try {
					await fd.writeFile(newContent, "utf-8");
				} finally {
					await fd.close();
				}
			} else await fs.writeFile(resolvedPath, newContent, "utf-8");
			return {
				path: filePath,
				filesUpdate: null,
				occurrences
			};
		} catch (e) {
			return { error: `Error editing file '${filePath}': ${e.message}` };
		}
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, dirPath = "/", glob = null) {
		try {
			new RegExp(pattern);
		} catch (e) {
			return `Invalid regex pattern: ${e.message}`;
		}
		let baseFull;
		try {
			baseFull = this.resolvePath(dirPath || ".");
		} catch {
			return [];
		}
		try {
			await fs.stat(baseFull);
		} catch {
			return [];
		}
		let results = await this.ripgrepSearch(pattern, baseFull, glob);
		if (results === null) results = await this.pythonSearch(pattern, baseFull, glob);
		const matches = [];
		for (const [fpath, items] of Object.entries(results)) for (const [lineNum, lineText] of items) matches.push({
			path: fpath,
			line: lineNum,
			text: lineText
		});
		return matches;
	}
	/**
	* Try to use ripgrep for fast searching.
	* Returns null if ripgrep is not available or fails.
	*/
	async ripgrepSearch(pattern, baseFull, includeGlob) {
		return new Promise((resolve) => {
			const args = ["--json"];
			if (includeGlob) args.push("--glob", includeGlob);
			args.push("--", pattern, baseFull);
			const proc = spawn("rg", args, { timeout: 3e4 });
			const results = {};
			let output = "";
			proc.stdout.on("data", (data) => {
				output += data.toString();
			});
			proc.on("close", (code) => {
				if (code !== 0 && code !== 1) {
					resolve(null);
					return;
				}
				for (const line of output.split("\n")) {
					if (!line.trim()) continue;
					try {
						const data = JSON.parse(line);
						if (data.type !== "match") continue;
						const pdata = data.data || {};
						const ftext = pdata.path?.text;
						if (!ftext) continue;
						let virtPath;
						if (this.virtualMode) try {
							const resolved = path.resolve(ftext);
							const relative = path.relative(this.cwd, resolved);
							if (relative.startsWith("..")) continue;
							virtPath = "/" + relative.split(path.sep).join("/");
						} catch {
							continue;
						}
						else virtPath = ftext;
						const ln = pdata.line_number;
						const lt = pdata.lines?.text?.replace(/\n$/, "") || "";
						if (ln === void 0) continue;
						if (!results[virtPath]) results[virtPath] = [];
						results[virtPath].push([ln, lt]);
					} catch {
						continue;
					}
				}
				resolve(results);
			});
			proc.on("error", () => {
				resolve(null);
			});
		});
	}
	/**
	* Fallback regex search implementation.
	*/
	async pythonSearch(pattern, baseFull, includeGlob) {
		let regex;
		try {
			regex = new RegExp(pattern);
		} catch {
			return {};
		}
		const results = {};
		const files = await fg("**/*", {
			cwd: (await fs.stat(baseFull)).isDirectory() ? baseFull : path.dirname(baseFull),
			absolute: true,
			onlyFiles: true,
			dot: true
		});
		for (const fp of files) try {
			if (includeGlob && !micromatch.isMatch(path.basename(fp), includeGlob)) continue;
			if ((await fs.stat(fp)).size > this.maxFileSizeBytes) continue;
			const lines = (await fs.readFile(fp, "utf-8")).split("\n");
			for (let i = 0; i < lines.length; i++) {
				const line = lines[i];
				if (regex.test(line)) {
					let virtPath;
					if (this.virtualMode) try {
						const relative = path.relative(this.cwd, fp);
						if (relative.startsWith("..")) continue;
						virtPath = "/" + relative.split(path.sep).join("/");
					} catch {
						continue;
					}
					else virtPath = fp;
					if (!results[virtPath]) results[virtPath] = [];
					results[virtPath].push([i + 1, line]);
				}
			}
		} catch {
			continue;
		}
		return results;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, searchPath = "/") {
		if (pattern.startsWith("/")) pattern = pattern.substring(1);
		const resolvedSearchPath = searchPath === "/" ? this.cwd : this.resolvePath(searchPath);
		try {
			if (!(await fs.stat(resolvedSearchPath)).isDirectory()) return [];
		} catch {
			return [];
		}
		const results = [];
		try {
			const matches = await fg(pattern, {
				cwd: resolvedSearchPath,
				absolute: true,
				onlyFiles: true,
				dot: true
			});
			for (const matchedPath of matches) try {
				const stat = await fs.stat(matchedPath);
				if (!stat.isFile()) continue;
				const normalizedPath = matchedPath.split("/").join(path.sep);
				if (!this.virtualMode) results.push({
					path: normalizedPath,
					is_dir: false,
					size: stat.size,
					modified_at: stat.mtime.toISOString()
				});
				else {
					const cwdStr = this.cwd.endsWith(path.sep) ? this.cwd : this.cwd + path.sep;
					let relativePath;
					if (normalizedPath.startsWith(cwdStr)) relativePath = normalizedPath.substring(cwdStr.length);
					else if (normalizedPath.startsWith(this.cwd)) relativePath = normalizedPath.substring(this.cwd.length).replace(/^[/\\]/, "");
					else relativePath = normalizedPath;
					relativePath = relativePath.split(path.sep).join("/");
					const virt = "/" + relativePath;
					results.push({
						path: virt,
						is_dir: false,
						size: stat.size,
						modified_at: stat.mtime.toISOString()
					});
				}
			} catch {
				continue;
			}
		} catch {}
		results.sort((a, b) => a.path.localeCompare(b.path));
		return results;
	}
	/**
	* Upload multiple files to the filesystem.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const responses = [];
		for (const [filePath, content] of files) try {
			const resolvedPath = this.resolvePath(filePath);
			await fs.mkdir(path.dirname(resolvedPath), { recursive: true });
			await fs.writeFile(resolvedPath, content);
			responses.push({
				path: filePath,
				error: null
			});
		} catch (e) {
			if (e.code === "ENOENT") responses.push({
				path: filePath,
				error: "file_not_found"
			});
			else if (e.code === "EACCES") responses.push({
				path: filePath,
				error: "permission_denied"
			});
			else if (e.code === "EISDIR") responses.push({
				path: filePath,
				error: "is_directory"
			});
			else responses.push({
				path: filePath,
				error: "invalid_path"
			});
		}
		return responses;
	}
	/**
	* Download multiple files from the filesystem.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const responses = [];
		for (const filePath of paths) try {
			const resolvedPath = this.resolvePath(filePath);
			const content = await fs.readFile(resolvedPath);
			responses.push({
				path: filePath,
				content,
				error: null
			});
		} catch (e) {
			if (e.code === "ENOENT") responses.push({
				path: filePath,
				content: null,
				error: "file_not_found"
			});
			else if (e.code === "EACCES") responses.push({
				path: filePath,
				content: null,
				error: "permission_denied"
			});
			else if (e.code === "EISDIR") responses.push({
				path: filePath,
				content: null,
				error: "is_directory"
			});
			else responses.push({
				path: filePath,
				content: null,
				error: "invalid_path"
			});
		}
		return responses;
	}
};

//#endregion
//#region src/backends/composite.ts
/**
* Backend that routes file operations to different backends based on path prefix.
*
* This enables hybrid storage strategies like:
* - `/memories/` → StoreBackend (persistent, cross-thread)
* - Everything else → StateBackend (ephemeral, per-thread)
*
* The CompositeBackend handles path prefix stripping/re-adding transparently.
*/
var CompositeBackend = class {
	default;
	routes;
	sortedRoutes;
	constructor(defaultBackend, routes) {
		this.default = defaultBackend;
		this.routes = routes;
		this.sortedRoutes = Object.entries(routes).sort((a, b) => b[0].length - a[0].length);
	}
	/**
	* Determine which backend handles this key and strip prefix.
	*
	* @param key - Original file path
	* @returns Tuple of [backend, stripped_key] where stripped_key has the route
	*          prefix removed (but keeps leading slash).
	*/
	getBackendAndKey(key) {
		for (const [prefix, backend] of this.sortedRoutes) if (key.startsWith(prefix)) {
			const suffix = key.substring(prefix.length);
			return [backend, suffix ? "/" + suffix : "/"];
		}
		return [this.default, key];
	}
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects with route prefixes added, for files and directories
	*          directly in the directory. Directories have a trailing / in their path and is_dir=true.
	*/
	async lsInfo(path$2) {
		for (const [routePrefix, backend] of this.sortedRoutes) if (path$2.startsWith(routePrefix.replace(/\/$/, ""))) {
			const suffix = path$2.substring(routePrefix.length);
			const searchPath = suffix ? "/" + suffix : "/";
			const infos = await backend.lsInfo(searchPath);
			const prefixed = [];
			for (const fi of infos) prefixed.push({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			});
			return prefixed;
		}
		if (path$2 === "/") {
			const results = [];
			const defaultInfos = await this.default.lsInfo(path$2);
			results.push(...defaultInfos);
			for (const [routePrefix] of this.sortedRoutes) results.push({
				path: routePrefix,
				is_dir: true,
				size: 0,
				modified_at: ""
			});
			results.sort((a, b) => a.path.localeCompare(b.path));
			return results;
		}
		return await this.default.lsInfo(path$2);
	}
	/**
	* Read file content, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit$1 = 500) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.read(strippedKey, offset, limit$1);
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.readRaw(strippedKey);
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, path$2 = "/", glob = null) {
		for (const [routePrefix, backend] of this.sortedRoutes) if (path$2.startsWith(routePrefix.replace(/\/$/, ""))) {
			const searchPath = path$2.substring(routePrefix.length - 1);
			const raw = await backend.grepRaw(pattern, searchPath || "/", glob);
			if (typeof raw === "string") return raw;
			return raw.map((m) => ({
				...m,
				path: routePrefix.slice(0, -1) + m.path
			}));
		}
		const allMatches = [];
		const rawDefault = await this.default.grepRaw(pattern, path$2, glob);
		if (typeof rawDefault === "string") return rawDefault;
		allMatches.push(...rawDefault);
		for (const [routePrefix, backend] of Object.entries(this.routes)) {
			const raw = await backend.grepRaw(pattern, "/", glob);
			if (typeof raw === "string") return raw;
			allMatches.push(...raw.map((m) => ({
				...m,
				path: routePrefix.slice(0, -1) + m.path
			})));
		}
		return allMatches;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, path$2 = "/") {
		const results = [];
		for (const [routePrefix, backend] of this.sortedRoutes) if (path$2.startsWith(routePrefix.replace(/\/$/, ""))) {
			const searchPath = path$2.substring(routePrefix.length - 1);
			return (await backend.globInfo(pattern, searchPath || "/")).map((fi) => ({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			}));
		}
		const defaultInfos = await this.default.globInfo(pattern, path$2);
		results.push(...defaultInfos);
		for (const [routePrefix, backend] of Object.entries(this.routes)) {
			const infos = await backend.globInfo(pattern, "/");
			results.push(...infos.map((fi) => ({
				...fi,
				path: routePrefix.slice(0, -1) + fi.path
			})));
		}
		results.sort((a, b) => a.path.localeCompare(b.path));
		return results;
	}
	/**
	* Create a new file, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param content - File content as string
	* @returns WriteResult with path or error
	*/
	async write(filePath, content) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.write(strippedKey, content);
	}
	/**
	* Edit a file, routing to appropriate backend.
	*
	* @param filePath - Absolute file path
	* @param oldString - String to find and replace
	* @param newString - Replacement string
	* @param replaceAll - If true, replace all occurrences
	* @returns EditResult with path, occurrences, or error
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const [backend, strippedKey] = this.getBackendAndKey(filePath);
		return await backend.edit(strippedKey, oldString, newString, replaceAll);
	}
	/**
	* Execute a command via the default backend.
	* Execution is not path-specific, so it always delegates to the default backend.
	*
	* @param command - Full shell command string to execute
	* @returns ExecuteResponse with combined output, exit code, and truncation flag
	* @throws Error if the default backend doesn't support command execution
	*/
	execute(command) {
		if (!isSandboxBackend(this.default)) throw new Error("Default backend doesn't support command execution (SandboxBackendProtocol). To enable execution, provide a default backend that implements SandboxBackendProtocol.");
		return Promise.resolve(this.default.execute(command));
	}
	/**
	* Upload multiple files, batching by backend for efficiency.
	*
	* @param files - List of [path, content] tuples to upload
	* @returns List of FileUploadResponse objects, one per input file
	*/
	async uploadFiles(files) {
		const results = Array.from({ length: files.length }, () => null);
		const batchesByBackend = /* @__PURE__ */ new Map();
		for (let idx = 0; idx < files.length; idx++) {
			const [path$2, content] = files[idx];
			const [backend, strippedPath] = this.getBackendAndKey(path$2);
			if (!batchesByBackend.has(backend)) batchesByBackend.set(backend, []);
			batchesByBackend.get(backend).push({
				idx,
				path: strippedPath,
				content
			});
		}
		for (const [backend, batch] of batchesByBackend) {
			if (!backend.uploadFiles) throw new Error("Backend does not support uploadFiles");
			const batchFiles = batch.map((b) => [b.path, b.content]);
			const batchResponses = await backend.uploadFiles(batchFiles);
			for (let i = 0; i < batch.length; i++) {
				const originalIdx = batch[i].idx;
				results[originalIdx] = {
					path: files[originalIdx][0],
					error: batchResponses[i]?.error ?? null
				};
			}
		}
		return results;
	}
	/**
	* Download multiple files, batching by backend for efficiency.
	*
	* @param paths - List of file paths to download
	* @returns List of FileDownloadResponse objects, one per input path
	*/
	async downloadFiles(paths) {
		const results = Array.from({ length: paths.length }, () => null);
		const batchesByBackend = /* @__PURE__ */ new Map();
		for (let idx = 0; idx < paths.length; idx++) {
			const path$2 = paths[idx];
			const [backend, strippedPath] = this.getBackendAndKey(path$2);
			if (!batchesByBackend.has(backend)) batchesByBackend.set(backend, []);
			batchesByBackend.get(backend).push({
				idx,
				path: strippedPath
			});
		}
		for (const [backend, batch] of batchesByBackend) {
			if (!backend.downloadFiles) throw new Error("Backend does not support downloadFiles");
			const batchPaths = batch.map((b) => b.path);
			const batchResponses = await backend.downloadFiles(batchPaths);
			for (let i = 0; i < batch.length; i++) {
				const originalIdx = batch[i].idx;
				results[originalIdx] = {
					path: paths[originalIdx],
					content: batchResponses[i]?.content ?? null,
					error: batchResponses[i]?.error ?? null
				};
			}
		}
		return results;
	}
};

//#endregion
//#region src/backends/sandbox.ts
/**
* Node.js command template for glob operations.
* Uses web-standard atob() for base64 decoding.
*/
function buildGlobCommand(searchPath, pattern) {
	return `node -e "
const fs = require('fs');
const path = require('path');

const searchPath = atob('${btoa(searchPath)}');
const pattern = atob('${btoa(pattern)}');

function globMatch(relativePath, pattern) {
  const regexPattern = pattern
    .replace(/\\*\\*/g, '<<<GLOBSTAR>>>')
    .replace(/\\*/g, '[^/]*')
    .replace(/\\?/g, '.')
    .replace(/<<<GLOBSTAR>>>/g, '.*');
  return new RegExp('^' + regexPattern + '$').test(relativePath);
}

function walkDir(dir, baseDir, results) {
  try {
    const entries = fs.readdirSync(dir, { withFileTypes: true });
    for (const entry of entries) {
      const fullPath = path.join(dir, entry.name);
      const relativePath = path.relative(baseDir, fullPath);
      if (entry.isDirectory()) {
        walkDir(fullPath, baseDir, results);
      } else if (globMatch(relativePath, pattern)) {
        const stat = fs.statSync(fullPath);
        console.log(JSON.stringify({
          path: relativePath,
          size: stat.size,
          mtime: stat.mtimeMs,
          isDir: false
        }));
      }
    }
  } catch (e) {
    // Silent failure for non-existent paths
  }
}

try {
  process.chdir(searchPath);
  walkDir('.', '.', []);
} catch (e) {
  // Silent failure for non-existent paths
}
"`;
}
/**
* Node.js command template for listing directory contents.
*/
function buildLsCommand(dirPath) {
	return `node -e "
const fs = require('fs');
const path = require('path');

const dirPath = atob('${btoa(dirPath)}');

try {
  const entries = fs.readdirSync(dirPath, { withFileTypes: true });
  for (const entry of entries) {
    const fullPath = path.join(dirPath, entry.name);
    const stat = fs.statSync(fullPath);
    console.log(JSON.stringify({
      path: entry.isDirectory() ? fullPath + '/' : fullPath,
      size: stat.size,
      mtime: stat.mtimeMs,
      isDir: entry.isDirectory()
    }));
  }
} catch (e) {
  console.error('Error: ' + e.message);
  process.exit(1);
}
"`;
}
/**
* Node.js command template for reading files.
*/
function buildReadCommand(filePath, offset, limit$1) {
	return `node -e "
const fs = require('fs');

const filePath = atob('${btoa(filePath)}');
const offset = ${Number.isFinite(offset) && offset > 0 ? Math.floor(offset) : 0};
const limit = ${Number.isFinite(limit$1) && limit$1 > 0 && limit$1 < Number.MAX_SAFE_INTEGER ? Math.floor(limit$1) : 0};

if (!fs.existsSync(filePath)) {
  console.log('Error: File not found');
  process.exit(1);
}

const stat = fs.statSync(filePath);
if (stat.size === 0) {
  console.log('System reminder: File exists but has empty contents');
  process.exit(0);
}

const content = fs.readFileSync(filePath, 'utf-8');
const lines = content.split('\\n');
const selected = lines.slice(offset, offset + limit);

for (let i = 0; i < selected.length; i++) {
  const lineNum = offset + i + 1;
  console.log(String(lineNum).padStart(6) + '\\t' + selected[i]);
}
"`;
}
/**
* Node.js command template for writing files.
*/
function buildWriteCommand(filePath, content) {
	return `node -e "
const fs = require('fs');
const path = require('path');

const filePath = atob('${btoa(filePath)}');
const content = atob('${btoa(content)}');

if (fs.existsSync(filePath)) {
  console.error('Error: File already exists');
  process.exit(1);
}

const parentDir = path.dirname(filePath) || '.';
fs.mkdirSync(parentDir, { recursive: true });

fs.writeFileSync(filePath, content, 'utf-8');
console.log('OK');
"`;
}
/**
* Node.js command template for editing files.
*/
function buildEditCommand(filePath, oldStr, newStr, replaceAll) {
	return `node -e "
const fs = require('fs');

const filePath = atob('${btoa(filePath)}');
const oldStr = atob('${btoa(oldStr)}');
const newStr = atob('${btoa(newStr)}');
const replaceAll = ${Boolean(replaceAll)};

let text;
try {
  text = fs.readFileSync(filePath, 'utf-8');
} catch (e) {
  process.exit(3);
}

const count = text.split(oldStr).length - 1;

if (count === 0) {
  process.exit(1);
}
if (count > 1 && !replaceAll) {
  process.exit(2);
}

const result = text.split(oldStr).join(newStr);
fs.writeFileSync(filePath, result, 'utf-8');
console.log(count);
"`;
}
/**
* Node.js command template for grep operations.
*/
function buildGrepCommand(pattern, searchPath, globPattern) {
	const patternB64 = btoa(pattern);
	const pathB64 = btoa(searchPath);
	const globB64 = globPattern ? btoa(globPattern) : "";
	return `node -e "
const fs = require('fs');
const path = require('path');

const pattern = atob('${patternB64}');
const searchPath = atob('${pathB64}');
const globPattern = ${globPattern ? `atob('${globB64}')` : "null"};

let regex;
try {
  regex = new RegExp(pattern);
} catch (e) {
  console.error('Invalid regex: ' + e.message);
  process.exit(1);
}

function globMatch(filePath, pattern) {
  if (!pattern) return true;
  const regexPattern = pattern
    .replace(/\\*\\*/g, '<<<GLOBSTAR>>>')
    .replace(/\\*/g, '[^/]*')
    .replace(/\\?/g, '.')
    .replace(/<<<GLOBSTAR>>>/g, '.*');
  return new RegExp('^' + regexPattern + '$').test(filePath);
}

function walkDir(dir, results) {
  try {
    const entries = fs.readdirSync(dir, { withFileTypes: true });
    for (const entry of entries) {
      const fullPath = path.join(dir, entry.name);
      if (entry.isDirectory()) {
        walkDir(fullPath, results);
      } else {
        const relativePath = path.relative(searchPath, fullPath);
        if (globMatch(relativePath, globPattern)) {
          try {
            const content = fs.readFileSync(fullPath, 'utf-8');
            const lines = content.split('\\n');
            for (let i = 0; i < lines.length; i++) {
              if (regex.test(lines[i])) {
                console.log(JSON.stringify({
                  path: fullPath,
                  line: i + 1,
                  text: lines[i]
                }));
              }
            }
          } catch (e) {
            // Skip unreadable files
          }
        }
      }
    }
  } catch (e) {
    // Skip unreadable directories
  }
}

try {
  walkDir(searchPath, []);
} catch (e) {
  // Silent failure
}
"`;
}
/**
* Base sandbox implementation with execute() as the only abstract method.
*
* This class provides default implementations for all SandboxBackendProtocol
* methods using shell commands executed via execute(). Concrete implementations
* only need to implement the execute() method.
*
* Requires Node.js 20+ on the sandbox host.
*/
var BaseSandbox = class {
	/**
	* List files and directories in the specified directory (non-recursive).
	*
	* @param path - Absolute path to directory
	* @returns List of FileInfo objects for files and directories directly in the directory.
	*/
	async lsInfo(path$2) {
		const command = buildLsCommand(path$2);
		const result = await this.execute(command);
		if (result.exitCode !== 0) return [];
		const infos = [];
		const lines = result.output.trim().split("\n").filter(Boolean);
		for (const line of lines) try {
			const parsed = JSON.parse(line);
			infos.push({
				path: parsed.path,
				is_dir: parsed.isDir,
				size: parsed.size,
				modified_at: parsed.mtime ? new Date(parsed.mtime).toISOString() : void 0
			});
		} catch {}
		return infos;
	}
	/**
	* Read file content with line numbers.
	*
	* @param filePath - Absolute file path
	* @param offset - Line offset to start reading from (0-indexed)
	* @param limit - Maximum number of lines to read
	* @returns Formatted file content with line numbers, or error message
	*/
	async read(filePath, offset = 0, limit$1 = 500) {
		const command = buildReadCommand(filePath, offset, limit$1);
		const result = await this.execute(command);
		if (result.exitCode !== 0) return `Error: File '${filePath}' not found`;
		return result.output;
	}
	/**
	* Read file content as raw FileData.
	*
	* @param filePath - Absolute file path
	* @returns Raw file content as FileData
	*/
	async readRaw(filePath) {
		const command = buildReadCommand(filePath, 0, Number.MAX_SAFE_INTEGER);
		const result = await this.execute(command);
		if (result.exitCode !== 0) throw new Error(`File '${filePath}' not found`);
		const lines = [];
		for (const line of result.output.split("\n")) {
			const tabIndex = line.indexOf("	");
			if (tabIndex !== -1) lines.push(line.substring(tabIndex + 1));
		}
		const now = (/* @__PURE__ */ new Date()).toISOString();
		return {
			content: lines,
			created_at: now,
			modified_at: now
		};
	}
	/**
	* Structured search results or error string for invalid input.
	*/
	async grepRaw(pattern, path$2 = "/", glob = null) {
		const command = buildGrepCommand(pattern, path$2, glob);
		const result = await this.execute(command);
		if (result.exitCode === 1) {
			if (result.output.includes("Invalid regex:")) return result.output.trim();
		}
		const matches = [];
		const lines = result.output.trim().split("\n").filter(Boolean);
		for (const line of lines) try {
			const parsed = JSON.parse(line);
			matches.push({
				path: parsed.path,
				line: parsed.line,
				text: parsed.text
			});
		} catch {}
		return matches;
	}
	/**
	* Structured glob matching returning FileInfo objects.
	*/
	async globInfo(pattern, path$2 = "/") {
		const command = buildGlobCommand(path$2, pattern);
		const result = await this.execute(command);
		const infos = [];
		const lines = result.output.trim().split("\n").filter(Boolean);
		for (const line of lines) try {
			const parsed = JSON.parse(line);
			infos.push({
				path: parsed.path,
				is_dir: parsed.isDir,
				size: parsed.size,
				modified_at: parsed.mtime ? new Date(parsed.mtime).toISOString() : void 0
			});
		} catch {}
		return infos;
	}
	/**
	* Create a new file with content.
	*/
	async write(filePath, content) {
		const command = buildWriteCommand(filePath, content);
		if ((await this.execute(command)).exitCode !== 0) return { error: `Cannot write to ${filePath} because it already exists. Read and then make an edit, or write to a new path.` };
		return {
			path: filePath,
			filesUpdate: null
		};
	}
	/**
	* Edit a file by replacing string occurrences.
	*/
	async edit(filePath, oldString, newString, replaceAll = false) {
		const command = buildEditCommand(filePath, oldString, newString, replaceAll);
		const result = await this.execute(command);
		switch (result.exitCode) {
			case 0: return {
				path: filePath,
				filesUpdate: null,
				occurrences: parseInt(result.output.trim(), 10) || 1
			};
			case 1: return { error: `String not found in file '${filePath}'` };
			case 2: return { error: `Multiple occurrences found in '${filePath}'. Use replaceAll=true to replace all.` };
			case 3: return { error: `Error: File '${filePath}' not found` };
			default: return { error: `Unknown error editing file '${filePath}'` };
		}
	}
};

//#endregion
//#region src/agent.ts
const BASE_PROMPT = `In order to complete the objective that the user asks of you, you have access to a number of standard tools.`;
/**
* Create a Deep Agent with middleware-based architecture.
*
* Matches Python's create_deep_agent function, using middleware for all features:
* - Todo management (todoListMiddleware)
* - Filesystem tools (createFilesystemMiddleware)
* - Subagent delegation (createSubAgentMiddleware)
* - Conversation summarization (summarizationMiddleware)
* - Prompt caching (anthropicPromptCachingMiddleware)
* - Tool call patching (createPatchToolCallsMiddleware)
* - Human-in-the-loop (humanInTheLoopMiddleware) - optional
*
* @param params Configuration parameters for the agent
* @returns ReactAgent instance ready for invocation with properly inferred state types
*
* @example
* ```typescript
* // Middleware with custom state
* const ResearchMiddleware = createMiddleware({
*   name: "ResearchMiddleware",
*   stateSchema: z.object({ research: z.string().default("") }),
* });
*
* const agent = createDeepAgent({
*   middleware: [ResearchMiddleware],
* });
*
* const result = await agent.invoke({ messages: [...] });
* // result.research is properly typed as string
* ```
*/
function createDeepAgent(params = {}) {
	const { model = "claude-sonnet-4-5-20250929", tools = [], systemPrompt, middleware: customMiddleware = [], subagents = [], responseFormat, contextSchema, checkpointer, store, backend, interruptOn, name, memory, skills } = params;
	const finalSystemPrompt = systemPrompt ? typeof systemPrompt === "string" ? `${systemPrompt}\n\n${BASE_PROMPT}` : new SystemMessage({ content: [{
		type: "text",
		text: BASE_PROMPT
	}, ...typeof systemPrompt.content === "string" ? [{
		type: "text",
		text: systemPrompt.content
	}] : systemPrompt.content] }) : BASE_PROMPT;
	const filesystemBackend = backend ? backend : (config) => new StateBackend(config);
	const skillsMiddleware = skills != null && skills.length > 0 ? [createSkillsMiddleware({
		backend: filesystemBackend,
		sources: skills
	})] : [];
	const builtInMiddleware = [
		todoListMiddleware(),
		...skillsMiddleware,
		createFilesystemMiddleware({ backend: filesystemBackend }),
		createSubAgentMiddleware({
			defaultModel: model,
			defaultTools: tools,
			defaultMiddleware: [
				todoListMiddleware(),
				...skillsMiddleware,
				createFilesystemMiddleware({ backend: filesystemBackend }),
				summarizationMiddleware({
					model,
					trigger: { tokens: 17e4 },
					keep: { messages: 6 }
				}),
				anthropicPromptCachingMiddleware({ unsupportedModelBehavior: "ignore" }),
				createPatchToolCallsMiddleware()
			],
			defaultInterruptOn: interruptOn,
			subagents,
			generalPurposeAgent: true
		}),
		summarizationMiddleware({
			model,
			trigger: { tokens: 17e4 },
			keep: { messages: 6 }
		}),
		anthropicPromptCachingMiddleware({ unsupportedModelBehavior: "ignore" }),
		createPatchToolCallsMiddleware(),
		...memory != null && memory.length > 0 ? [createMemoryMiddleware({
			backend: filesystemBackend,
			sources: memory
		})] : []
	];
	if (interruptOn) builtInMiddleware.push(humanInTheLoopMiddleware({ interruptOn }));
	return createAgent({
		model,
		systemPrompt: finalSystemPrompt,
		tools,
		middleware: [...builtInMiddleware, ...customMiddleware],
		responseFormat,
		contextSchema,
		checkpointer,
		store,
		name
	});
}

//#endregion
//#region src/config.ts
/**
* Configuration and settings for deepagents.
*
* Provides project detection, path management, and environment configuration
* for skills and agent memory middleware.
*/
/**
* Find the project root by looking for .git directory.
*
* Walks up the directory tree from startPath (or cwd) looking for a .git
* directory, which indicates the project root.
*
* @param startPath - Directory to start searching from. Defaults to current working directory.
* @returns Path to the project root if found, null otherwise.
*/
function findProjectRoot(startPath) {
	let current = path.resolve(startPath || process.cwd());
	while (current !== path.dirname(current)) {
		const gitDir = path.join(current, ".git");
		if (fs$1.existsSync(gitDir)) return current;
		current = path.dirname(current);
	}
	const rootGitDir = path.join(current, ".git");
	if (fs$1.existsSync(rootGitDir)) return current;
	return null;
}
/**
* Validate agent name to prevent invalid filesystem paths and security issues.
*
* @param agentName - The agent name to validate
* @returns True if valid, false otherwise
*/
function isValidAgentName(agentName) {
	if (!agentName || !agentName.trim()) return false;
	return /^[a-zA-Z0-9_\-\s]+$/.test(agentName);
}
/**
* Create a Settings instance with detected environment.
*
* @param options - Configuration options
* @returns Settings instance with project detection and path management
*/
function createSettings(options = {}) {
	const projectRoot = findProjectRoot(options.startPath);
	const userDeepagentsDir = path.join(os.homedir(), ".deepagents");
	return {
		projectRoot,
		userDeepagentsDir,
		hasProject: projectRoot !== null,
		getAgentDir(agentName) {
			if (!isValidAgentName(agentName)) throw new Error(`Invalid agent name: ${JSON.stringify(agentName)}. Agent names can only contain letters, numbers, hyphens, underscores, and spaces.`);
			return path.join(userDeepagentsDir, agentName);
		},
		ensureAgentDir(agentName) {
			const agentDir = this.getAgentDir(agentName);
			fs$1.mkdirSync(agentDir, { recursive: true });
			return agentDir;
		},
		getUserAgentMdPath(agentName) {
			return path.join(this.getAgentDir(agentName), "agent.md");
		},
		getProjectAgentMdPath() {
			if (!projectRoot) return null;
			return path.join(projectRoot, ".deepagents", "agent.md");
		},
		getUserSkillsDir(agentName) {
			return path.join(this.getAgentDir(agentName), "skills");
		},
		ensureUserSkillsDir(agentName) {
			const skillsDir = this.getUserSkillsDir(agentName);
			fs$1.mkdirSync(skillsDir, { recursive: true });
			return skillsDir;
		},
		getProjectSkillsDir() {
			if (!projectRoot) return null;
			return path.join(projectRoot, ".deepagents", "skills");
		},
		ensureProjectSkillsDir() {
			const skillsDir = this.getProjectSkillsDir();
			if (!skillsDir) return null;
			fs$1.mkdirSync(skillsDir, { recursive: true });
			return skillsDir;
		},
		ensureProjectDeepagentsDir() {
			if (!projectRoot) return null;
			const deepagentsDir = path.join(projectRoot, ".deepagents");
			fs$1.mkdirSync(deepagentsDir, { recursive: true });
			return deepagentsDir;
		}
	};
}

//#endregion
//#region src/middleware/agent-memory.ts
/**
* Middleware for loading agent-specific long-term memory into the system prompt.
*
* This middleware loads the agent's long-term memory from agent.md files
* and injects it into the system prompt. Memory is loaded from:
* - User memory: ~/.deepagents/{agent_name}/agent.md
* - Project memory: {project_root}/.deepagents/agent.md
*
* @deprecated Use `createMemoryMiddleware` from `./memory.js` instead.
* This middleware uses direct filesystem access (Node.js fs module) which is not
* portable across backends. The `createMemoryMiddleware` function uses the
* `BackendProtocol` abstraction and follows the AGENTS.md specification.
*
* Migration example:
* ```typescript
* // Before (deprecated):
* import { createAgentMemoryMiddleware } from "./agent-memory.js";
* const middleware = createAgentMemoryMiddleware({ settings, assistantId });
*
* // After (recommended):
* import { createMemoryMiddleware } from "./memory.js";
* import { FilesystemBackend } from "../backends/filesystem.js";
*
* const middleware = createMemoryMiddleware({
*   backend: new FilesystemBackend({ rootDir: "/" }),
*   sources: [
*     `~/.deepagents/${assistantId}/AGENTS.md`,
*     `${projectRoot}/.deepagents/AGENTS.md`,
*   ],
* });
* ```
*/
/**
* State schema for agent memory middleware.
*/
const AgentMemoryStateSchema = z$1.object({
	userMemory: z$1.string().optional(),
	projectMemory: z$1.string().optional()
});
/**
* Default template for memory injection.
*/
const DEFAULT_MEMORY_TEMPLATE = `<user_memory>
{user_memory}
</user_memory>

<project_memory>
{project_memory}
</project_memory>`;
/**
* Long-term Memory Documentation system prompt.
*/
const LONGTERM_MEMORY_SYSTEM_PROMPT = `

## Long-term Memory

Your long-term memory is stored in files on the filesystem and persists across sessions.

**User Memory Location**: \`{agent_dir_absolute}\` (displays as \`{agent_dir_display}\`)
**Project Memory Location**: {project_memory_info}

Your system prompt is loaded from TWO sources at startup:
1. **User agent.md**: \`{agent_dir_absolute}/agent.md\` - Your personal preferences across all projects
2. **Project agent.md**: Loaded from project root if available - Project-specific instructions

Project-specific agent.md is loaded from these locations (both combined if both exist):
- \`[project-root]/.deepagents/agent.md\` (preferred)
- \`[project-root]/agent.md\` (fallback, but also included if both exist)

**When to CHECK/READ memories (CRITICAL - do this FIRST):**
- **At the start of ANY new session**: Check both user and project memories
  - User: \`ls {agent_dir_absolute}\`
  - Project: \`ls {project_deepagents_dir}\` (if in a project)
- **BEFORE answering questions**: If asked "what do you know about X?" or "how do I do Y?", check project memories FIRST, then user
- **When user asks you to do something**: Check if you have project-specific guides or examples
- **When user references past work**: Search project memory files for related context

**Memory-first response pattern:**
1. User asks a question → Check project directory first: \`ls {project_deepagents_dir}\`
2. If relevant files exist → Read them with \`read_file '{project_deepagents_dir}/[filename]'\`
3. Check user memory if needed → \`ls {agent_dir_absolute}\`
4. Base your answer on saved knowledge supplemented by general knowledge

**When to update memories:**
- **IMMEDIATELY when the user describes your role or how you should behave**
- **IMMEDIATELY when the user gives feedback on your work** - Update memories to capture what was wrong and how to do it better
- When the user explicitly asks you to remember something
- When patterns or preferences emerge (coding styles, conventions, workflows)
- After significant work where context would help in future sessions

**Learning from feedback:**
- When user says something is better/worse, capture WHY and encode it as a pattern
- Each correction is a chance to improve permanently - don't just fix the immediate issue, update your instructions
- When user says "you should remember X" or "be careful about Y", treat this as HIGH PRIORITY - update memories IMMEDIATELY
- Look for the underlying principle behind corrections, not just the specific mistake

## Deciding Where to Store Memory

When writing or updating agent memory, decide whether each fact, configuration, or behavior belongs in:

### User Agent File: \`{agent_dir_absolute}/agent.md\`
→ Describes the agent's **personality, style, and universal behavior** across all projects.

**Store here:**
- Your general tone and communication style
- Universal coding preferences (formatting, comment style, etc.)
- General workflows and methodologies you follow
- Tool usage patterns that apply everywhere
- Personal preferences that don't change per-project

**Examples:**
- "Be concise and direct in responses"
- "Always use type hints in Python"
- "Prefer functional programming patterns"

### Project Agent File: \`{project_deepagents_dir}/agent.md\`
→ Describes **how this specific project works** and **how the agent should behave here only.**

**Store here:**
- Project-specific architecture and design patterns
- Coding conventions specific to this codebase
- Project structure and organization
- Testing strategies for this project
- Deployment processes and workflows
- Team conventions and guidelines

**Examples:**
- "This project uses FastAPI with SQLAlchemy"
- "Tests go in tests/ directory mirroring src/ structure"
- "All API changes require updating OpenAPI spec"

### Project Memory Files: \`{project_deepagents_dir}/*.md\`
→ Use for **project-specific reference information** and structured notes.

**Store here:**
- API design documentation
- Architecture decisions and rationale
- Deployment procedures
- Common debugging patterns
- Onboarding information

**Examples:**
- \`{project_deepagents_dir}/api-design.md\` - REST API patterns used
- \`{project_deepagents_dir}/architecture.md\` - System architecture overview
- \`{project_deepagents_dir}/deployment.md\` - How to deploy this project

### File Operations:

**User memory:**
\`\`\`
ls {agent_dir_absolute}                              # List user memory files
read_file '{agent_dir_absolute}/agent.md'            # Read user preferences
edit_file '{agent_dir_absolute}/agent.md' ...        # Update user preferences
\`\`\`

**Project memory (preferred for project-specific information):**
\`\`\`
ls {project_deepagents_dir}                          # List project memory files
read_file '{project_deepagents_dir}/agent.md'        # Read project instructions
edit_file '{project_deepagents_dir}/agent.md' ...    # Update project instructions
write_file '{project_deepagents_dir}/agent.md' ...  # Create project memory file
\`\`\`

**Important**:
- Project memory files are stored in \`.deepagents/\` inside the project root
- Always use absolute paths for file operations
- Check project memories BEFORE user when answering project-specific questions`;
/**
* Create middleware for loading agent-specific long-term memory.
*
* This middleware loads the agent's long-term memory from a file (agent.md)
* and injects it into the system prompt. The memory is loaded once at the
* start of the conversation and stored in state.
*
* @param options - Configuration options
* @returns AgentMiddleware for memory loading and injection
*
* @deprecated Use `createMemoryMiddleware` from `./memory.js` instead.
* This function uses direct filesystem access which limits portability.
*/
function createAgentMemoryMiddleware(options) {
	const { settings, assistantId, systemPromptTemplate } = options;
	const agentDir = settings.getAgentDir(assistantId);
	const agentDirDisplay = `~/.deepagents/${assistantId}`;
	const agentDirAbsolute = agentDir;
	const projectRoot = settings.projectRoot;
	const projectMemoryInfo = projectRoot ? `\`${projectRoot}\` (detected)` : "None (not in a git project)";
	const projectDeepagentsDir = projectRoot ? `${projectRoot}/.deepagents` : "[project-root]/.deepagents (not in a project)";
	const template = systemPromptTemplate || DEFAULT_MEMORY_TEMPLATE;
	return createMiddleware({
		name: "AgentMemoryMiddleware",
		stateSchema: AgentMemoryStateSchema,
		beforeAgent(state) {
			const result = {};
			if (!("userMemory" in state)) {
				const userPath = settings.getUserAgentMdPath(assistantId);
				if (fs$1.existsSync(userPath)) try {
					result.userMemory = fs$1.readFileSync(userPath, "utf-8");
				} catch {}
			}
			if (!("projectMemory" in state)) {
				const projectPath = settings.getProjectAgentMdPath();
				if (projectPath && fs$1.existsSync(projectPath)) try {
					result.projectMemory = fs$1.readFileSync(projectPath, "utf-8");
				} catch {}
			}
			return Object.keys(result).length > 0 ? result : void 0;
		},
		wrapModelCall(request, handler) {
			const userMemory = request.state?.userMemory;
			const projectMemory = request.state?.projectMemory;
			const baseSystemPrompt = request.systemPrompt || "";
			const memorySection = template.replace("{user_memory}", userMemory || "(No user agent.md)").replace("{project_memory}", projectMemory || "(No project agent.md)");
			const memoryDocs = LONGTERM_MEMORY_SYSTEM_PROMPT.replaceAll("{agent_dir_absolute}", agentDirAbsolute).replaceAll("{agent_dir_display}", agentDirDisplay).replaceAll("{project_memory_info}", projectMemoryInfo).replaceAll("{project_deepagents_dir}", projectDeepagentsDir);
			let systemPrompt = memorySection;
			if (baseSystemPrompt) systemPrompt += "\n\n" + baseSystemPrompt;
			systemPrompt += "\n\n" + memoryDocs;
			return handler({
				...request,
				systemPrompt
			});
		}
	});
}

//#endregion
//#region src/skills/loader.ts
/**
* Skill loader for parsing and loading agent skills from SKILL.md files.
*
* This module implements Anthropic's agent skills pattern with YAML frontmatter parsing.
* Each skill is a directory containing a SKILL.md file with:
* - YAML frontmatter (name, description required)
* - Markdown instructions for the agent
* - Optional supporting files (scripts, configs, etc.)
*
* @example
* ```markdown
* ---
* name: web-research
* description: Structured approach to conducting thorough web research
* ---
*
* # Web Research Skill
*
* ## When to Use
* - User asks you to research a topic
* ...
* ```
*
* @see https://agentskills.io/specification
*/
/** Maximum size for SKILL.md files (10MB) */
const MAX_SKILL_FILE_SIZE$1 = 10 * 1024 * 1024;
/** Agent Skills spec constraints */
const MAX_SKILL_NAME_LENGTH$1 = 64;
const MAX_SKILL_DESCRIPTION_LENGTH$1 = 1024;
/** Pattern for validating skill names per Agent Skills spec */
const SKILL_NAME_PATTERN = /^[a-z0-9]+(-[a-z0-9]+)*$/;
/** Pattern for extracting YAML frontmatter */
const FRONTMATTER_PATTERN = /^---\s*\n([\s\S]*?)\n---\s*\n/;
/**
* Check if a path is safely contained within base_dir.
*
* This prevents directory traversal attacks via symlinks or path manipulation.
* The function resolves both paths to their canonical form (following symlinks)
* and verifies that the target path is within the base directory.
*
* @param targetPath - The path to validate
* @param baseDir - The base directory that should contain the path
* @returns True if the path is safely within baseDir, false otherwise
*/
function isSafePath(targetPath, baseDir) {
	try {
		const resolvedPath = fs$1.realpathSync(targetPath);
		const resolvedBase = fs$1.realpathSync(baseDir);
		return resolvedPath.startsWith(resolvedBase + path.sep) || resolvedPath === resolvedBase;
	} catch {
		return false;
	}
}
/**
* Validate skill name per Agent Skills spec.
*
* Requirements:
* - Max 64 characters
* - Lowercase alphanumeric and hyphens only (a-z, 0-9, -)
* - Cannot start or end with hyphen
* - No consecutive hyphens
* - Must match parent directory name
*
* @param name - The skill name from YAML frontmatter
* @param directoryName - The parent directory name
* @returns Validation result with error message if invalid
*/
function validateSkillName(name, directoryName) {
	if (!name) return {
		valid: false,
		error: "name is required"
	};
	if (name.length > MAX_SKILL_NAME_LENGTH$1) return {
		valid: false,
		error: "name exceeds 64 characters"
	};
	if (!SKILL_NAME_PATTERN.test(name)) return {
		valid: false,
		error: "name must be lowercase alphanumeric with single hyphens only"
	};
	if (name !== directoryName) return {
		valid: false,
		error: `name '${name}' must match directory name '${directoryName}'`
	};
	return { valid: true };
}
/**
* Parse YAML frontmatter from content.
*
* @param content - The file content
* @returns Parsed frontmatter object, or null if parsing fails
*/
function parseFrontmatter(content) {
	const match = content.match(FRONTMATTER_PATTERN);
	if (!match) return null;
	try {
		const parsed = yaml.parse(match[1]);
		return typeof parsed === "object" && parsed !== null ? parsed : null;
	} catch {
		return null;
	}
}
/**
* Parse YAML frontmatter from a SKILL.md file per Agent Skills spec.
*
* @param skillMdPath - Path to the SKILL.md file
* @param source - Source of the skill ('user' or 'project')
* @returns SkillMetadata with all fields, or null if parsing fails
*/
function parseSkillMetadata(skillMdPath, source) {
	try {
		const stats = fs$1.statSync(skillMdPath);
		if (stats.size > MAX_SKILL_FILE_SIZE$1) {
			console.warn(`Skipping ${skillMdPath}: file too large (${stats.size} bytes)`);
			return null;
		}
		const frontmatter = parseFrontmatter(fs$1.readFileSync(skillMdPath, "utf-8"));
		if (!frontmatter) {
			console.warn(`Skipping ${skillMdPath}: no valid YAML frontmatter found`);
			return null;
		}
		const name = frontmatter.name;
		const description = frontmatter.description;
		if (!name || !description) {
			console.warn(`Skipping ${skillMdPath}: missing required 'name' or 'description'`);
			return null;
		}
		const directoryName = path.basename(path.dirname(skillMdPath));
		const validation = validateSkillName(String(name), directoryName);
		if (!validation.valid) console.warn(`Skill '${name}' in ${skillMdPath} does not follow Agent Skills spec: ${validation.error}. Consider renaming to be spec-compliant.`);
		let descriptionStr = String(description);
		if (descriptionStr.length > MAX_SKILL_DESCRIPTION_LENGTH$1) {
			console.warn(`Description exceeds ${MAX_SKILL_DESCRIPTION_LENGTH$1} chars in ${skillMdPath}, truncating`);
			descriptionStr = descriptionStr.slice(0, MAX_SKILL_DESCRIPTION_LENGTH$1);
		}
		return {
			name: String(name),
			description: descriptionStr,
			path: skillMdPath,
			source,
			license: frontmatter.license ? String(frontmatter.license) : void 0,
			compatibility: frontmatter.compatibility ? String(frontmatter.compatibility) : void 0,
			metadata: frontmatter.metadata && typeof frontmatter.metadata === "object" ? frontmatter.metadata : void 0,
			allowedTools: frontmatter["allowed-tools"] ? String(frontmatter["allowed-tools"]) : void 0
		};
	} catch (error) {
		console.warn(`Error reading ${skillMdPath}: ${error}`);
		return null;
	}
}
/**
* List all skills from a single skills directory (internal helper).
*
* Scans the skills directory for subdirectories containing SKILL.md files,
* parses YAML frontmatter, and returns skill metadata.
*
* Skills are organized as:
* ```
* skills/
* ├── skill-name/
* │   ├── SKILL.md        # Required: instructions with YAML frontmatter
* │   ├── script.py       # Optional: supporting files
* │   └── config.json     # Optional: supporting files
* ```
*
* @param skillsDir - Path to the skills directory
* @param source - Source of the skills ('user' or 'project')
* @returns List of skill metadata
*/
function listSkillsFromDir(skillsDir, source) {
	const expandedDir = skillsDir.startsWith("~") ? path.join(process.env.HOME || process.env.USERPROFILE || "", skillsDir.slice(1)) : skillsDir;
	if (!fs$1.existsSync(expandedDir)) return [];
	let resolvedBase;
	try {
		resolvedBase = fs$1.realpathSync(expandedDir);
	} catch {
		return [];
	}
	const skills = [];
	let entries;
	try {
		entries = fs$1.readdirSync(resolvedBase, { withFileTypes: true });
	} catch {
		return [];
	}
	for (const entry of entries) {
		const skillDir = path.join(resolvedBase, entry.name);
		if (!isSafePath(skillDir, resolvedBase)) continue;
		if (!entry.isDirectory()) continue;
		const skillMdPath = path.join(skillDir, "SKILL.md");
		if (!fs$1.existsSync(skillMdPath)) continue;
		if (!isSafePath(skillMdPath, resolvedBase)) continue;
		const metadata = parseSkillMetadata(skillMdPath, source);
		if (metadata) skills.push(metadata);
	}
	return skills;
}
/**
* List skills from user and/or project directories.
*
* When both directories are provided, project skills with the same name as
* user skills will override them.
*
* @param options - Options specifying which directories to search
* @returns Merged list of skill metadata from both sources, with project skills
*          taking precedence over user skills when names conflict
*/
function listSkills(options) {
	const allSkills = /* @__PURE__ */ new Map();
	if (options.userSkillsDir) {
		const userSkills = listSkillsFromDir(options.userSkillsDir, "user");
		for (const skill of userSkills) allSkills.set(skill.name, skill);
	}
	if (options.projectSkillsDir) {
		const projectSkills = listSkillsFromDir(options.projectSkillsDir, "project");
		for (const skill of projectSkills) allSkills.set(skill.name, skill);
	}
	return Array.from(allSkills.values());
}

//#endregion
export { BaseSandbox, CompositeBackend, FilesystemBackend, MAX_SKILL_DESCRIPTION_LENGTH, MAX_SKILL_FILE_SIZE, MAX_SKILL_NAME_LENGTH, StateBackend, StoreBackend, createAgentMemoryMiddleware, createDeepAgent, createFilesystemMiddleware, createMemoryMiddleware, createPatchToolCallsMiddleware, createSettings, createSkillsMiddleware, createSubAgentMiddleware, findProjectRoot, isSandboxBackend, listSkills, parseSkillMetadata };
//# sourceMappingURL=index.js.map